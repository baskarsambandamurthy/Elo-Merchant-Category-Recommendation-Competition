{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9e97164a7191d9d43ba8158c0f7e13aa455a0ce9"
   },
   "source": [
    "# Combining your model with a model without outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "7dd3ea3236069cf9706e377d6594c39b5e5a5507"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import log_loss\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "import gc\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "from sklearn.metrics import f1_score,precision_recall_curve,roc_curve, recall_score,precision_score\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "\n",
    "%matplotlib inline\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_columns  = 999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fcf141a7658f4089b1de2822ba41da9646c73835"
   },
   "source": [
    "* # Part 1 Training Model Without Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "f2379efdaca702827cf4393d4e0b11c718f033f0"
   },
   "outputs": [],
   "source": [
    "# import h5py\n",
    "# Path = '../input/elo-rfm-and-business-feats/'\n",
    "# f = h5py.File(Path+'train_preproc.hdf')\n",
    "# # list(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "bf5555c1ad80d3fffe74a6493995959a93b80b3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.4\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "f2379efdaca702827cf4393d4e0b11c718f033f0"
   },
   "outputs": [],
   "source": [
    "# print(list(f['data']))\n",
    "# # print(list(f['data']['axis0']))\n",
    "# print()\n",
    "# print()\n",
    "# print(list(f['data']['block0_values'][0:10]))\n",
    "\n",
    "# # print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "e7bd6dbc0148c27473ca5bff20b4795bf1880546"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.9 s, sys: 796 ms, total: 13.7 s\n",
      "Wall time: 13.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Path = '../input/elo-preproc-3/'\n",
    "Path = '../input/elo-rfm-and-business-feats/'\n",
    "\n",
    "df_train = pd.read_csv(Path+'train_preproc.csv',index_col=0)\n",
    "df_test = pd.read_csv(Path +'test_preproc.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "18d93d315eee398fb1cff4ad8d115c21d359cd49"
   },
   "source": [
    "## filtering out outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "3eb5ac784a25271b3433f6371d7b46cdabb420b5"
   },
   "outputs": [],
   "source": [
    "mask_without_outlier = df_train['outliers'] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "3eb5ac784a25271b3433f6371d7b46cdabb420b5"
   },
   "outputs": [],
   "source": [
    "df_train_outliers = df_train[~mask_without_outlier]\n",
    "outlier_card_ids = df_train_outliers['card_id'].values\n",
    "df_train_full = df_train.copy()\n",
    "df_train = df_train[mask_without_outlier]\n",
    "target = df_train['target']\n",
    "del df_train['target']\n",
    "# features = [c for c in df_train.columns if c not in ['card_id', 'first_active_month','outliers']]\n",
    "# categorical_feats = [c for c in features if 'feature_' in c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8256bfde460ac4adc45852228a12194b2be6b497"
   },
   "source": [
    "## parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "d33d87b32ca39f3428f865963df5c3de1945399a"
   },
   "outputs": [],
   "source": [
    "# param = {'num_leaves': 31,\n",
    "#          'min_data_in_leaf': 30, \n",
    "#          'objective':'regression',\n",
    "#          'max_depth': -1,\n",
    "#          'learning_rate': 0.01,\n",
    "#          \"min_child_samples\": 20,\n",
    "#          \"boosting\": \"gbdt\",\n",
    "#          \"feature_fraction\": 0.9,\n",
    "#          \"bagging_freq\": 1,\n",
    "#          \"bagging_fraction\": 0.9 ,\n",
    "#          \"bagging_seed\": 11,\n",
    "#          \"metric\": 'rmse',\n",
    "#          \"lambda_l1\": 0.1,\n",
    "#          \"verbosity\": -1,\n",
    "#          \"nthread\": 4,\n",
    "#          \"random_state\": 4590}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "204231995912b388ad8d6bed1c602c6fa29ebb4a"
   },
   "source": [
    "## training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "d17e1c0e134cb530cb60cc0c11cd61bcbb07f41e"
   },
   "outputs": [],
   "source": [
    "enc_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "5b956835abeb534b5d3404ed65dcb49c38bd6542"
   },
   "outputs": [],
   "source": [
    "def getenc(mask_without_outlier=None):\n",
    "    tr_encs = []\n",
    "    val_encs = []\n",
    "    test_encs = []\n",
    "    Path='../input/elo-output/'\n",
    "#     Path='../input/elo-target-encoding-100-splits/'\n",
    "\n",
    "    for i in range(0,enc_splits):\n",
    "        cur_tr_enc = pd.read_hdf(Path+'train_targetenc_feats'+str(i)+'.hdf')\n",
    "        cur_val_enc = pd.read_hdf(Path+'val_targetenc_feats'+str(i)+'.hdf')\n",
    "\n",
    "        if mask_without_outlier is not None:\n",
    "            cur_tr_enc = cur_tr_enc[mask_without_outlier]\n",
    "            cur_val_enc = cur_val_enc[mask_without_outlier]\n",
    "\n",
    "        tr_encs += [cur_tr_enc]\n",
    "        val_encs +=[ cur_val_enc]\n",
    "\n",
    "        test_encs += [pd.read_hdf(Path+'test_targetenc_feats'+str(i)+'.hdf')]\n",
    "        print('read complete for:',i)\n",
    "        \n",
    "    return tr_encs,val_encs,test_encs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "d832821c1b7de34e0b101634228f827bb386941f"
   },
   "outputs": [],
   "source": [
    "def get_logloss_score(labels,preds):\n",
    "     return log_loss(labels, preds)\n",
    "def get_rmse_score(labels,preds):\n",
    "     return np.sqrt(mean_squared_error(labels, preds))\n",
    "def get_f1loss_score(labels,preds):\n",
    "    optcutoff,f1score = get_opt_cutoff_prec(labels,preds)\n",
    "    f1loss = 1 - f1score\n",
    "    return f1loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "d08f1baa2e4f7fe7c9198207e9ccb77b5dbd03e3"
   },
   "outputs": [],
   "source": [
    "def permutation_feature_selection(model, X_val, y_val, score_function,subset_feats=None,pred_proba=False, rep=3, max_delta_score=0.0001):\n",
    "\n",
    "    # to do: predict_proba from Booster\n",
    "\n",
    "    \"\"\"\"\"\n",
    "    Perform permutation feature importance calculation for trained LightGBM model. \n",
    "    Scorer - ROC AUC. \n",
    "    The lower score with permuted feature - the more important feature is. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : lightGBM.Booster\n",
    "        Trained model to perform feature importance calculation \n",
    "        \n",
    "    X_val : pandas.DataFrame\n",
    "        Validation dataset\n",
    "            \n",
    "    y_val : pandas.Series \n",
    "        Targets for validation dataset\n",
    "       \n",
    "    rep : integer (default = 3)\n",
    "        Number of permutations. More permutations lead to more robust results, but requires more calculation time.\n",
    "        It is recommended to use values in the range [3; 10].\n",
    "        \n",
    "    min_delta_score : float (default = 0.0)\n",
    "        Minimum delta ins score to keep feature \n",
    "        It is recommended to use values in the range [-0.0001; 0].\n",
    "        \n",
    "    Output\n",
    "    ----------\n",
    "    selected_features : list\n",
    "        Important features \n",
    "        \n",
    "    importance_df : pandas.DataFrame \n",
    "        Records of calculations \n",
    "    \"\"\"\"\"\n",
    "\n",
    "    print('Permutation feature importance is calculating...')\n",
    "\n",
    "    columns = []\n",
    "    scores = []\n",
    "    stds = []\n",
    "    score_max = []\n",
    "    score_min = []\n",
    "\n",
    "    # calculate the score of model with no permuted features, this is our baseline\n",
    "    if pred_proba:\n",
    "        y_hat_nopert = model.predict_proba(X_val)[:,1]\n",
    "    else:\n",
    "        y_hat_nopert = model.predict(X_val,model.best_iteration_)\n",
    "    score_init = score_function(y_val, y_hat_nopert)\n",
    "#     score_init = np.sqrt(mean_squared_error(y_val, y_hat_nopert))\n",
    "#     print('score init:',score_init)\n",
    "    col_iter = 1\n",
    "    \n",
    "    if subset_feats!=None:\n",
    "        cols = subset_feats\n",
    "    else:\n",
    "        cols = X_val.columns\n",
    "    \n",
    "    for cc in tqdm(cols):\n",
    "#         if col_iter > 3:\n",
    "#             break\n",
    "        scores_cc = []\n",
    "        for seed in range(rep):\n",
    "            # shuffle single column of dataset\n",
    "            data_temp = X_val.copy()\n",
    "            data_temp[cc] = np.nan\n",
    "#             data_temp[cc] = data_temp[cc].sample(n=X_val.shape[0], random_state=seed).reset_index(drop=True)\n",
    "\n",
    "            # make prediction on 'shuffled' dataset and score it\n",
    "            if pred_proba:\n",
    "                # calibration classifier fit\n",
    "#                 model.fit(data_temp, y_val)\n",
    "                y_hat = model.predict_proba(data_temp)[:,1]\n",
    "            else:\n",
    "                y_hat = model.predict(data_temp,model.best_iteration_)\n",
    "            score = score_function(y_val, y_hat)\n",
    "#             print('col:',cc)\n",
    "#             print('score new:',score)\n",
    "#             score = np.sqrt(mean_squared_error(y_val, y_hat))\n",
    "            scores_cc.append(score)\n",
    "\n",
    "        columns.append(cc)\n",
    "        scores.append(np.mean(scores_cc) - score_init)\n",
    "        stds.append(np.std(scores_cc))\n",
    "        score_max.append(np.max(scores_cc) - score_init)\n",
    "        score_min.append(np.min(scores_cc) - score_init)\n",
    "        \n",
    "        col_iter+=1\n",
    "\n",
    "    importance_df = pd.DataFrame({'delta_score_mean': scores,\n",
    "                                  'delta_score_std': stds,\n",
    "                                  'delta_score_max': score_max,\n",
    "                                  'delta_score_min': score_min,\n",
    "                                  'feature': columns\n",
    "                                  })\n",
    "    importance_df = importance_df.sort_values(by='delta_score_mean')\n",
    "    selected_features = list(importance_df.loc[importance_df['delta_score_mean'] >= max_delta_score, 'feature'])\n",
    "\n",
    "    print('Permutation feature importance calculation is done. Overall number of features: ', importance_df.shape[0],\n",
    "          'Number of selected features:', len(selected_features))\n",
    "\n",
    "    return selected_features, importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "f443d124a14c70ccf50725eb47ddcb7eda299276"
   },
   "outputs": [],
   "source": [
    "def getenccolname(colname,cols_agg):\n",
    "    if 'var' in cols_agg:\n",
    "        colname =\"targetvarenc_\"+colname\n",
    "    elif 'std' in cols_agg:\n",
    "        colname =\"targetstdenc_\"+colname \n",
    "    elif 'sum' in cols_agg:   \n",
    "        colname =\"targetsumenc_\"+colname\n",
    "    elif 'min' in cols_agg:   \n",
    "        colname =\"targetminenc_\"+colname\n",
    "    elif 'max' in cols_agg:   \n",
    "        colname =\"targetmaxenc_\"+colname\n",
    "    elif 'median' in cols_agg:   \n",
    "        colname =\"targetmedianenc_\"+colname\n",
    "    elif 'count' in cols_agg:   \n",
    "        colname =\"targetcountenc_\"+colname\n",
    "    elif 'iqmean' in cols_agg:   \n",
    "        colname =\"targetiqmeanenc_\"+colname\n",
    "    else:\n",
    "        colname =\"targetenc_\"+colname\n",
    "    return colname\n",
    "def computeexpcomponent(countSeries,min_samples_leaf,smoothing):\n",
    "    return -((countSeries - min_samples_leaf) / smoothing)        \n",
    "def getexpcomponent(countSeries,min_samples_leaf,smoothing):\n",
    "    expcomponent = computeexpcomponent(countSeries,min_samples_leaf,smoothing)\n",
    "    return expcomponent\n",
    "def performsmoothing(averages,targetcolname,train,agg,countSeries,smoothing,min_samples_leaf,noise_level,global_agg_val=None):\n",
    "        expcomponent = getexpcomponent(countSeries,min_samples_leaf,smoothing)\n",
    "        smoothing_v = 1 / (1 + np.exp(expcomponent) )\n",
    "        \n",
    "        if global_agg_val is None:\n",
    "            global_agg_val = np.nanmean(train[targetcolname].values)\n",
    "            \n",
    "        newcol ='newcol'\n",
    "        averages[newcol] = global_agg_val * (1 - smoothing_v) + averages[agg] * smoothing_v\n",
    "\n",
    "        np.random.seed(42)\n",
    "        noise = np.random.randn(len(averages[newcol])) * noise_level\n",
    "        print('noise mean:{0} std:{1} min:{2} max:{3}'.format(np.mean(noise),np.std(noise),np.min(noise),np.max(noise)))\n",
    "        averages[newcol] = averages[newcol] + noise\n",
    "        \n",
    "        del smoothing_v,noise;gc.collect()\n",
    "        return averages[newcol]\n",
    "             \n",
    "def targetenc(train,test,val,catcolnames,targetcolname,\n",
    "             smoothing,min_samples_leaf,noise_level):\n",
    "    start = time.time()\n",
    "    \n",
    "    for i,curcol in enumerate(catcolnames):\n",
    "        enccol = getenccolname(curcol,'mean')\n",
    "#         print('enccol:',enccol)\n",
    "        averages = train[[curcol,targetcolname]].groupby(curcol).agg({targetcolname: \"mean\",\n",
    "                                                                     curcol: 'size'})\n",
    "        averages.columns =['mean','count']\n",
    "        print('curcol:',curcol)\n",
    "        print('averages count describe:',averages['count'].describe())\n",
    "\n",
    "        #Multiplication factor for each enc col since target enc is having very low value and noise will change the actual value itself\n",
    "        averages['mean'] = 1000 * averages['mean']\n",
    "        #Perform smoothing\n",
    "        q25 = averages['count'].quantile(0.05)\n",
    "        min_samples_leaf = np.max(np.array([25,q25]))\n",
    "#         if min_samples_leaf > 5000:\n",
    "#             min_samples_leaf=1000\n",
    "#         elif min_samples_leaf > 1000:\n",
    "#             min_samples_leaf = 1000\n",
    "        min_samples_leaf = np.min(np.array([1000,min_samples_leaf]))\n",
    "        print('min_samples_leaf:',min_samples_leaf)\n",
    "        #compute cumulative mean:\n",
    "        averages.sort_values('count',inplace=True)\n",
    "        agg='cum_mean'\n",
    "        averages[agg] = averages['mean'].expanding().mean()\n",
    "        \n",
    "        averages[enccol] = performsmoothing(averages,targetcolname,train,agg,averages['count'],\n",
    "                                          smoothing[i],min_samples_leaf,noise_level[i])\n",
    "        averages.drop(['mean','count',agg],axis=1,inplace=True)\n",
    "\n",
    "        print('averages enccol describe:',averages[enccol].describe())\n",
    "#         print('train curcol head:',train[curcol].head())\n",
    "        # Use only the null merchant id enccol records to update the curcol encoding\n",
    "        train[enccol] = train[curcol].map(averages[enccol])\n",
    "        val[enccol] = val[curcol].map(averages[enccol])\n",
    "        test[enccol] = test[curcol].map(averages[enccol])\n",
    "        \n",
    "        #Fill NA\n",
    "        globalmean  = train[enccol].mean()\n",
    "        train[enccol].fillna(globalmean,inplace=True)\n",
    "        val[enccol].fillna(globalmean,inplace=True)\n",
    "        test[enccol].fillna(globalmean,inplace=True)\n",
    "    \n",
    "    end = time.time()\n",
    "    print('Target Enc Generation exec time:',end- start)\n",
    "    \n",
    "    return train,test,val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "2967642dfc3cd5de69e27c8fc00a25a0f6d0e566"
   },
   "outputs": [],
   "source": [
    "def droptargetenccols(train, val,test):\n",
    "     #remove target encoding fields if present\n",
    "    enccols_train = [col for col in train.columns if ('targetenc' in col) & ('merchant_id' not in col)]\n",
    "    train.drop(enccols_train,axis=1,inplace=True)\n",
    "    val.drop(enccols_train,axis=1,inplace=True)\n",
    "    enccols_test = [col for col in test.columns if ('targetenc' in col) & ('merchant_id' not in col)]\n",
    "    test.drop(enccols_test,axis=1,inplace=True)\n",
    "    \n",
    "    return train, val,test\n",
    "def targetencprocess(tr,val,test,catcolnames,fold_):\n",
    "    #drop any existing target enc cols\n",
    "    tr,val,test = droptargetenccols(tr,val,test)\n",
    "    targetcolname='outliers'\n",
    "    \n",
    "    smoothing=[]\n",
    "    min_samples_leaf=[]\n",
    "    noise_level=[]\n",
    "    for col in catcolnames:\n",
    "        smoothing +=  [50] \n",
    "        min_samples_leaf +=  [100] \n",
    "        noise_level +=  [0.1] \n",
    "    #target encoding on transaction merchant id\n",
    "    tr,val,test = targetenc(tr, val,test,catcolnames,targetcolname,\n",
    "                           smoothing,min_samples_leaf,noise_level)\n",
    "\n",
    "    enc_cols_for_sum = [col for col in tr.columns if ('targetenc' in col) and ('merchant_id' not in col)]\n",
    "    #weighted sum\n",
    "#     weights = [ 0.042229,0.058662,0.055087,0.030753,0.015725,0.047265,0.047882,0.063172,0.005299,0.052449,0.041647,0.05688,\n",
    "#                 0.045107,0.040973,0.054825,0.026013,0.039557,0.018088,0.012036,0.0354,0.061585,0.035343,0.039157]\n",
    "    \n",
    "#     enc_cols_for_sum.sort()\n",
    "    \n",
    "#     for df in [tr,val,test]:\n",
    "#         df['sum_targetenc'] = 0\n",
    "#         for i,col in enumerate(enc_cols_for_sum):\n",
    "#             print('col:',col)\n",
    "#             print('dtype:',df[col].dtype)\n",
    "# #             print('df[col].describe:',df[col].describe)\n",
    "#             df['sum_targetenc'] += df[col] * weights[i]\n",
    "#         df['sum_targetenc'] = 100 * df['sum_targetenc'] / np.sum(np.array(weights))\n",
    "        \n",
    "    for df in [tr,val,test]:\n",
    "        df['sum_targetenc'] = df[enc_cols_for_sum].sum(axis=1)\n",
    "#         df['mean_targetenc'] = df[enc_cols_for_sum].mean(axis=1)\n",
    "#         df['std_targetenc'] = df[enc_cols_for_sum].std(axis=1)\n",
    "#         df['min_targetenc'] = df[enc_cols_for_sum].min(axis=1)\n",
    "#         df['max_targetenc'] = df[enc_cols_for_sum].max(axis=1)\n",
    "    enc_cols = [col for col in tr.columns if ('targetenc' in col) and ('merchant_id' not in col)]\n",
    "\n",
    "#     print('enc cols:',enc_cols)\n",
    "    print('save encoding feats...')\n",
    "    #save target encoding features in separate file\n",
    "    \n",
    "#     print(list(tr[['card_id']+enc_cols].columns))\n",
    "#     print(len(list(tr[['card_id']+enc_cols].columns)))\n",
    "\n",
    "    \n",
    "    tr[['card_id']+enc_cols].to_hdf('train_targetenc_feats'+str(fold_)+'.hdf',key='data')\n",
    "    val[['card_id']+enc_cols].to_hdf('val_targetenc_feats'+str(fold_)+'.hdf',key='data')\n",
    "    test[['card_id']+enc_cols].to_hdf('test_targetenc_feats'+str(fold_)+'.hdf',key='data')\n",
    "\n",
    "    return tr,val,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "49c0a7b48e726c603ebca46ef2e0e1533097bc62"
   },
   "outputs": [],
   "source": [
    "def computef1scoreandconfmatrix(y_true,y_preds):    \n",
    "    opt_cutoff, f1score = get_opt_cutoff_prec(y_true,y_preds)\n",
    "    print('opt_cutoff:',opt_cutoff)\n",
    "    print('f1 score:',f1score)\n",
    "    pred_labels = convert_probtolabels(y_preds,cutoff=opt_cutoff)\n",
    "    print('conf matrix:',confusion_matrix(y_true,pred_labels))\n",
    "    return opt_cutoff,f1score,pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "898eb60e7399b93e4736f360258325e0606156b1"
   },
   "outputs": [],
   "source": [
    "def lgb_fit(regression,param,tr,y_tr,val,y_val,cur_feval):\n",
    "    if regression:\n",
    "        model = lgb.LGBMRegressor(**param)\n",
    "    else:\n",
    "        model = lgb.LGBMClassifier(**param)\n",
    "        \n",
    "    model.fit(tr,y_tr,eval_set=[(val, y_val)],\n",
    "                    early_stopping_rounds=200,\n",
    "                    verbose=100,\n",
    "                    eval_metric=cur_feval,\n",
    "             )\n",
    "    return model\n",
    "def lgb_predict(model,n_estimators,test):\n",
    "    preds= model.predict(test,num_iteration = getbestiteration(model,n_estimators))\n",
    "    return preds\n",
    "# def xgb_fit(regression,param,tr,y_tr,val,y_val,cur_feval):\n",
    "#     num_round= param['n_estimators']\n",
    "#     trn_data = xgb.DMatrix(data=tr, label=y_tr)\n",
    "#     val_data = xgb.DMatrix(data=val, label=y_val)\n",
    "#     watchlist = [(trn_data, 'train'), (val_data, 'valid')]\n",
    "\n",
    "#     model = xgb.train(param, trn_data, num_round, watchlist, \n",
    "#                       early_stopping_rounds=200, verbose_eval=100)\n",
    "\n",
    "#     return model\n",
    "\n",
    "def xgb_fit(regression,param,tr,y_tr,val,y_val,cur_feval):\n",
    "    if regression:\n",
    "        model = xgb.XGBRegressor(**param)\n",
    " \n",
    "    else:\n",
    "        model = xgb.XGBClassifier(**param)\n",
    "\n",
    "    model.fit(tr,y_tr,eval_set=[(val, y_val)],\n",
    "                    early_stopping_rounds=200,\n",
    "                    verbose=50,\n",
    "                    eval_metric=cur_feval,\n",
    "             )\n",
    "    return model\n",
    "def xgb_predict(model,n_estimators,test):\n",
    "    preds= model.predict(test)\n",
    "#     preds = model.predict(xgb.DMatrix(test), ntree_limit=model.best_ntree_limit)\n",
    "    return preds\n",
    "def getbestiteration(model,n_estimators):\n",
    "    if hasattr(model, 'best_iteration'):\n",
    "        if model.best_iteration is None:\n",
    "            return n_estimators\n",
    "        else:\n",
    "            return model.best_iteration\n",
    "    elif hasattr(model, 'best_iteration_'):\n",
    "        if model.best_iteration_ is None:\n",
    "            return n_estimators\n",
    "        else:\n",
    "            return model.best_iteration_\n",
    "    \n",
    "    return -1\n",
    "\n",
    "def xgb_getbestscore(model,cur_feval):\n",
    "    if isinstance(model,xgb.XGBRegressor) :\n",
    "#         print('model best_score object:',model.get_booster().best_score)\n",
    "        score= model.get_booster().best_score\n",
    "    elif isinstance(model,xgb.XGBClassifier):\n",
    "        print('model best_score object:',model.get_booster().best_score)\n",
    "        score= model.get_booster().best_score\n",
    "       \n",
    "    return score\n",
    "\n",
    "def lgb_getbestscore(model,cur_feval):\n",
    "    if  isinstance(model,lgb.LGBMRegressor) :\n",
    "        score= model.best_score_['valid_0'][param['metric']]\n",
    "    else: #isinstance(model,lgb.LGBMClassifier) :\n",
    "        if (cur_feval is None):\n",
    "            score= model.best_score_['valid_0'][param['metric']]\n",
    "        else:\n",
    "            score= model.best_score_['valid_0']['f1_score']\n",
    "    return score\n",
    "# def xgb_getbestscore(model,cur_feval):\n",
    "#     return model.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "da777d8641e67e90d9064bcdd9e1e8bf5ca5b7c3"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def runlgb(random_state,ispermutefeats,train,test,param,features,\n",
    "           y_train, fit_function,predict_function,bestscore_function,\n",
    "           score_function=None,regression=True,\n",
    "           train_outlier=None,likely_card_ids=None,targetenc=False,to_enc_cols=None,\n",
    "          subset_feats = None, fold_feats=False, feval=None,\n",
    "           fold_to_start=None,\n",
    "          fold_to_stop = None):\n",
    "\n",
    "#     global oof_rank,oof_rank_test,oof_rank_prob,oof_rank_p_labels, oof_rank_p_labels_test\n",
    "#     global oof_labels, oof_labels_test\n",
    "    global oof_rank,oof_rank_prob,oof_rank_p_labels\n",
    "    global oof_labels\n",
    "    global oof_rank_p_prob,oof_rank_p_prob_prop_high, oof_rank_p_prob_prop_low\n",
    "    \n",
    "    n_estimators = param['n_estimators']\n",
    "    overall_sel_feats =[]\n",
    "    overall_imp_df = pd.DataFrame()\n",
    "    \n",
    "    if subset_feats == None:\n",
    "        overall_imp_df['feature']= np.array(features)\n",
    "    else:\n",
    "        overall_imp_df['feature']= np.array(subset_feats)\n",
    "        \n",
    "    overall_imp_df['overall_score_mean'] =0 \n",
    "    overall_imp_df['overall_score_max'] =-9999 \n",
    "    overall_imp_df['overall_score_min'] =9999 \n",
    "    \n",
    "    oof = np.zeros(len(train))\n",
    "    oof_rank = np.zeros(len(train))\n",
    "    oof_rank_prob = np.zeros(len(train))\n",
    "    oof_rank_p_labels = np.zeros(len(train))    \n",
    "    oof_labels = np.zeros(len(train))\n",
    "    oof_rank_p_prob = np.zeros(len(train))\n",
    "    oof_rank_p_prob_prop_high = np.zeros(len(train))\n",
    "    oof_rank_p_prob_prop_low = np.zeros(len(train))\n",
    "    \n",
    "    \n",
    "    predictions_train= np.zeros(len(train))\n",
    "    predictions_train_w= np.zeros(len(train))\n",
    "    predictions_train_mul_prob= np.ones(len(train))\n",
    "    predictions_train_mul_prob_inv= np.ones(len(train))\n",
    "    goodfolds =0\n",
    "    train_outlier_preds = None\n",
    "    if train_outlier is not None:\n",
    "        train_outlier_preds= np.zeros(len(train_outlier)) \n",
    "    predictions = np.zeros(len(test))\n",
    "    start = time.time()\n",
    "    valid_scores =[]\n",
    "    valid_auc_scores=[]\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = features\n",
    "    fold_importance_df[\"importance\"] =0    \n",
    "    fold_importance_df[\"gain\"] =0\n",
    "    val_preds_cum = None\n",
    "    y_val_cum = None    \n",
    "\n",
    "    folds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    if regression:\n",
    "        indices =  folds.split(train.values,train['outliers'].values)   \n",
    "    else:\n",
    "        indices = folds.split(train.values, y_train.values)\n",
    "        \n",
    "#     combined_encs = pd.concat([tr_encs[0],val_encs[0]])\n",
    "#     train_with_enc = pd.concat([train,combined_encs[sel_enc_cols[0]]],axis=1)\n",
    "#     train_with_enc = train_with_enc[features]\n",
    "#     del combined_encs;gc.collect()\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(indices):\n",
    "        if (fold_to_stop is not None):\n",
    "            if (fold_ >=fold_to_stop):\n",
    "                break\n",
    "                \n",
    "        if (fold_to_start is not None):\n",
    "            if (fold_ < fold_to_start):\n",
    "                continue\n",
    "        \n",
    "        print()\n",
    "        print(\"fold nÂ°{}\".format(fold_))\n",
    "        \n",
    "        if fold_feats:\n",
    "            cur_features = features[fold_].copy()\n",
    "        else:\n",
    "            cur_features = features.copy()\n",
    "            \n",
    "        tr = train.iloc[trn_idx]\n",
    "        val = train.iloc[val_idx]\n",
    "        print('y_train shape:',y_train.shape)\n",
    "        y_val = y_train.iloc[val_idx]\n",
    "        print('val shape:',val.shape)\n",
    "        print('y_val shape:',y_val.shape)\n",
    "        y_tr = y_train.iloc[trn_idx]\n",
    "        \n",
    "        \n",
    "        #Concat current encoding train, valid and test files\n",
    "        if fold_ < enc_splits:\n",
    "            cur_tr_encs = tr_encs[fold_]\n",
    "            cur_val_encs= val_encs[fold_]\n",
    "            cur_test_encs= test_encs[fold_]\n",
    "        \n",
    "        if train_outlier is not None:\n",
    "            cur_tr_encs = cur_tr_encs[~cur_tr_encs['card_id'].isin(outlier_card_ids)]\n",
    "            cur_val_encs= cur_val_encs[~cur_val_encs['card_id'].isin(outlier_card_ids)]\n",
    "        elif likely_card_ids is not None:\n",
    "               \n",
    "            cur_tr_encs = pd.concat([tr_encs[0],val_encs[0]])\n",
    "            cur_val_encs = val_encs[0]\n",
    "            for i in range(1,5):\n",
    "                cur_val_encs = pd.concat([cur_val_encs,val_encs[i]])\n",
    "        \n",
    "            tr_orig_card_ids = tr['card_id']\n",
    "            val_orig_card_ids = val['card_id']\n",
    "            test_orig_card_ids = test['card_id']\n",
    "            \n",
    "        tr=pd.concat([tr,cur_tr_encs[sel_enc_cols[0]]],axis=1)\n",
    "        val=pd.concat([val,cur_val_encs[sel_enc_cols[0]]],axis=1)\n",
    "        test_cur=pd.concat([test,cur_test_encs[sel_enc_cols[0]]],axis=1)\n",
    "        \n",
    "        #remove extra card ids\n",
    "        if likely_card_ids is not None:\n",
    "            tr_mask = tr['card_id'].isin(tr_orig_card_ids)\n",
    "            tr = tr[tr_mask]\n",
    "            val_mask = val['card_id'].isin(val_orig_card_ids)\n",
    "            val = val[val_mask]\n",
    "            test_mask = test_cur['card_id'].isin(test_orig_card_ids)\n",
    "            test_cur = test_cur[test_mask]\n",
    "            \n",
    "        print('tr shape:',tr.shape)\n",
    "        print('val shape:',val.shape)\n",
    "        if regression==False:\n",
    "            print('yval 1 shape:',y_val[y_val==1].shape)\n",
    "            print('yval 0 shape:',y_val[y_val==0].shape)\n",
    "            print('yval 1 ratio:',y_val[y_val==1].shape[0] / (y_val[y_val==1].shape[0]+y_val[y_val==0].shape[0]))\n",
    "#         print('tr columns:',list(tr.columns))\n",
    "        #Target encoding\n",
    "        if targetenc:\n",
    "            tr,val,test_cur= targetencprocess(tr,val,test_cur,to_enc_cols,fold_)\n",
    "#             enccols = [col for col in tr.columns if ('targetenc' in col) & ('merchant_id' not in col)]\n",
    "            enccols = ['sum_targetenc']\n",
    "            print('enccols:',enccols)\n",
    "            cur_features += enccols\n",
    "            \n",
    "#         print('cur_features',cur_features)\n",
    "        \n",
    "        trn_data = lgb.Dataset(tr[cur_features], label=y_tr)#,, categorical_feature=categorical_feats)\n",
    "        val_data = lgb.Dataset(val[cur_features], label=y_val)#,, categorical_feature=categorical_feats)\n",
    "        \n",
    "        cur_feval = feval\n",
    "#         if fold_ in [4]:\n",
    "#             cur_feval=None\n",
    "            \n",
    "        base_model =fit_function(regression,param,tr[cur_features],y_tr,val[cur_features],y_val,cur_feval)\n",
    "        if regression:\n",
    "            clf = base_model\n",
    "        else:\n",
    "            clf = CalibratedClassifierCV(base_model,cv='prefit',method='sigmoid') #isotonic\n",
    "            clf.fit(val[cur_features], y_val)\n",
    "#             if ispermutefeats:\n",
    "#                 clf.fit(tr[cur_features], y_tr)\n",
    "#             else:\n",
    "#                 clf.fit(val[cur_features], y_val)\n",
    "\n",
    "        \n",
    "\n",
    "        #Prediction based on current fold selected features\n",
    "        if ispermutefeats:\n",
    "            \n",
    "            selected_features, importance_df = permutation_feature_selection(clf, val[cur_features], \n",
    "                                                                             y_val,score_function,\n",
    "                                                                             subset_feats=subset_feats,\n",
    "                                                                             rep=4,max_delta_score=max_delta_score,\n",
    "                                                                            pred_proba = not regression)\n",
    "            overall_sel_feats += [selected_features]\n",
    "            print(selected_features)\n",
    "\n",
    "#             print('overal imp shape:{0} importance_df shape:{1}'.format(overall_imp_df.shape,importance_df.shape))\n",
    "            \n",
    "            overall_imp_df['fold_'+str(fold_)+'score_mean'] = importance_df['delta_score_mean']\n",
    "            overall_imp_df['fold_'+str(fold_)+'score_max'] = importance_df['delta_score_max']\n",
    "            overall_imp_df['fold_'+str(fold_)+'score_min'] = importance_df['delta_score_min']\n",
    "        else:\n",
    "            \n",
    "            if regression:\n",
    "                val_preds =predict_function(clf,n_estimators,val[cur_features])\n",
    "#                 val_preds = clf.predict(val[cur_features], num_iteration=getbestiteration(clf,n_estimators))\n",
    "                oof[val_idx] = val_preds\n",
    "            else:\n",
    "                val_preds = clf.predict_proba(val[cur_features])[:,1]\n",
    "                oof[val_idx] = val_preds\n",
    "                opt_cutoff, f1score = get_opt_cutoff_prec(y_val,val_preds)\n",
    "                print('opt_cutoff:',opt_cutoff)\n",
    "                print('f1 score after calib:',f1score)\n",
    "                auc_score = roc_auc_score(y_val,val_preds)\n",
    "                print('auc score after calib:',auc_score)\n",
    "                print(\"log loss score after calib: {:<8.5f}\".format(log_loss(y_val,val_preds)))\n",
    "    \n",
    "\n",
    "                oof_labels[val_idx] = convert_probtolabels(val_preds,cutoff=opt_cutoff)\n",
    "    \n",
    "                rank_df = pd.DataFrame()\n",
    "                rank_df['preds']= val_preds\n",
    "                rank_df['rank']= rank_df['preds'].rank(pct=True)\n",
    "                oof_rank[val_idx] = rank_df['rank'].values\n",
    "                oof_rank_p_labels[val_idx] = rank_df['rank'].values + oof_labels[val_idx]\n",
    "                oof_rank_prob[val_idx] = rank_df['rank'].values * val_preds\n",
    "                oof_rank_p_prob[val_idx] = rank_df['rank'].values + val_preds\n",
    "                oof_rank_p_prob_prop_high[val_idx] = rank_df['rank'].values + 10 * val_preds\n",
    "                oof_rank_p_prob_prop_low[val_idx] = rank_df['rank'].values + 0.25 * val_preds\n",
    "                \n",
    "                if val_preds_cum is None:\n",
    "                    val_preds_cum = val_preds\n",
    "                    y_val_cum = y_val\n",
    "                else:\n",
    "                    val_preds_cum = np.concatenate((val_preds_cum, val_preds), axis=0)\n",
    "                    y_val_cum = np.concatenate((y_val_cum, y_val), axis=0)\n",
    "                \n",
    "                print('CUMULATIVE SCORE')\n",
    "                computef1scoreandconfmatrix(y_val_cum,val_preds_cum) \n",
    "\n",
    "    #             lgb_prediction = clf.predict_proba(val[cur_features], num_iteration=clf.best_iteration)\n",
    "#             print('lgb pred head:',lgb_prediction[0:10])\n",
    "#             lgb_prediction = lgb_prediction.reshape(-1, 2)\n",
    "#             oof[val_idx]   =  lgb_prediction.argmax(axis = 1)\n",
    "\n",
    "            if train_outlier is not None:\n",
    "                outlier_tr_encs = tr_encs[fold_][tr_encs[fold_]['card_id'].isin(outlier_card_ids)]\n",
    "                trcardids = outlier_tr_encs['card_id']\n",
    "                train_outlier.loc[train_outlier['card_id'].isin(trcardids),sel_enc_cols[0]]\\\n",
    "                    = outlier_tr_encs[sel_enc_cols[0]]\n",
    "                outlier_val_encs = val_encs[fold_][val_encs[fold_]['card_id'].isin(outlier_card_ids)]\n",
    "                valcardids = outlier_val_encs['card_id']\n",
    "                train_outlier.loc[train_outlier['card_id'].isin(valcardids),sel_enc_cols[0]]\\\n",
    "                    = outlier_val_encs[sel_enc_cols[0]]\n",
    "            #             print('train_outlier[cur_features].shape',train_outlier[cur_features].shape)\n",
    "                if regression:\n",
    "                    train_outlier_preds += predict_function(clf,n_estimators,train_outlier[cur_features]) / folds.n_splits\n",
    "                else:\n",
    "                    train_outlier_preds += clf.predict_proba(train_outlier[cur_features])[:,1]\n",
    "                \n",
    "\n",
    "#             if fold_==0:\n",
    "#                 fold_importance_df[\"feature\"] = cur_features\n",
    "#                 fold_importance_df[\"importance\"] =0\n",
    "\n",
    "#             fold_importance_df[\"importance\"] += base_model.booster_.feature_importance(importance_type='split') / folds.n_splits\n",
    "#             fold_importance_df[\"gain\"] += base_model.booster_.feature_importance(importance_type='gain') / folds.n_splits\n",
    "            if regression:\n",
    "                valid_scores+=[bestscore_function(clf,cur_feval)]\n",
    "#                 valid_scores+=[clf.best_score_['valid_0'][param['metric']]]\n",
    "                predictions += predict_function(clf,n_estimators,test_cur[cur_features]) / folds.n_splits\n",
    "            else:\n",
    "            \n",
    "                valid_score =bestscore_function(base_model,cur_feval)\n",
    "#                 if cur_feval is not None:\n",
    "#                     valid_score = base_model.best_score_['valid_0']['f1_score']\n",
    "#                 else:\n",
    "#                     valid_score=base_model.best_score_['valid_0'][param['metric']]\n",
    "                    \n",
    "                valid_scores+=[valid_score]\n",
    "                valid_auc_scores+=[auc_score]\n",
    "#                 predictions += clf.predict_proba(test_cur[cur_features])[:,1] / folds.n_splits\n",
    "                cur_tr_encs = pd.concat([tr_encs[0],val_encs[0]])\n",
    "                \n",
    "#                 cur_train_preds = clf.predict_proba(train_with_enc)[:,1]\n",
    "#                 predictions_train += cur_train_preds / folds.n_splits\n",
    "                \n",
    "#                 predictions_train_w += valid_score * cur_train_preds\n",
    "#                 predictions_train_mul_prob  *= cur_train_preds             \n",
    "#                 predictions_train_mul_prob_inv  *= (1-cur_train_preds)\n",
    "\n",
    "                predictions += clf.predict_proba(test_cur[cur_features])[:,1] / folds.n_splits\n",
    "\n",
    "\n",
    "    if ispermutefeats:\n",
    "        fold_mean_cols = [col for col in overall_imp_df.columns if ('score_mean' in col) and ('fold_' in col) ]\n",
    "        fold_max_cols = [col for col in overall_imp_df.columns if ('score_max' in col) and ('fold_' in col) ]\n",
    "        fold_min_cols = [col for col in overall_imp_df.columns if ('score_min' in col) and ('fold_' in col) ]\n",
    "        overall_imp_df['overall_score_mean'] = overall_imp_df[fold_mean_cols].mean(axis=1)\n",
    "        overall_imp_df['overall_score_max'] = overall_imp_df[fold_max_cols].max(axis=1)\n",
    "        overall_imp_df['overall_score_min'] = overall_imp_df[fold_min_cols].min(axis=1)\n",
    "    else:\n",
    "        print('valid scores:',valid_scores)\n",
    "        if regression:\n",
    "            print(\"CV score: {:<8.5f}\".format(mean_squared_error(oof, y_train)**0.5))\n",
    "        else:\n",
    "            print('CUMULATIVE SCORE')\n",
    "            computef1scoreandconfmatrix(y_val_cum,val_preds_cum) \n",
    "            print()\n",
    "            print(\"CV score log loss: {:<8.5f}\".format(log_loss(y_train, oof)))\n",
    "            opt_cutoff, f1score = get_opt_cutoff_prec(y_train, oof)\n",
    "            print(\"CV score f1 score: {0} cutoff:{1}\".format(f1score,opt_cutoff))\n",
    "            auc_score = roc_auc_score(y_train, oof)\n",
    "            print(\"CV score AUC score: {0}\".format(auc_score))\n",
    "                \n",
    "            opt_cutoff, f1score = get_opt_cutoff_prec(y_train, oof_rank)\n",
    "            print(\"Rank CV score f1 score: {0} cutoff:{1}\".format(f1score,opt_cutoff))\n",
    "            opt_cutoff, f1score = get_opt_cutoff_prec(y_train, oof_rank_prob)\n",
    "            print(\"Rank Prob CV score f1 score: {0} cutoff:{1}\".format(f1score,opt_cutoff))\n",
    "            opt_cutoff, f1score = get_opt_cutoff_prec(y_train, oof_rank_p_prob)\n",
    "            print(\"Rank Plus Prob CV score f1 score: {0} cutoff:{1}\".format(f1score,opt_cutoff))\n",
    "            opt_cutoff, f1score = get_opt_cutoff_prec(y_train, oof_rank_p_prob_prop_high)\n",
    "            print(\"Rank Plus Prob Prop High CV score f1 score: {0} cutoff:{1}\".format(f1score,opt_cutoff))\n",
    "            opt_cutoff, f1score = get_opt_cutoff_prec(y_train, oof_rank_p_prob_prop_low)\n",
    "            print(\"Rank Plus Prob Prop Low CV score f1 score: {0} cutoff:{1}\".format(f1score,opt_cutoff))\n",
    "            \n",
    "            opt_cutoff, f1score = get_opt_cutoff_prec(y_train, oof_labels)\n",
    "            print(\"Labels CV score f1 score: {0} cutoff:{1}\".format(f1score,opt_cutoff))\n",
    "            opt_cutoff, f1score = get_opt_cutoff_prec(y_train, oof_rank_p_labels)\n",
    "            print('Labels plus rank head:',oof_rank_p_labels[0:10])\n",
    "            print(\"Labels Plus Rank f1 score: {0} cutoff:{1}\".format(f1score,opt_cutoff))\n",
    "            print(\"Labels Plus Rank AUC score: {0} \".format(roc_auc_score(y_train,  oof_rank_p_labels)))\n",
    "    \n",
    "            opt_cutoff, f1score = get_opt_cutoff_prec(y_train, (oof_rank + oof) / 2)\n",
    "            print(\"Normal and Rank Average score f1 score: {0} cutoff:{1}\".format(f1score,opt_cutoff))\n",
    "            oof_labels_p_prob = (oof_labels + oof) / 2\n",
    "            opt_cutoff, f1score = get_opt_cutoff_prec(y_train, oof_labels_p_prob)\n",
    "            print(\"Normal and Labels Average score f1 score: {0} cutoff:{1}\".format(f1score,opt_cutoff))\n",
    "            print(\"Normal and Labels Average score AUC score: {0}\".format( roc_auc_score(y_train,  oof_labels_p_prob)))\n",
    "            oof_rank_p_labels_p_prob = (oof_labels + oof + oof_rank) / 2\n",
    "            opt_cutoff, f1score = get_opt_cutoff_prec(y_train, oof_rank_p_labels_p_prob)\n",
    "            print(\"Normal and Labels and Rank Average score f1 score: {0} cutoff:{1}\".format(f1score,opt_cutoff))\n",
    "            print(\"Normal and Labels and Rank Average score AUC score: {0} \".format(roc_auc_score(y_train,  oof_rank_p_labels_p_prob)))\n",
    "            \n",
    "#             opt_cutoff, f1score = get_opt_cutoff_prec(y_train, predictions_train )\n",
    "#             print(\"Train CV Avg score f1 score: {0} cutoff:{1}\".format(f1score,opt_cutoff))\n",
    "            \n",
    "#             opt_cutoff, f1score = get_opt_cutoff_prec(y_train, predictions_train_w)\n",
    "#             print(\"Train CV Weighted Avg score f1 score: {0} cutoff:{1}\".format(f1score,opt_cutoff))\n",
    "#             predictions_train_bayesian = predictions_train_mul_prob / (predictions_train_mul_prob + predictions_train_mul_prob_inv)\n",
    "#             opt_cutoff, f1score = get_opt_cutoff_prec(y_train, predictions_train_bayesian)\n",
    "#             print(\"Train CV Bayesian score f1 score: {0} cutoff:{1}\".format(f1score,opt_cutoff))\n",
    "   \n",
    "    return fold_importance_df,predictions,oof,\\\n",
    "        train_outlier_preds,overall_imp_df,overall_sel_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "6dca614abad0408c408e0dfa3f992ad8866674af"
   },
   "outputs": [],
   "source": [
    "sel_feats = ['feature_1',\n",
    " 'feature_2',\n",
    " 'feature_3',\n",
    " 'hist_authorized_flag_mean',\n",
    " 'hist_authorized_flag_sum',\n",
    " 'hist_card_id_size',\n",
    " 'hist_category_1_mean',\n",
    " 'hist_category_3_mean_mean',\n",
    " 'hist_category_4_mean',\n",
    " 'hist_dayofweek_nunique',\n",
    " 'hist_hour_nunique',\n",
    " 'hist_installments_mean',\n",
    " 'hist_installments_min',\n",
    " 'hist_installments_sum',\n",
    " 'hist_installments_var',\n",
    " 'hist_merchant_category_id_nunique',\n",
    " 'hist_merchant_group_id_nunique',\n",
    " 'hist_merchant_id_nunique',\n",
    " 'hist_month_diff_mean',\n",
    " 'hist_month_lag_max',\n",
    " 'hist_month_lag_mean',\n",
    " 'hist_month_lag_min',\n",
    " 'hist_month_lag_var',\n",
    " 'hist_month_nunique',\n",
    " 'hist_most_recent_purchases_range_max',\n",
    " 'hist_most_recent_purchases_range_mean',\n",
    " 'hist_most_recent_sales_range_max',\n",
    " 'hist_most_recent_sales_range_mean',\n",
    " 'hist_most_recent_sales_range_std',\n",
    " 'hist_purchase_amount_max',\n",
    " 'hist_purchase_amount_mean',\n",
    " 'hist_purchase_amount_min',\n",
    " 'hist_purchase_amount_sum',\n",
    " 'hist_purchase_amount_var',\n",
    " 'hist_purchase_date_average',\n",
    " 'hist_purchase_date_count_mean',\n",
    " 'hist_purchase_date_count_std',\n",
    " 'hist_purchase_date_diff',\n",
    " 'hist_purchase_date_max',\n",
    " 'hist_purchase_date_min',\n",
    " 'hist_purchase_date_uptonow',\n",
    " 'hist_purchase_duration_max_max',\n",
    " 'hist_purchase_duration_max_mean',\n",
    " 'hist_purchase_duration_max_std',\n",
    " 'hist_repeat_purchase_amount_sum_max',\n",
    " 'hist_repeat_purchase_amount_sum_mean',\n",
    " 'hist_repeat_purchase_amount_sum_min',\n",
    " 'hist_sum_purchases_lag_max',\n",
    " 'hist_sum_purchases_lag_mean',\n",
    " 'hist_sum_purchases_lag_min',\n",
    " 'hist_sum_purchases_lag_std',\n",
    " 'hist_sum_purchases_lag_sum',\n",
    " 'hist_sum_sales_lag_max',\n",
    " 'hist_sum_sales_lag_mean',\n",
    " 'hist_sum_sales_lag_min',\n",
    " 'hist_sum_sales_lag_std',\n",
    " 'hist_sum_sales_lag_sum',\n",
    " 'hist_sum_sales_p_purchases_lag_max',\n",
    " 'hist_sum_sales_p_purchases_lag_mean',\n",
    " 'hist_sum_sales_p_purchases_lag_min',\n",
    " 'hist_sum_sales_p_purchases_lag_sum',\n",
    " 'hist_weekend_mean',\n",
    " 'hist_weekofyear_nunique',\n",
    " 'new_hist_card_id_size',\n",
    " 'new_hist_category_1_mean',\n",
    " 'new_hist_category_3_mean_mean',\n",
    " 'new_hist_first_buy',\n",
    " 'new_hist_installments_max',\n",
    " 'new_hist_installments_mean',\n",
    " 'new_hist_installments_min',\n",
    " 'new_hist_installments_sum',\n",
    " 'new_hist_installments_var',\n",
    " 'new_hist_merchant_category_id_nunique',\n",
    " 'new_hist_merchant_id_nunique',\n",
    " 'new_hist_month_diff_mean',\n",
    " 'new_hist_month_lag_mean',\n",
    " 'new_hist_month_lag_var',\n",
    " 'new_hist_purchase_amount_max',\n",
    " 'new_hist_purchase_amount_mean',\n",
    " 'new_hist_purchase_amount_min',\n",
    " 'new_hist_purchase_amount_sum',\n",
    " 'new_hist_purchase_amount_var',\n",
    " 'new_hist_purchase_date_average',\n",
    " 'new_hist_purchase_date_diff',\n",
    " 'new_hist_purchase_date_max',\n",
    " 'new_hist_purchase_date_min',\n",
    " 'new_hist_purchase_date_uptonow',\n",
    " 'new_hist_weekofyear_nunique',\n",
    " 'new_hist_year_nunique',\n",
    " 'new_most_recent_purchases_range_std',\n",
    "#  'new_sum_purchases_lag_std',\n",
    "#  'new_sum_purchases_lag_sum',\n",
    "#  'new_sum_sales_lag_sum',\n",
    "#  'new_sum_sales_p_purchases_lag_sum',\n",
    " 'purchase_amount_total',\n",
    " 'weekofyear']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "4edddeda48af1a92e187335a2acdc4bb6408e029"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read complete for: 0\n",
      "read complete for: 1\n",
      "read complete for: 2\n",
      "read complete for: 3\n",
      "read complete for: 4\n",
      "(161533, 2)\n",
      "(40384, 2)\n",
      "(123623, 2)\n"
     ]
    }
   ],
   "source": [
    "# tr_encs, val_encs,test_encs = getenc(mask_without_outlier=mask_without_outlier)\n",
    "tr_encs, val_encs,test_encs = getenc()\n",
    "print(tr_encs[0].shape)\n",
    "print(val_encs[0].shape)\n",
    "print(test_encs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "d6fed31c5d3cb37a5709bd44013ea22ef7e1955f"
   },
   "outputs": [],
   "source": [
    "sel_enc_cols =['trans_merged_targetenc_merchant_id_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "31880e57be08930b7b44604f94004a03b6700b9e"
   },
   "outputs": [],
   "source": [
    "hist_monthlagmeancols=[]\n",
    "hist_monthlagsumcols=[]\n",
    "for i in range(-13,1):\n",
    "    hist_monthlagmeancols+=['hist_month_lag_'+str(i) + '_mean']\n",
    "    hist_monthlagsumcols+=['hist_month_lag_'+str(i) + '_sum']\n",
    "new_hist_monthlagmeancols=[]\n",
    "new_hist_monthlagsumcols=[]\n",
    "for i in range(1,3):\n",
    "    new_hist_monthlagmeancols+=['new_hist_month_lag_'+str(i) + '_mean']\n",
    "    new_hist_monthlagsumcols+=['new_hist_month_lag_'+str(i) + '_sum']\n",
    "\n",
    "new_feats_reg =[\n",
    "    'hist_category_3_A_sum',\n",
    "    'hist_category_3_A_mean',\n",
    "    'new_hist_category_3_A_sum',\n",
    "    'new_hist_category_3_A_mean',\n",
    "    'hist_city_id_nunique', 'new_hist_city_id_nunique',\n",
    "    'hist_category_2_raw_mean',  'new_hist_category_2_raw_mean',\n",
    "    'hist_category_3_raw_mean',  'new_hist_category_3_raw_mean',\n",
    "    'hist_purchase_amount_per_install_sum', 'hist_purchase_amount_per_install_mean',\n",
    "    'hist_purchase_amount_per_install_var',\n",
    "    'new_hist_purchase_amount_per_install_sum', 'new_hist_purchase_amount_per_install_mean',\n",
    "    'new_hist_purchase_amount_per_install_var',\n",
    "    'hist_purchase_amount_scaled_sum', 'hist_purchase_amount_scaled_mean',\n",
    "    'hist_purchase_amount_scaled_var',\n",
    "    'new_hist_purchase_amount_scaled_sum', 'new_hist_purchase_amount_scaled_mean',\n",
    "    'new_hist_purchase_amount_scaled_var',    \n",
    "    ]\n",
    "\n",
    "new_feats_reg += hist_monthlagsumcols + hist_monthlagmeancols + new_hist_monthlagsumcols + new_hist_monthlagmeancols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "67e6bfc28d72fde68d7a9222c5eefe14dc5fe8aa"
   },
   "outputs": [],
   "source": [
    "num_round = 10000\n",
    "n_splits =5\n",
    "max_delta_score =0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "7f98f45d1226d9f62d3c1e348f1a40d0f050e623"
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# #Run permute features\n",
    "# features = list(df_train.columns) + sel_enc_cols\n",
    "# exclude_cols = ['card_id','first_active_month','target','outliers']\n",
    "# features = [col for col in features if col not in exclude_cols]\n",
    "\n",
    "# # new_feats_reg=new_feats_reg[0:5]\n",
    "# features = sel_feats + sel_enc_cols + new_feats_reg\n",
    "\n",
    "# # print(features)\n",
    "# # features = sel_feats[0:5]\n",
    "# fold_importance_df,predictions,oof,train_outlier_preds,overall_imp_df,overall_sel_feats\\\n",
    "#     = runlgb(True,df_train,df_test,param,features,get_rmse_score,train_outlier=df_train_outliers,\n",
    "#             subset_feats=new_feats_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "fc070d0761bf751004dc0dca8f0a2fef4034c4c0"
   },
   "outputs": [],
   "source": [
    "# selected_features = list(overall_imp_df.loc[overall_imp_df['overall_score_mean'] >= max_delta_score, 'feature'])\n",
    "# selected_features.sort()\n",
    "# df = pd.DataFrame( np.array(selected_features))\n",
    "# df.to_csv(\"overall_selected_features.csv\")\n",
    "# selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "65707cad8ecc009a2c9a716f16c5bf56dc51cc93"
   },
   "outputs": [],
   "source": [
    "# overall_imp_df.sort_values(by='overall_score_mean',ascending=False,inplace=True)\n",
    "# overall_imp_df.to_csv('overall_feats_allfolds.csv')\n",
    "# overall_imp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "784cc8dd4f5eb28231c48830efbce3e3999c0be6"
   },
   "outputs": [],
   "source": [
    "# overall_sel_feats[0].sort()\n",
    "# df = pd.DataFrame( np.array(overall_sel_feats[0]))\n",
    "# df.to_csv(\"sel_feats_0.csv\")\n",
    "# overall_sel_feats[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "2153155999c2d67b0dbcdc1177cd63fe27e1771f"
   },
   "outputs": [],
   "source": [
    "# overall_sel_feats[1].sort()\n",
    "# df = pd.DataFrame( np.array(overall_sel_feats[1]))\n",
    "# df.to_csv(\"sel_feats_1.csv\")\n",
    "# overall_sel_feats[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "fb7af81d88014049987560e762c82cf519014550"
   },
   "outputs": [],
   "source": [
    "# overall_sel_feats[2].sort()\n",
    "# df = pd.DataFrame( np.array(overall_sel_feats[2]))\n",
    "# df.to_csv(\"sel_feats_2.csv\")\n",
    "# overall_sel_feats[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "0ed22c79f06433f22b62d066d73941ed5b9215a7"
   },
   "outputs": [],
   "source": [
    "# overall_sel_feats[3].sort()\n",
    "# df = pd.DataFrame( np.array(overall_sel_feats[3]))\n",
    "# df.to_csv(\"sel_feats_3.csv\")\n",
    "# overall_sel_feats[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "b214350df224227610a613e4d6dbb7e3ea8be126"
   },
   "outputs": [],
   "source": [
    "# overall_sel_feats[4].sort()\n",
    "# df = pd.DataFrame( np.array(overall_sel_feats[4]))\n",
    "# df.to_csv(\"sel_feats_4.csv\")\n",
    "# overall_sel_feats[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "7dbb86c6b7eba23c96782acd949c9d43abff1e7f"
   },
   "outputs": [],
   "source": [
    "overall_sel_feats =[1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "db7c65884c3435e33dfc673e242f5f190cbfb38b"
   },
   "outputs": [],
   "source": [
    "overall_sel_feats[0] = ['hist_category_3_A_sum', 'hist_city_id_nunique',\n",
    " 'hist_month_lag_-1_mean', 'hist_month_lag_-2_mean', 'hist_month_lag_-2_sum',\n",
    " 'hist_month_lag_0_mean', 'hist_purchase_amount_per_install_mean', 'hist_purchase_amount_per_install_sum',\n",
    " 'hist_purchase_amount_per_install_var', \n",
    "'hist_purchase_amount_scaled_mean', 'hist_purchase_amount_scaled_sum',\n",
    " 'new_hist_category_3_A_sum', 'new_hist_month_lag_1_mean', 'new_hist_month_lag_1_sum',\n",
    " 'new_hist_purchase_amount_per_install_mean', 'new_hist_purchase_amount_per_install_sum',\n",
    " 'new_hist_purchase_amount_per_install_var', \n",
    "'new_hist_purchase_amount_scaled_mean',\n",
    " 'new_hist_purchase_amount_scaled_sum', \n",
    "                        'new_hist_purchase_amount_scaled_var'\n",
    "                       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "d42e5f4b50f76125c3ea1af7711498ab90d7b58d"
   },
   "outputs": [],
   "source": [
    "overall_sel_feats[1] = ['hist_city_id_nunique', 'hist_month_lag_-13_mean', 'hist_month_lag_-1_mean',\n",
    " 'hist_month_lag_-1_sum', 'hist_month_lag_-2_mean', 'hist_month_lag_-2_sum',\n",
    " 'hist_month_lag_-3_mean', 'hist_month_lag_-4_mean', 'hist_month_lag_0_mean', 'hist_month_lag_0_sum',\n",
    " 'hist_purchase_amount_per_install_mean', 'hist_purchase_amount_per_install_sum',\n",
    "'hist_purchase_amount_scaled_mean', 'hist_purchase_amount_scaled_sum', \n",
    "#                         'hist_purchase_amount_scaled_var',\n",
    " 'new_hist_category_3_A_sum', 'new_hist_month_lag_1_sum',\n",
    " 'new_hist_purchase_amount_per_install_mean', 'new_hist_purchase_amount_per_install_sum',\n",
    " 'new_hist_purchase_amount_per_install_var', \n",
    "'new_hist_purchase_amount_scaled_mean',\n",
    " 'new_hist_purchase_amount_scaled_sum',\n",
    "#                         'new_hist_purchase_amount_scaled_var'\n",
    "                       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "47fbfb1f718f422ddb5d4cfc01aea22f08174419"
   },
   "outputs": [],
   "source": [
    "overall_sel_feats[2] =['hist_category_3_A_sum', 'hist_city_id_nunique',\n",
    " 'hist_month_lag_-11_mean', 'hist_month_lag_-1_mean', 'hist_month_lag_-2_sum',\n",
    " 'hist_month_lag_-3_mean', 'hist_month_lag_0_mean', 'hist_month_lag_0_sum',\n",
    " 'hist_purchase_amount_per_install_mean', 'hist_purchase_amount_per_install_var',\n",
    " 'hist_purchase_amount_scaled_mean', 'hist_purchase_amount_scaled_sum',\n",
    " 'new_hist_category_2_raw_mean', 'new_hist_category_3_A_sum',\n",
    " 'new_hist_month_lag_1_mean', 'new_hist_month_lag_1_sum',\n",
    " 'new_hist_purchase_amount_per_install_mean', 'new_hist_purchase_amount_per_install_sum',\n",
    " 'new_hist_purchase_amount_per_install_var', \n",
    "'new_hist_purchase_amount_scaled_mean',\n",
    " 'new_hist_purchase_amount_scaled_sum'\n",
    "                      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "1b7dfc0493fa7b7c8e479e86bc608975d5be2135"
   },
   "outputs": [],
   "source": [
    "overall_sel_feats[3] =['hist_category_2_raw_mean', 'hist_category_3_A_sum', 'hist_category_3_raw_mean',\n",
    " 'hist_city_id_nunique', 'hist_month_lag_-1_mean', 'hist_month_lag_-1_sum',\n",
    " 'hist_month_lag_-2_mean', 'hist_month_lag_-2_sum', 'hist_month_lag_-3_mean',\n",
    " 'hist_month_lag_0_mean', 'hist_month_lag_0_sum',\n",
    " 'hist_purchase_amount_per_install_mean', 'hist_purchase_amount_per_install_sum',\n",
    " 'hist_purchase_amount_per_install_var',\n",
    "                       'hist_purchase_amount_scaled_mean',\n",
    " 'hist_purchase_amount_scaled_sum', 'new_hist_category_2_raw_mean',\n",
    " 'new_hist_category_3_A_mean', 'new_hist_category_3_A_sum', 'new_hist_month_lag_1_mean',\n",
    " 'new_hist_month_lag_1_sum',\n",
    " 'new_hist_purchase_amount_per_install_mean', 'new_hist_purchase_amount_per_install_sum',\n",
    " 'new_hist_purchase_amount_per_install_var', \n",
    "                       'new_hist_purchase_amount_scaled_mean',\n",
    " 'new_hist_purchase_amount_scaled_sum',\n",
    "                       'new_hist_purchase_amount_scaled_var'\n",
    "                      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "8fd3e2578376914321e9200553ab08300aa53e6c"
   },
   "outputs": [],
   "source": [
    "overall_sel_feats[4] =['hist_category_2_raw_mean', 'hist_category_3_A_sum', 'hist_category_3_raw_mean',\n",
    " 'hist_city_id_nunique', 'hist_month_lag_-1_mean', 'hist_month_lag_-1_sum', 'hist_month_lag_-2_sum',\n",
    " 'hist_month_lag_-3_sum', 'hist_month_lag_0_mean', 'hist_month_lag_0_sum',\n",
    " 'hist_purchase_amount_per_install_mean', 'hist_purchase_amount_per_install_sum',\n",
    " 'hist_purchase_amount_per_install_var', \n",
    "'hist_purchase_amount_scaled_mean', 'hist_purchase_amount_scaled_sum', \n",
    "                       'new_hist_category_3_A_sum',\n",
    " 'new_hist_month_lag_1_mean', 'new_hist_month_lag_1_sum', 'new_hist_month_lag_2_mean',\n",
    " 'new_hist_purchase_amount_per_install_mean', 'new_hist_purchase_amount_per_install_sum',\n",
    " 'new_hist_purchase_amount_per_install_var', \n",
    "                       'new_hist_purchase_amount_scaled_mean',\n",
    " 'new_hist_purchase_amount_scaled_sum',\n",
    "#                        'new_hist_purchase_amount_scaled_var'\n",
    "                      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "8a54a303c36c835d9d17072ae6f48eea00203bfa"
   },
   "outputs": [],
   "source": [
    "num_round=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_uuid": "7dbc673ce135ea277c1dcddac93f9259b38ebff7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hist_category_3_A_sum', 'hist_city_id_nunique', 'hist_month_lag_-1_mean', 'hist_month_lag_-2_mean', 'hist_month_lag_-2_sum', 'hist_month_lag_0_mean', 'hist_purchase_amount_per_install_mean', 'hist_purchase_amount_per_install_sum', 'hist_purchase_amount_per_install_var', 'hist_purchase_amount_scaled_mean', 'hist_purchase_amount_scaled_sum', 'new_hist_category_3_A_sum', 'new_hist_month_lag_1_mean', 'new_hist_month_lag_1_sum', 'new_hist_purchase_amount_per_install_mean', 'new_hist_purchase_amount_per_install_sum', 'new_hist_purchase_amount_per_install_var', 'new_hist_purchase_amount_scaled_mean', 'new_hist_purchase_amount_scaled_sum', 'new_hist_purchase_amount_scaled_var', 'feature_1', 'feature_2', 'feature_3', 'hist_authorized_flag_mean', 'hist_authorized_flag_sum', 'hist_card_id_size', 'hist_category_1_mean', 'hist_category_3_mean_mean', 'hist_category_4_mean', 'hist_dayofweek_nunique', 'hist_hour_nunique', 'hist_installments_mean', 'hist_installments_min', 'hist_installments_sum', 'hist_installments_var', 'hist_merchant_category_id_nunique', 'hist_merchant_group_id_nunique', 'hist_merchant_id_nunique', 'hist_month_diff_mean', 'hist_month_lag_max', 'hist_month_lag_mean', 'hist_month_lag_min', 'hist_month_lag_var', 'hist_month_nunique', 'hist_most_recent_purchases_range_max', 'hist_most_recent_purchases_range_mean', 'hist_most_recent_sales_range_max', 'hist_most_recent_sales_range_mean', 'hist_most_recent_sales_range_std', 'hist_purchase_amount_max', 'hist_purchase_amount_mean', 'hist_purchase_amount_min', 'hist_purchase_amount_sum', 'hist_purchase_amount_var', 'hist_purchase_date_average', 'hist_purchase_date_count_mean', 'hist_purchase_date_count_std', 'hist_purchase_date_diff', 'hist_purchase_date_max', 'hist_purchase_date_min', 'hist_purchase_date_uptonow', 'hist_purchase_duration_max_max', 'hist_purchase_duration_max_mean', 'hist_purchase_duration_max_std', 'hist_repeat_purchase_amount_sum_max', 'hist_repeat_purchase_amount_sum_mean', 'hist_repeat_purchase_amount_sum_min', 'hist_sum_purchases_lag_max', 'hist_sum_purchases_lag_mean', 'hist_sum_purchases_lag_min', 'hist_sum_purchases_lag_std', 'hist_sum_purchases_lag_sum', 'hist_sum_sales_lag_max', 'hist_sum_sales_lag_mean', 'hist_sum_sales_lag_min', 'hist_sum_sales_lag_std', 'hist_sum_sales_lag_sum', 'hist_sum_sales_p_purchases_lag_max', 'hist_sum_sales_p_purchases_lag_mean', 'hist_sum_sales_p_purchases_lag_min', 'hist_sum_sales_p_purchases_lag_sum', 'hist_weekend_mean', 'hist_weekofyear_nunique', 'new_hist_card_id_size', 'new_hist_category_1_mean', 'new_hist_category_3_mean_mean', 'new_hist_first_buy', 'new_hist_installments_max', 'new_hist_installments_mean', 'new_hist_installments_min', 'new_hist_installments_sum', 'new_hist_installments_var', 'new_hist_merchant_category_id_nunique', 'new_hist_merchant_id_nunique', 'new_hist_month_diff_mean', 'new_hist_month_lag_mean', 'new_hist_month_lag_var', 'new_hist_purchase_amount_max', 'new_hist_purchase_amount_mean', 'new_hist_purchase_amount_min', 'new_hist_purchase_amount_sum', 'new_hist_purchase_amount_var', 'new_hist_purchase_date_average', 'new_hist_purchase_date_diff', 'new_hist_purchase_date_max', 'new_hist_purchase_date_min', 'new_hist_purchase_date_uptonow', 'new_hist_weekofyear_nunique', 'new_hist_year_nunique', 'new_most_recent_purchases_range_std', 'purchase_amount_total', 'weekofyear', 'trans_merged_targetenc_merchant_id_mean']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Run model with selected features\n",
    "# features = selected_features\n",
    "# features = sel_feats\n",
    "\n",
    "for i,val in enumerate(overall_sel_feats):\n",
    "    overall_sel_feats[i] += sel_feats + sel_enc_cols #sel_feats\n",
    "print(overall_sel_feats[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_uuid": "a0ecb4a6605d477b290275f404a508bfacd614e0"
   },
   "outputs": [],
   "source": [
    "# param = {'objective':'regression',\n",
    "#          'num_leaves': 31,\n",
    "#          'min_data_in_leaf': 25,\n",
    "#          'max_depth': 7,\n",
    "#          'learning_rate': 0.01,\n",
    "#          'lambda_l1':0.13,\n",
    "#          \"boosting\": \"gbdt\",\n",
    "#          \"feature_fraction\":0.85,\n",
    "#          'bagging_freq':8,\n",
    "#          \"bagging_fraction\": 0.9 ,\n",
    "#          \"metric\": 'rmse',\n",
    "#          \"verbosity\": -1,\n",
    "#          \"random_state\": 2333,\n",
    "#          'n_estimators': 10000,\n",
    "#          'n_jobs' :-1\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_uuid": "2854c1b691bdd12ec81a98a36e604fe98c58c594"
   },
   "outputs": [],
   "source": [
    "# param = {'colsample_bytree': 0.6090229042575085,\n",
    "#    'min_child_samples': 215,\n",
    "#    'num_leaves': 77,\n",
    "#    'reg_alpha': 0.830466583246783,\n",
    "#    'reg_lambda': 0.24303972756965078,\n",
    "#    'subsample': 0.717525538993901,\n",
    "#    'subsample_for_bin': 160000,\n",
    "#    'learning_rate': 0.01,\n",
    "#    'boosting': 'gbdt',\n",
    "#    'bagging_seed': 2018,\n",
    "#    'bagging_freq': 8,\n",
    "#    'n_estimators': 3000,\n",
    "#    'objective': 'regression',\n",
    "#    'metric': 'rmse',\n",
    "#    'random_state': 2333,\n",
    "#    'max_depth': 7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_uuid": "c0922285f066f4c2e49cf6fffc9b921d223b240f"
   },
   "outputs": [],
   "source": [
    "#Optimized after 200 iterations on bagging freq = 2\n",
    "param =  {'colsample_bytree': 0.6461416260298948,\n",
    "   'min_child_samples': 100,\n",
    "   'num_leaves': 56,\n",
    "   'reg_alpha': 0.924728048030499,\n",
    "   'reg_lambda': 0.7379608279900612,\n",
    "   'subsample': 0.542352168720072,\n",
    "   'subsample_for_bin': 120000,\n",
    "   'learning_rate': 0.01,\n",
    "   'boosting': 'gbdt',\n",
    "   'bagging_seed': 2018,\n",
    "   'min_data_in_bin': 100,\n",
    "   'bagging_freq': 2,\n",
    "   'n_estimators': 10000,\n",
    "   'objective': 'regression',\n",
    "   'metric': 'rmse',\n",
    "   'random_state': 2333,\n",
    "   'max_depth': 7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_uuid": "7dbc673ce135ea277c1dcddac93f9259b38ebff7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fold nÂ°0\n",
      "y_train shape: (199710,)\n",
      "val shape: (39942, 205)\n",
      "y_val shape: (39942,)\n",
      "tr shape: (159768, 206)\n",
      "val shape: (39942, 206)\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\tvalid_0's rmse: 1.62086\n",
      "[200]\tvalid_0's rmse: 1.59303\n",
      "[300]\tvalid_0's rmse: 1.58324\n",
      "[400]\tvalid_0's rmse: 1.57861\n",
      "[500]\tvalid_0's rmse: 1.57633\n",
      "[600]\tvalid_0's rmse: 1.57495\n",
      "[700]\tvalid_0's rmse: 1.57408\n",
      "[800]\tvalid_0's rmse: 1.57352\n",
      "[900]\tvalid_0's rmse: 1.57306\n",
      "[1000]\tvalid_0's rmse: 1.5727\n",
      "[1100]\tvalid_0's rmse: 1.57264\n",
      "[1200]\tvalid_0's rmse: 1.57251\n",
      "[1300]\tvalid_0's rmse: 1.5724\n",
      "[1400]\tvalid_0's rmse: 1.57227\n",
      "[1500]\tvalid_0's rmse: 1.57234\n",
      "[1600]\tvalid_0's rmse: 1.57257\n",
      "Early stopping, best iteration is:\n",
      "[1400]\tvalid_0's rmse: 1.57227\n",
      "\n",
      "fold nÂ°1\n",
      "y_train shape: (199710,)\n",
      "val shape: (39942, 205)\n",
      "y_val shape: (39942,)\n",
      "tr shape: (159768, 206)\n",
      "val shape: (39942, 206)\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\tvalid_0's rmse: 1.59451\n",
      "[200]\tvalid_0's rmse: 1.56775\n",
      "[300]\tvalid_0's rmse: 1.55845\n",
      "[400]\tvalid_0's rmse: 1.55463\n",
      "[500]\tvalid_0's rmse: 1.5525\n",
      "[600]\tvalid_0's rmse: 1.55136\n",
      "[700]\tvalid_0's rmse: 1.55068\n",
      "[800]\tvalid_0's rmse: 1.55028\n",
      "[900]\tvalid_0's rmse: 1.54992\n",
      "[1000]\tvalid_0's rmse: 1.54965\n",
      "[1100]\tvalid_0's rmse: 1.54959\n",
      "[1200]\tvalid_0's rmse: 1.54965\n",
      "[1300]\tvalid_0's rmse: 1.54952\n",
      "[1400]\tvalid_0's rmse: 1.54948\n",
      "[1500]\tvalid_0's rmse: 1.54957\n",
      "Early stopping, best iteration is:\n",
      "[1369]\tvalid_0's rmse: 1.54944\n",
      "\n",
      "fold nÂ°2\n",
      "y_train shape: (199710,)\n",
      "val shape: (39942, 205)\n",
      "y_val shape: (39942,)\n",
      "tr shape: (159768, 206)\n",
      "val shape: (39942, 206)\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\tvalid_0's rmse: 1.61995\n",
      "[200]\tvalid_0's rmse: 1.59212\n",
      "[300]\tvalid_0's rmse: 1.58224\n",
      "[400]\tvalid_0's rmse: 1.57757\n",
      "[500]\tvalid_0's rmse: 1.57507\n",
      "[600]\tvalid_0's rmse: 1.57351\n",
      "[700]\tvalid_0's rmse: 1.57244\n",
      "[800]\tvalid_0's rmse: 1.57186\n",
      "[900]\tvalid_0's rmse: 1.5713\n",
      "[1000]\tvalid_0's rmse: 1.57078\n",
      "[1100]\tvalid_0's rmse: 1.57052\n",
      "[1200]\tvalid_0's rmse: 1.57025\n",
      "[1300]\tvalid_0's rmse: 1.57008\n",
      "[1400]\tvalid_0's rmse: 1.56992\n",
      "[1500]\tvalid_0's rmse: 1.5699\n",
      "[1600]\tvalid_0's rmse: 1.56972\n",
      "[1700]\tvalid_0's rmse: 1.56962\n",
      "[1800]\tvalid_0's rmse: 1.56965\n",
      "[1900]\tvalid_0's rmse: 1.56968\n",
      "Early stopping, best iteration is:\n",
      "[1772]\tvalid_0's rmse: 1.56957\n",
      "\n",
      "fold nÂ°3\n",
      "y_train shape: (199710,)\n",
      "val shape: (39942, 205)\n",
      "y_val shape: (39942,)\n",
      "tr shape: (159768, 206)\n",
      "val shape: (39942, 206)\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\tvalid_0's rmse: 1.59992\n",
      "[200]\tvalid_0's rmse: 1.57154\n",
      "[300]\tvalid_0's rmse: 1.56139\n",
      "[400]\tvalid_0's rmse: 1.55658\n",
      "[500]\tvalid_0's rmse: 1.55417\n",
      "[600]\tvalid_0's rmse: 1.55262\n",
      "[700]\tvalid_0's rmse: 1.55172\n",
      "[800]\tvalid_0's rmse: 1.55105\n",
      "[900]\tvalid_0's rmse: 1.55064\n",
      "[1000]\tvalid_0's rmse: 1.55022\n",
      "[1100]\tvalid_0's rmse: 1.54997\n",
      "[1200]\tvalid_0's rmse: 1.54984\n",
      "[1300]\tvalid_0's rmse: 1.54964\n",
      "[1400]\tvalid_0's rmse: 1.54969\n",
      "[1500]\tvalid_0's rmse: 1.5496\n",
      "[1600]\tvalid_0's rmse: 1.54965\n",
      "[1700]\tvalid_0's rmse: 1.54974\n",
      "Early stopping, best iteration is:\n",
      "[1505]\tvalid_0's rmse: 1.54958\n",
      "\n",
      "fold nÂ°4\n",
      "y_train shape: (199710,)\n",
      "val shape: (39942, 205)\n",
      "y_val shape: (39942,)\n",
      "tr shape: (159768, 206)\n",
      "val shape: (39942, 206)\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\tvalid_0's rmse: 1.56786\n",
      "[200]\tvalid_0's rmse: 1.53997\n",
      "[300]\tvalid_0's rmse: 1.53035\n",
      "[400]\tvalid_0's rmse: 1.52614\n",
      "[500]\tvalid_0's rmse: 1.5238\n",
      "[600]\tvalid_0's rmse: 1.52241\n",
      "[700]\tvalid_0's rmse: 1.52156\n",
      "[800]\tvalid_0's rmse: 1.52102\n",
      "[900]\tvalid_0's rmse: 1.52056\n",
      "[1000]\tvalid_0's rmse: 1.52031\n",
      "[1100]\tvalid_0's rmse: 1.52008\n",
      "[1200]\tvalid_0's rmse: 1.51988\n",
      "[1300]\tvalid_0's rmse: 1.5198\n",
      "[1400]\tvalid_0's rmse: 1.51978\n",
      "[1500]\tvalid_0's rmse: 1.51983\n",
      "Early stopping, best iteration is:\n",
      "[1373]\tvalid_0's rmse: 1.51975\n",
      "valid scores: [1.572274810325697, 1.5494444891678432, 1.5695671338074535, 1.5495809669007699, 1.5197523313344137]\n",
      "CV score: 1.55224 \n",
      "CPU times: user 22min 22s, sys: 15.5 s, total: 22min 37s\n",
      "Wall time: 6min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fold_importance_df,predictions,oof,train_outlier_preds,dummy_overall_imp_df,dummy_overall_sel_feats\\\n",
    "    = runlgb(4590,False,df_train,df_test,param,overall_sel_feats,target,\n",
    "             lgb_fit, lgb_predict,lgb_getbestscore,train_outlier=df_train_outliers,\n",
    "            fold_feats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_uuid": "445970b2ba5e7b0f44b2e398399e2f5628d8d11e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('oof_lgb.csv',oof,delimiter=',')\n",
    "np.savetxt('predictions_lgb.csv',predictions,delimiter=',')\n",
    "np.savetxt('train_outlier_preds_lgb.csv',train_outlier_preds,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_uuid": "eca89bd5758c84dc77c11420f994c6e40132bca4"
   },
   "outputs": [],
   "source": [
    "xgbparam = {'objective':'reg:linear',\n",
    "         'max_leaf_nodes': 31,\n",
    "         'min_child_weight': 25,\n",
    "         'max_depth': 5,\n",
    "         'learning_rate': 0.01,\n",
    "         'reg_lambda':0.13,\n",
    "         \"booster\": \"gbtree\",\n",
    "         \"colsample_bytree\":0.85,\n",
    "         'colsample_bylevel':0.9,\n",
    "         \"subsample\": 0.9 ,\n",
    "         \"eval_metric\": 'rmse',\n",
    "#          \"verbosity\": 0,\n",
    "#          'silent': True,\n",
    "         \"random_state\": 2333,\n",
    "         'n_estimators': 5000,\n",
    "         'n_jobs' :4\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_uuid": "7a04eff3e7947e87b9c47d2df0f7c904e8a34187"
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# #XGBoost Predictions\n",
    "# fold_importance_df,predictions_xgb,oof_xgb,train_outlier_preds_xgb,dummy_overall_imp_df,dummy_overall_sel_feats\\\n",
    "#     = runlgb(4590,False,df_train,df_test,xgbparam,overall_sel_feats,target,\n",
    "#              xgb_fit,xgb_predict,xgb_getbestscore,train_outlier=df_train_outliers,\n",
    "#             fold_feats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_uuid": "6d7c903c4776691e25593131e804f590b405f192"
   },
   "outputs": [],
   "source": [
    "Path = '../input/elo-combine-reg-and-best-subm-and-classification/'\n",
    "oof_xgb=np.loadtxt(Path+'oof_xgb.csv',delimiter=',')\n",
    "predictions_xgb=np.loadtxt(Path+'predictions_xgb.csv',delimiter=',')\n",
    "train_outlier_preds_xgb=np.loadtxt(Path+'train_outlier_preds_xgb.csv',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_uuid": "d4c6f895eedd363af89d190f0fcceb5c128b9da3"
   },
   "outputs": [],
   "source": [
    "np.savetxt('oof_xgb.csv',oof_xgb,delimiter=',')\n",
    "np.savetxt('predictions_xgb.csv',predictions_xgb,delimiter=',')\n",
    "np.savetxt('train_outlier_preds_xgb.csv',train_outlier_preds_xgb,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_uuid": "23bfbabb05691fc8d8c32c80717937eed3bac8a3"
   },
   "outputs": [],
   "source": [
    "def getrmsescore(target,preds):\n",
    "    score = mean_squared_error(target,preds)**0.5\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_uuid": "bc7dbdde2070e6267861df4516c9bbb7b10efa7c"
   },
   "outputs": [],
   "source": [
    "getscore  = getrmsescore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_uuid": "12fb827fa2f8527660904f5ae5cb59b720580243"
   },
   "outputs": [],
   "source": [
    "train_ens_arr= np.vstack([oof, oof_xgb]).transpose()\n",
    "test_ens_arr= np.vstack([predictions, predictions_xgb]).transpose()\n",
    "train_outlier_ens_arr = np.vstack([train_outlier_preds, train_outlier_preds_xgb]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_uuid": "55485d35aa60eecbd9f716a8780ce52d4575ec02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble score weighted mean score: 1.5521684071642086\n"
     ]
    }
   ],
   "source": [
    "#weighted average ensemble\n",
    "#score weighted average\n",
    "score_weight = [0.8,0.2]\n",
    "\n",
    "train_mean_w = score_weight[0] * train_ens_arr[:,0] + score_weight[1] * train_ens_arr[:,1] \n",
    "test_mean_w = score_weight[0] * test_ens_arr[:,0] + score_weight[1] * test_ens_arr[:,1] \n",
    "train_outlier_mean_w = score_weight[0] * train_outlier_ens_arr[:,0] + score_weight[1] * train_outlier_ens_arr[:,1] \n",
    "score = getscore(target,train_mean_w)\n",
    "print('Ensemble score weighted mean score:',score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_uuid": "a6b8f34733da36f136290d6fdb4186c259a374b8"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_uuid": "583dd2f552b2c3a6ceb90febc3aeed2cb8067e49"
   },
   "outputs": [],
   "source": [
    "# #STacking ensemble of xgb and lgb\n",
    "# folds = KFold(n_splits=5, shuffle=True, random_state=15)\n",
    "# oof_stack = np.zeros(train_ens_arr.shape[0])\n",
    "# predictions_stack = np.zeros(test_ens_arr.shape[0])\n",
    "\n",
    "# for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_ens_arr, target)):\n",
    "#     print(\"fold nÂ°{}\".format(fold_))\n",
    "#     trn_data, trn_y = train_ens_arr[trn_idx], target.iloc[trn_idx].values\n",
    "#     val_data, val_y = train_ens_arr[val_idx], target.iloc[val_idx].values\n",
    "\n",
    "#     print(\"-\" * 10 + \"Regression \" + str(fold_) + \"-\" * 10)\n",
    "    \n",
    "#     clf = Ridge(alpha=0.01)#,solver='saga')\n",
    "#     clf.fit(trn_data,trn_y)\n",
    "    \n",
    "#     oof_stack[val_idx] = clf.predict(val_data)\n",
    "#     print(\"current score: {0} \".format(getscore(val_y, oof_stack[val_idx])))    \n",
    "# #     train_outlier_preds_stack += clf.predict(test_ens_arr) / 5\n",
    "#     predictions_stack += clf.predict(test_ens_arr) / 5\n",
    "    \n",
    "# print(\"Stacked score: {0} \".format(getscore(target, oof_stack)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_uuid": "5ac12168d77c205f32546bb09b7df3ed6f0f5312"
   },
   "outputs": [],
   "source": [
    "#Ensemble of xgb and lgbm\n",
    "oof = train_mean_w\n",
    "predictions = test_mean_w\n",
    "train_outlier_preds = train_outlier_mean_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_uuid": "464039e0737ef8267d6fa6986658b48fbffe97b0"
   },
   "outputs": [],
   "source": [
    "# n_ensembles=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_uuid": "a27dd3d1f60fe2e7c7f77cba49b596b8eac00235"
   },
   "outputs": [],
   "source": [
    "# predictions_list=[]\n",
    "# oof_list =[]\n",
    "# for i in range(0,n_ensembles):\n",
    "# #     randomno = 1234\n",
    "#     randomno = 888 + i\n",
    "#     param['random_state'] = randomno\n",
    "#     fold_importance_df,predictions,oof,train_outlier_preds,dummy_overall_imp_df,dummy_overall_sel_feats\\\n",
    "#         = runlgb(4590,False,df_train,df_test,param,overall_sel_feats,target,train_outlier=df_train_outliers,\n",
    "#                 fold_feats=True)\n",
    "\n",
    "#     oof_list +=[oof]\n",
    "#     predictions_list +=[predictions]\n",
    "#     np.savetxt('test_reg_'+str(randomno)+'.csv',predictions,delimiter=',')\n",
    "#     np.savetxt('oof_reg_'+str(randomno)+'.csv',oof,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_uuid": "4842069eb5b3fa8617b624c1d2fd00940c75137e"
   },
   "outputs": [],
   "source": [
    "model_without_outliers = pd.DataFrame({\"card_id\":df_test[\"card_id\"].values})\n",
    "model_without_outliers[\"target\"] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "_uuid": "b0f1ef591bdb7981cbb382db292cb67ab2f91c4d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning:\n",
      "\n",
      "Boolean Series key will be reindexed to match DataFrame index.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#OOF\n",
    "model_without_outliers_oof = pd.DataFrame({\"card_id\":df_train[\"card_id\"].values})\n",
    "model_without_outliers_oof[\"target\"] = oof\n",
    "model_without_outliers_oof.index = df_train[mask_without_outlier].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "_uuid": "b48b901f89458902d32b14dcac61edab8c09cc60"
   },
   "outputs": [],
   "source": [
    "train_outlier_preds_df = pd.DataFrame()\n",
    "train_outlier_preds_df['card_id'] = df_train_outliers['card_id']\n",
    "train_outlier_preds_df['target'] = train_outlier_preds\n",
    "train_outlier_preds_df.index = df_train_outliers.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "_uuid": "b48b901f89458902d32b14dcac61edab8c09cc60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(201917, 2)\n"
     ]
    }
   ],
   "source": [
    "model_full_oof = pd.concat([model_without_outliers_oof,train_outlier_preds_df])\n",
    "print(model_full_oof.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "_uuid": "dde5c77192743b05c3c7ae2cb26944103f8b82b1"
   },
   "outputs": [],
   "source": [
    "# Path='../input/elo-combine-permute-feats-likely-liers/'\n",
    "# model_full_oof = pd.read_csv(Path+'model_full_oof.csv',index_col=0)\n",
    "# model_without_outliers = pd.read_csv(Path+'model_without_outliers.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "_uuid": "9da6864f476ccd922a05cd1302f8e1b87d67b150"
   },
   "outputs": [],
   "source": [
    "model_full_oof.to_csv('model_full_oof.csv')\n",
    "model_without_outliers_oof.to_csv('model_without_outliers_oof.csv')\n",
    "model_without_outliers.to_csv('model_without_outliers.csv')\n",
    "train_outlier_preds_df.to_csv('train_outlier_preds.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "60b9a679754a23f8a07a4e06bf2003abdfbe57db"
   },
   "source": [
    "# Part 2 Training Model For Outliers Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "_uuid": "6e55524ceae5ba44c7b37eb3deb67cce35820708"
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Path = '../input/elo-preproc-3/'\n",
    "# Path = '../input/elo-rfm-and-business-feats/'\n",
    "\n",
    "# df_train = pd.read_csv(Path+'train_preproc.csv',index_col=0)\n",
    "# df_test = pd.read_csv(Path +'test_preproc.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "_uuid": "dd2113c1d8402e11578b2b2806463a1d336957fd"
   },
   "outputs": [],
   "source": [
    "df_train = df_train_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c2410c958c49b761f042ecdf5619af54730865d3"
   },
   "source": [
    "## using outliers column as labels instead of target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "_uuid": "19cf3932dbbbc42a8e7e0c50ac7ae4c74fd15f62"
   },
   "outputs": [],
   "source": [
    "target_reg = df_train['target']\n",
    "target = df_train['outliers']\n",
    "# del df_train['outliers']\n",
    "# del df_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "_uuid": "a60cc387b88592ae2ae6ad78f33d6adcce22780d"
   },
   "outputs": [],
   "source": [
    "# features = [c for c in df_train.columns if c not in ['card_id', 'first_active_month']]\n",
    "# categorical_feats = [c for c in features if 'feature_' in c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8867a7ba6c07d9a6c0ee04277960ff4d9c1559a8"
   },
   "source": [
    "## parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "_uuid": "b6d18775a0893306ed5a42ab0a3dc0cdd5f38a20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read complete for: 0\n",
      "read complete for: 1\n",
      "read complete for: 2\n",
      "read complete for: 3\n",
      "read complete for: 4\n",
      "(161533, 2)\n",
      "(40384, 2)\n",
      "(123623, 2)\n"
     ]
    }
   ],
   "source": [
    "tr_encs, val_encs,test_encs = getenc()\n",
    "print(tr_encs[0].shape)\n",
    "print(val_encs[0].shape)\n",
    "print(test_encs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "_uuid": "f90bf73d92be5f38aa9e8a3ef0bb776c8096cbbb"
   },
   "outputs": [],
   "source": [
    "def rank_preproc(combined):\n",
    "    nullzerocols = ['new_hist_purchase_amount_scaled_max','new_hist_purchase_amount_scaled_min',\n",
    "                   'new_hist_purchase_amount_scaled_sum','new_hist_purchase_amount_scaled_mean',\n",
    "                'new_hist_purchase_amount_per_install_max','new_hist_purchase_amount_per_install_min',\n",
    "                'new_hist_purchase_amount_per_install_sum','new_hist_purchase_amount_per_install_mean']\n",
    "    nullmaxcols = ['new_hist_purchase_date_uptonow','new_hist_first_buy']\n",
    "    for df in [combined]:\n",
    "        df['purchase_amount_scaled_total'] = df['new_hist_purchase_amount_scaled_sum']+df['hist_purchase_amount_scaled_sum']\n",
    "        df['purchase_amount_per_install_total'] = df['new_hist_purchase_amount_per_install_sum']+df['hist_purchase_amount_per_install_sum']\n",
    "        for col in nullmaxcols:\n",
    "            maxval = df[col].max() + 30\n",
    "            df[col].fillna(maxval,inplace=True)\n",
    "        for col in nullzerocols:\n",
    "            df[col].fillna(0,inplace=True)\n",
    "            \n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "_uuid": "64410148188824561d14c69b925e1c66dd7a863f"
   },
   "outputs": [],
   "source": [
    "def gen_newfeats_outlier(combined):\n",
    "    for df in [combined]:\n",
    "        df['hist_size_per_merchant'] = df['hist_card_id_size'] / df['hist_merchant_id_nunique']\n",
    "        df['hist_size_per_merchant_category'] = df['hist_card_id_size'] / df['hist_merchant_category_id_nunique']\n",
    "        df['hist_size_per_subsector'] = df['hist_card_id_size'] / df['hist_subsector_id_nunique']\n",
    "        df['hist_size_per_merchant_group'] = df['hist_card_id_size'] / df['hist_merchant_group_id_nunique']\n",
    "        df['hist_size_per_city'] = df['hist_card_id_size'] / df['hist_city_id_nunique']\n",
    "    \n",
    "    return combined\n",
    "        \n",
    "def generate_rank_outlier(combined):\n",
    "\n",
    "\n",
    "        rankdesccols = [\n",
    "'elapsed_time',\n",
    "'hist_size_per_merchant',\n",
    "'new_hist_first_buy',\n",
    "'new_hist_card_id_size',\n",
    "'hist_first_buy',\n",
    "'hist_category_1_mean',\n",
    "'hist_installments_mean',\n",
    "'hist_purchase_date_count_mean',\n",
    "'new_hist_purchase_date_uptonow',\n",
    "'hist_purchase_date_count_max',\n",
    "'hist_purchase_date_diff',\n",
    "'hist_month_lag_-4_sum',\n",
    "'hist_month_lag_-5_sum',\n",
    "'hist_month_lag_-6_sum',            \n",
    "]\n",
    "    \n",
    "        rankasccols = [\n",
    "        'hist_category_2_raw_mean',\n",
    "        'hist_purchase_amount_per_install_sum',\n",
    "        'hist_purchase_amount_per_install_mean',\n",
    "        'hist_card_id_size',\n",
    "        'new_hist_purchase_date_min',\n",
    "        'hist_purchase_date_min',\n",
    "        'hist_month_lag_0_sum',\n",
    "        'hist_month_lag_-1_sum',\n",
    "        'hist_month_lag_-2_sum',\n",
    "        'hist_month_lag_-3_sum',\n",
    "#         'hist_month_lag_-4_sum',\n",
    "#         'hist_month_lag_-5_sum',\n",
    "#         'hist_month_lag_-6_sum',\n",
    "        'hist_purchase_date_diff',\n",
    "       # 'hist_most_recent_sales_range_std',\n",
    "                       \n",
    "        'hist_authorized_flag_mean',\n",
    "        'hist_authorized_flag_sum',\n",
    "        'new_hist_card_id_size',\n",
    "        'new_hist_purchase_amount_per_install_sum',\n",
    "        'hist_category_3_A_mean',\n",
    "        'hist_month_lag_-12_mean',\n",
    "        'hist_month_lag_-13_sum',\n",
    "        'new_hist_purchase_date_diff',\n",
    "        'hist_purchase_amount_per_install_mean',\n",
    "\n",
    "        ]\n",
    "        \n",
    "        rankcols =[col for col in combined.columns if ('rank_' in col)]\n",
    "        if len(rankcols)!=0:\n",
    "            combined.drop(rankcols,axis=1,inplace=True)\n",
    "#         rankcols_train =[col for col in train.columns if ('rank_' in col)]\n",
    "#         if len(rankcols_train)!=0:\n",
    "#             train.drop(rankcols_train,axis=1,inplace=True)\n",
    "#         rankcols_test =[col for col in test.columns if ('rank_' in col)]\n",
    "#         if len(rankcols_test)!=0:\n",
    "#             test.drop(rankcols_test,axis=1,inplace=True)\n",
    "        for i,df in enumerate([combined]):\n",
    "            print('df :',i)\n",
    "            for col in rankasccols:\n",
    "                print('col:',col)\n",
    "                df['rank_'+col] =  df[col].rank()\n",
    "    #             df['rank_'+col] = weightasccols[i] * df[col].rank()\n",
    "\n",
    "            for col in rankdesccols:\n",
    "                print('col:',col)\n",
    "                df['rank_'+col] = df[col].rank(ascending=False)\n",
    "    #             df['rank_'+col] = weightdesccols[i] * df[col].rank(ascending=False)\n",
    "\n",
    "            rankcols =[col for col in df.columns if ('rank_' in col)\n",
    "                       & ('hist_purchase_amount_per_install_sum' !=col)\n",
    "#                        & ('hist_card_id_size' not in col)\n",
    "                        \n",
    "                      ]\n",
    "            df['rank_sum'] = df[rankcols].sum(axis=1) / 1E+5\n",
    "            df['rank_mean'] = df[rankcols].mean(axis=1) / 1E+5\n",
    "    #         weight_sum = (np.sum(np.array(weightasccols)) + np.sum(np.array(weightdesccols)))\n",
    "    #         df['rank_mean'] = df[rankcols].sum(axis=1) / (1E+5 * weight_sum)\n",
    "\n",
    "    #         df['rank_sum'] = df[rankcols].product(axis=1) / pow(10,len(rankdesccols) + len (rankasccols))\n",
    "\n",
    "        return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "_uuid": "b9c71678ad474a0c7cf0aa4caeb374e8dda14283"
   },
   "outputs": [],
   "source": [
    "def rank_postproc(combined):\n",
    "    for df in [combined]:\n",
    "        df['rank_rank_sum']= df['rank_sum'].rank()\n",
    "        col = 'rank_hist_purchase_amount_per_install_sum'\n",
    "        df['rank_diff_' + col]= df[col] - df['rank_rank_sum']\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7664c74d5ed84d82d0a045b5f2862cf813a52fd8"
   },
   "source": [
    "## training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "_uuid": "10226e5a243d63d96ae40832b06c1043ff6d7a55"
   },
   "outputs": [],
   "source": [
    "sel_enc_cols =['trans_merged_targetenc_merchant_id_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "_uuid": "5fec6e0fec8540b45c2510eca33c63480227a755"
   },
   "outputs": [],
   "source": [
    "sel_feats  = ['elapsed_time',\n",
    " 'hist_authorized_flag_mean',\n",
    " 'hist_authorized_flag_sum',\n",
    " 'hist_category_1_mean',\n",
    " 'hist_category_1_sum',\n",
    " 'hist_installments_sum',\n",
    " 'hist_month_diff_mean',\n",
    " 'hist_month_lag_mean',\n",
    " 'hist_month_lag_min',\n",
    " 'hist_month_lag_var',\n",
    " 'hist_month_nunique',\n",
    " 'hist_most_recent_sales_range_std',\n",
    " 'hist_purchase_date_diff',\n",
    " 'hist_purchase_date_max',\n",
    " 'hist_purchase_date_min',\n",
    " 'hist_purchase_date_uptonow',\n",
    " 'hist_weekofyear_nunique',\n",
    " 'new_hist_category_1_mean',\n",
    " 'new_hist_category_1_sum',\n",
    " 'new_hist_month_lag_mean',\n",
    " 'new_hist_purchase_date_diff',\n",
    " 'new_hist_purchase_date_max',\n",
    " 'new_hist_purchase_date_uptonow' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "_uuid": "18979ad6cfbdb9540d67930b067011d456f8a733"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:3: FutureWarning:\n",
      "\n",
      "Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************ Pre proc complete ********************************\n",
      "df : 0\n",
      "col: hist_category_2_raw_mean\n",
      "col: hist_purchase_amount_per_install_sum\n",
      "col: hist_purchase_amount_per_install_mean\n",
      "col: hist_card_id_size\n",
      "col: new_hist_purchase_date_min\n",
      "col: hist_purchase_date_min\n",
      "col: hist_month_lag_0_sum\n",
      "col: hist_month_lag_-1_sum\n",
      "col: hist_month_lag_-2_sum\n",
      "col: hist_month_lag_-3_sum\n",
      "col: hist_purchase_date_diff\n",
      "col: hist_authorized_flag_mean\n",
      "col: hist_authorized_flag_sum\n",
      "col: new_hist_card_id_size\n",
      "col: new_hist_purchase_amount_per_install_sum\n",
      "col: hist_category_3_A_mean\n",
      "col: hist_month_lag_-12_mean\n",
      "col: hist_month_lag_-13_sum\n",
      "col: new_hist_purchase_date_diff\n",
      "col: hist_purchase_amount_per_install_mean\n",
      "col: elapsed_time\n",
      "col: hist_size_per_merchant\n",
      "col: new_hist_first_buy\n",
      "col: new_hist_card_id_size\n",
      "col: hist_first_buy\n",
      "col: hist_category_1_mean\n",
      "col: hist_installments_mean\n",
      "col: hist_purchase_date_count_mean\n",
      "col: new_hist_purchase_date_uptonow\n",
      "col: hist_purchase_date_count_max\n",
      "col: hist_purchase_date_diff\n",
      "col: hist_month_lag_-4_sum\n",
      "col: hist_month_lag_-5_sum\n",
      "col: hist_month_lag_-6_sum\n",
      "\n",
      "************ Generate Rank complete ********************************\n",
      "\n",
      "************ Post proc complete ********************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['istrain']=1\n",
    "df_test['istrain']=0\n",
    "combined = pd.concat([df_train,df_test])\n",
    "\n",
    "combined = rank_preproc(combined)\n",
    "print()\n",
    "print('************ Pre proc complete ********************************')\n",
    "combined = gen_newfeats_outlier(combined)\n",
    "\n",
    "combined = generate_rank_outlier(combined)\n",
    "print()\n",
    "print('************ Generate Rank complete ********************************')\n",
    "combined = rank_postproc(combined)\n",
    "print()\n",
    "print('************ Post proc complete ********************************')\n",
    "\n",
    "#separate back combined to train and test\n",
    "df_train=combined[combined['istrain']==1]\n",
    "df_test=combined[combined['istrain']==0]\n",
    "del combined;gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "_uuid": "ceea775be65ea9a0cfbc716dbe7e48eb7d31e912"
   },
   "outputs": [],
   "source": [
    "new_feats_cls = new_feats_reg.copy()\n",
    "\n",
    "rankcols = [col for col in df_train.columns if ('rank_' in col)\n",
    "           & ('rank_sum' not in col )\n",
    "           & ('rank_diff' not in col)\n",
    "           & ('rank_mean' not in col)]\n",
    "\n",
    "new_feats_cls += rankcols + ['rank_diff_rank_hist_purchase_amount_per_install_sum']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "_uuid": "c18ce04402ada987e17687b9b4d3a460b5fd9179"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import brier_score_loss\n",
    "def lgb_sk_f1(labels,preds):\n",
    "    optcutoff,f1score = get_opt_cutoff_prec(labels,preds)\n",
    "    return 'f1_score', f1score, True\n",
    "\n",
    "def lgb_f1(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "#     preds = preds.reshape(-1, 2)\n",
    "#     print('preds before :',preds[0:10])\n",
    "#     preds = preds.argmax(axis = 1)\n",
    "#     preds = preds.min(axis = 1)\n",
    "#     f1score = f1_score(preds, labels, average = 'micro')\n",
    "    optcutoff,f1score = get_opt_cutoff_prec(labels,preds)\n",
    "    \n",
    "#     bscore = brier_score_loss(labels,preds) \n",
    "#     bss = 1- (bscore/0.010944928684627575) \n",
    "    return 'f1_score', f1score, True\n",
    "\n",
    "def get_opt_cutoff_prec(labels,preds):\n",
    "    precision, recall, thresholds  = precision_recall_curve(labels, preds)\n",
    "    f1_score= 2*((precision*recall)/(precision+recall))\n",
    "    optimal_idx = np.nanargmax(f1_score)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    return optimal_threshold, f1_score[optimal_idx]\n",
    "\n",
    "def get_opt_cutoff_recall(labels,preds):\n",
    "    precision, recall, thresholds  = precision_recall_curve(labels, preds)\n",
    "    optimal_idx = np.nanargmax(recall)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    return optimal_threshold, recall[optimal_idx]\n",
    "\n",
    "def convert_probtolabels(preds,cutoff=0.5):\n",
    "    y_bin= preds.copy()\n",
    "    y_bin[preds>cutoff] = 1\n",
    "    y_bin[preds<=cutoff] = 0\n",
    "    y_bin=y_bin.astype(int)\n",
    "\n",
    "    return y_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "_uuid": "4e9e15c4c9c6cf198da80d9527ee41c5aaf1cce7"
   },
   "outputs": [],
   "source": [
    "num_round = 10000\n",
    "n_splits =5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "_uuid": "68cdbe94d19a35326312fece5bd352a7fc02f85e"
   },
   "outputs": [],
   "source": [
    "# param = {'num_leaves': 31,\n",
    "#          'min_data_in_leaf': 30, \n",
    "#          'objective':'binary',\n",
    "# #          'num_class':2,\n",
    "#          'max_depth': -1,\n",
    "#          'learning_rate': 0.01,\n",
    "#          \"min_child_samples\": 20,\n",
    "#          \"boosting\": \"gbdt\",\n",
    "#          \"feature_fraction\": 0.9,\n",
    "#          \"bagging_freq\": 1,\n",
    "#          \"bagging_fraction\": 0.9 ,\n",
    "#          \"bagging_seed\": 11,\n",
    "#          \"metric\": 'binary_logloss',\n",
    "# #          \"metric\": 'auc',\n",
    "# #          \"metric\": 'multi_logloss',\n",
    "#          \"lambda_l1\": 0.1,\n",
    "#          \"verbosity\": -1,\n",
    "#          \"nthread\": 4,\n",
    "#          'n_estimators' : 10000,\n",
    "#          \"random_state\": 4590}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "_uuid": "6c7e52c0f7df6bb771ca9517e65d1caae85a444f"
   },
   "outputs": [],
   "source": [
    "\n",
    "# features = list(df_train.columns) + sel_enc_cols\n",
    "# exclude_cols = ['card_id','first_active_month','target','outliers']\n",
    "# features = [col for col in features if col not in exclude_cols]\n",
    "\n",
    "# new_feats_cls =new_feats_cls[0:5]\n",
    "# param['n_estimators'] =10\n",
    "features = sel_feats + sel_enc_cols + new_feats_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "_uuid": "6c7e52c0f7df6bb771ca9517e65d1caae85a444f"
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# #Run permute features\n",
    "# fold_importance_df,predictions,oof,train_outlier_preds,overall_imp_df,overall_sel_feats\\\n",
    "#     = runlgb(True,df_train,df_test,param,features,score_function=get_f1loss_score,regression=False,\n",
    "#             subset_feats = new_feats_cls,\n",
    "#             feval=lgb_sk_f1,\n",
    "#                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "_uuid": "c7ebeee5cee17e2304d8dbeedfe19b808f09457c"
   },
   "outputs": [],
   "source": [
    "# selected_features_cls = list(overall_imp_df.loc[overall_imp_df['overall_score_mean'] >= max_delta_score, 'feature'])\n",
    "# selected_features_cls.sort()\n",
    "# df = pd.DataFrame( np.array(selected_features_cls))\n",
    "# df.to_csv(\"overall_selected_features_cls.csv\")\n",
    "# selected_features_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "_uuid": "aded90d9ad021d9df2687cf30d48d73991f39753"
   },
   "outputs": [],
   "source": [
    "# overall_imp_df.sort_values(by='overall_score_mean',ascending=False,inplace=True)\n",
    "# overall_imp_df.to_csv('overall_feats_allfolds_cls.csv')\n",
    "# overall_imp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "_uuid": "7b69851c4f1dac7fb57227414a8c3caf11758f0f"
   },
   "outputs": [],
   "source": [
    "# overall_sel_feats[0].sort()\n",
    "# df = pd.DataFrame( np.array(overall_sel_feats[0]))\n",
    "# df.to_csv(\"sel_feats_0_cls.csv\")\n",
    "# overall_sel_feats[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "_uuid": "c6d7ead7d1f9bd7365b833a0faef6f35621ace36"
   },
   "outputs": [],
   "source": [
    "# overall_sel_feats[1].sort()\n",
    "# df = pd.DataFrame( np.array(overall_sel_feats[1]))\n",
    "# df.to_csv(\"sel_feats_1_cls.csv\")\n",
    "# overall_sel_feats[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "_uuid": "dfa16ef53460ea5190ff8c97d91d18483ea3a69a"
   },
   "outputs": [],
   "source": [
    "# overall_sel_feats[2].sort()\n",
    "# df = pd.DataFrame( np.array(overall_sel_feats[2]))\n",
    "# df.to_csv(\"sel_feats_2_cls.csv\")\n",
    "# overall_sel_feats[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "_uuid": "6cdd54421b2661554b679ddbd04e069585eea3c2"
   },
   "outputs": [],
   "source": [
    "# overall_sel_feats[3].sort()\n",
    "# df = pd.DataFrame( np.array(overall_sel_feats[3]))\n",
    "# df.to_csv(\"sel_feats_3_cls.csv\")\n",
    "# overall_sel_feats[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "_uuid": "381287f54842a5379a02471f6c1b791cc20547ea"
   },
   "outputs": [],
   "source": [
    "# overall_sel_feats[4].sort()\n",
    "# df = pd.DataFrame( np.array(overall_sel_feats[4]))\n",
    "# df.to_csv(\"sel_feats_4_cls.csv\")\n",
    "# overall_sel_feats[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "_uuid": "8bf539fdad2e43693c45d62a13b37692019e850d"
   },
   "outputs": [],
   "source": [
    "selected_features_cls =['hist_category_2_raw_mean', 'hist_category_3_A_mean', 'hist_category_3_A_sum',\n",
    " 'hist_city_id_nunique', 'hist_month_lag_-1_mean', 'hist_month_lag_-2_mean', 'hist_month_lag_-5_mean',\n",
    " 'hist_month_lag_-6_mean', 'hist_month_lag_-7_mean', 'hist_month_lag_-8_mean', 'hist_month_lag_-8_sum',\n",
    " 'hist_month_lag_-9_mean', 'hist_month_lag_-9_sum', 'hist_month_lag_0_mean', 'hist_month_lag_0_sum',\n",
    " 'hist_purchase_amount_per_install_mean', 'hist_purchase_amount_per_install_sum',\n",
    " 'hist_purchase_amount_per_install_var', 'hist_purchase_amount_scaled_sum', 'new_hist_category_2_raw_mean',\n",
    " 'new_hist_category_3_A_sum', 'new_hist_category_3_raw_mean', 'new_hist_month_lag_1_mean',\n",
    " 'new_hist_month_lag_1_sum', 'new_hist_purchase_amount_scaled_sum', 'new_hist_purchase_amount_scaled_var',\n",
    " 'rank_diff_rank_hist_purchase_amount_per_install_sum', 'rank_elapsed_time',\n",
    " 'rank_hist_authorized_flag_sum', 'rank_hist_category_2_raw_mean', 'rank_hist_first_buy',\n",
    " 'rank_hist_installments_mean', 'rank_hist_month_lag_-1_sum', 'rank_hist_month_lag_-4_sum',\n",
    " 'rank_hist_month_lag_-5_sum', 'rank_hist_month_lag_-6_sum', 'rank_hist_purchase_date_count_mean',\n",
    " 'rank_hist_purchase_date_diff', 'rank_hist_size_per_merchant',\n",
    " 'rank_new_hist_purchase_date_min', 'rank_new_hist_purchase_date_uptonow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "_uuid": "02d99fdfe8a314b6201f22e75d38768fc2c7e205"
   },
   "outputs": [],
   "source": [
    "overall_sel_feats_cls =[0,1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "_uuid": "2f413d621aef028eda52b951d99ca4635a7944a6"
   },
   "outputs": [],
   "source": [
    "overall_sel_feats_cls[0]=['hist_category_2_raw_mean', 'hist_category_3_A_mean',\n",
    " 'hist_city_id_nunique', 'hist_month_lag_-2_mean', 'hist_month_lag_-6_mean',\n",
    " 'hist_month_lag_-8_sum', 'hist_month_lag_-9_mean', 'hist_month_lag_0_sum',\n",
    " 'hist_purchase_amount_per_install_mean', 'hist_purchase_amount_per_install_sum',\n",
    " 'hist_purchase_amount_per_install_var', 'hist_purchase_amount_scaled_mean',\n",
    " 'new_hist_category_3_A_mean', 'new_hist_category_3_A_sum',\n",
    " 'new_hist_city_id_nunique', 'new_hist_purchase_amount_per_install_sum',\n",
    " 'new_hist_purchase_amount_per_install_var', 'new_hist_purchase_amount_scaled_var',\n",
    " 'rank_diff_rank_hist_purchase_amount_per_install_sum', 'rank_elapsed_time',\n",
    " 'rank_hist_authorized_flag_mean', 'rank_hist_category_1_mean',\n",
    " 'rank_hist_category_2_raw_mean', 'rank_hist_first_buy',\n",
    " 'rank_hist_installments_mean', 'rank_hist_month_lag_-1_sum',\n",
    " 'rank_hist_month_lag_-6_sum', 'rank_hist_purchase_amount_per_install_mean',\n",
    " 'rank_new_hist_first_buy', 'rank_new_hist_purchase_amount_per_install_sum',\n",
    " 'rank_new_hist_purchase_date_diff', 'rank_new_hist_purchase_date_min',\n",
    " 'rank_new_hist_purchase_date_uptonow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "_uuid": "29ce211fa01f6100e877f653c199a9319a03c221"
   },
   "outputs": [],
   "source": [
    "overall_sel_feats_cls[1]=['hist_category_2_raw_mean', 'hist_category_3_A_sum', 'hist_category_3_raw_mean',\n",
    " 'hist_city_id_nunique', 'hist_month_lag_-1_mean', 'hist_month_lag_-1_sum',\n",
    " 'hist_month_lag_-2_mean', 'hist_month_lag_-2_sum', 'hist_month_lag_-4_mean',\n",
    " 'hist_month_lag_-6_mean', 'hist_month_lag_-7_mean', 'hist_month_lag_-9_sum',\n",
    " 'hist_month_lag_0_mean', 'hist_month_lag_0_sum', 'hist_purchase_amount_per_install_mean',\n",
    " 'hist_purchase_amount_per_install_sum', 'hist_purchase_amount_per_install_var',\n",
    " 'new_hist_category_3_A_sum', 'new_hist_category_3_raw_mean', 'new_hist_month_lag_1_mean',\n",
    " 'new_hist_purchase_amount_scaled_mean', 'rank_elapsed_time', 'rank_hist_authorized_flag_sum',\n",
    " 'rank_hist_category_2_raw_mean', 'rank_hist_installments_mean', 'rank_hist_month_lag_-1_sum',\n",
    " 'rank_hist_month_lag_-3_sum', 'rank_hist_purchase_amount_per_install_sum', 'rank_hist_purchase_date_count_mean',\n",
    " 'rank_hist_purchase_date_diff', 'rank_hist_size_per_merchant', 'rank_new_hist_first_buy',\n",
    " 'rank_new_hist_purchase_amount_per_install_sum', 'rank_new_hist_purchase_date_diff',\n",
    " 'rank_new_hist_purchase_date_min', 'rank_new_hist_purchase_date_uptonow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "_uuid": "387844b9e65e820fd9e72e017b5317eea7859cc7"
   },
   "outputs": [],
   "source": [
    "overall_sel_feats_cls[2]=['hist_category_2_raw_mean', 'hist_category_3_raw_mean', 'hist_city_id_nunique',\n",
    " 'hist_month_lag_-11_mean', 'hist_month_lag_-1_mean', 'hist_month_lag_-2_mean',\n",
    " 'hist_month_lag_-3_mean', 'hist_month_lag_-4_mean', 'hist_month_lag_-5_mean',\n",
    " 'hist_month_lag_-5_sum', 'hist_month_lag_-6_mean', 'hist_month_lag_-7_mean',\n",
    " 'hist_month_lag_-8_mean', 'hist_month_lag_-8_sum', 'hist_month_lag_-9_mean',\n",
    " 'hist_month_lag_0_mean', 'hist_month_lag_0_sum',\n",
    " 'hist_purchase_amount_per_install_mean', 'hist_purchase_amount_per_install_sum',\n",
    " 'hist_purchase_amount_per_install_var', 'hist_purchase_amount_scaled_mean',\n",
    " 'hist_purchase_amount_scaled_sum', 'hist_purchase_amount_scaled_var', 'new_hist_category_2_raw_mean',\n",
    " 'new_hist_category_3_A_sum', 'new_hist_category_3_raw_mean', 'new_hist_month_lag_1_mean',\n",
    " 'new_hist_month_lag_1_sum', 'new_hist_month_lag_2_sum', 'new_hist_purchase_amount_per_install_mean',\n",
    " 'new_hist_purchase_amount_per_install_sum', 'new_hist_purchase_amount_scaled_sum',\n",
    " 'new_hist_purchase_amount_scaled_var', 'rank_diff_rank_hist_purchase_amount_per_install_sum',\n",
    " 'rank_hist_authorized_flag_sum', 'rank_hist_first_buy', 'rank_hist_installments_mean',\n",
    " 'rank_hist_month_lag_-4_sum', 'rank_hist_month_lag_-5_sum', 'rank_hist_month_lag_-6_sum',\n",
    " 'rank_hist_purchase_date_count_max', 'rank_hist_purchase_date_count_mean', 'rank_hist_purchase_date_diff',\n",
    " 'rank_hist_size_per_merchant', 'rank_new_hist_card_id_size', 'rank_new_hist_purchase_date_min',\n",
    " 'rank_new_hist_purchase_date_uptonow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "_uuid": "5c3d437929f7d17c71295860d2154da5ce325ce9"
   },
   "outputs": [],
   "source": [
    "overall_sel_feats_cls[3]=['hist_category_2_raw_mean', 'hist_category_3_A_sum', 'hist_city_id_nunique',\n",
    " 'hist_month_lag_-10_mean', 'hist_month_lag_-3_mean', 'hist_month_lag_-3_sum', 'hist_month_lag_-4_sum',\n",
    " 'hist_month_lag_-5_sum', 'hist_month_lag_-6_mean', 'hist_month_lag_-9_mean', 'hist_month_lag_-9_sum',\n",
    " 'hist_month_lag_0_sum', 'hist_purchase_amount_per_install_mean', 'hist_purchase_amount_per_install_sum',\n",
    " 'hist_purchase_amount_per_install_var', 'hist_purchase_amount_scaled_sum', 'new_hist_category_3_A_sum',\n",
    " 'new_hist_city_id_nunique', 'new_hist_month_lag_1_mean', 'new_hist_month_lag_1_sum', 'new_hist_month_lag_2_sum',\n",
    " 'new_hist_purchase_amount_per_install_mean', 'new_hist_purchase_amount_per_install_sum',\n",
    " 'new_hist_purchase_amount_per_install_var', 'rank_diff_rank_hist_purchase_amount_per_install_sum',\n",
    " 'rank_hist_authorized_flag_sum', 'rank_hist_category_2_raw_mean', 'rank_hist_month_lag_-5_sum',\n",
    " 'rank_hist_month_lag_-6_sum', 'rank_hist_purchase_amount_per_install_mean', 'rank_hist_purchase_date_count_max',\n",
    " 'rank_hist_purchase_date_min', 'rank_hist_size_per_merchant', 'rank_new_hist_first_buy',\n",
    " 'rank_new_hist_purchase_amount_per_install_sum', 'rank_new_hist_purchase_date_min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "_uuid": "c7ed3e17121bfd3f69033285b2fae311cbce6ced"
   },
   "outputs": [],
   "source": [
    "overall_sel_feats_cls[4]=['hist_category_2_raw_mean', 'hist_category_3_A_mean', 'hist_city_id_nunique',\n",
    " 'hist_month_lag_-1_mean', 'hist_month_lag_-1_sum', 'hist_month_lag_-2_sum', 'hist_month_lag_-5_mean',\n",
    " 'hist_month_lag_-7_mean', 'hist_month_lag_-9_mean', 'hist_month_lag_0_mean', 'hist_month_lag_0_sum',\n",
    " 'hist_purchase_amount_per_install_var', 'hist_purchase_amount_scaled_mean', 'new_hist_category_3_raw_mean',\n",
    " 'new_hist_month_lag_1_mean', 'new_hist_month_lag_1_sum', 'new_hist_purchase_amount_per_install_mean',\n",
    " 'new_hist_purchase_amount_scaled_sum', 'new_hist_purchase_amount_scaled_var',\n",
    " 'rank_diff_rank_hist_purchase_amount_per_install_sum', 'rank_elapsed_time', 'rank_hist_first_buy',\n",
    " 'rank_hist_installments_mean', 'rank_hist_month_lag_-1_sum', 'rank_hist_month_lag_-4_sum',\n",
    " 'rank_hist_month_lag_-5_sum', 'rank_hist_month_lag_-6_sum', 'rank_hist_month_lag_0_sum',\n",
    " 'rank_hist_purchase_date_diff', 'rank_hist_purchase_date_min', 'rank_hist_size_per_merchant',\n",
    " 'rank_new_hist_card_id_size', 'rank_new_hist_first_buy', 'rank_new_hist_purchase_date_diff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "_uuid": "35f0e858761722d3eb5267d2c5ddda66971b5e68"
   },
   "outputs": [],
   "source": [
    "# overall_sel_feats=[0,1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "_uuid": "6f8133bd6c18ffb22bfc2b925875b119b0024d8e"
   },
   "outputs": [],
   "source": [
    "# overall_sel_feats[0] =['hist_month_lag_-2_mean', 'hist_month_lag_-3_mean',\n",
    "#  'hist_purchase_amount_scaled_var', 'new_hist_month_lag_1_mean',\n",
    "#  'new_hist_purchase_amount_per_install_mean', 'new_hist_purchase_amount_per_install_var',\n",
    "#  'new_hist_purchase_amount_scaled_mean', 'rank_hist_authorized_flag_mean',\n",
    "#  'rank_hist_card_id_size', 'rank_hist_category_1_mean', 'rank_hist_first_buy',\n",
    "#  'rank_hist_installments_mean', 'rank_hist_month_lag_-6_sum', 'rank_hist_purchase_date_count_mean',\n",
    "#  'rank_hist_purchase_date_diff', 'rank_hist_purchase_date_min', 'rank_hist_size_per_merchant',\n",
    "#                       'rank_diff_rank_hist_purchase_amount_per_install_sum'\n",
    "#                       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "_uuid": "89c991a66217eeaf05aed6e4ba4def9ea0e4cc2e"
   },
   "outputs": [],
   "source": [
    "# overall_sel_feats[1] =['hist_category_3_raw_mean', 'hist_month_lag_-1_mean', 'hist_month_lag_-2_sum',\n",
    "#  'hist_month_lag_-3_mean', 'hist_month_lag_-4_mean', 'hist_month_lag_-4_sum', 'hist_month_lag_-5_sum',\n",
    "#  'hist_month_lag_-6_sum', 'hist_month_lag_0_mean', 'hist_month_lag_0_sum',\n",
    "#  'hist_purchase_amount_per_install_mean', 'hist_purchase_amount_per_install_sum',\n",
    "#  'hist_purchase_amount_scaled_mean', 'new_hist_category_2_raw_mean',\n",
    "#  'new_hist_purchase_amount_per_install_sum', 'new_hist_purchase_amount_scaled_mean',\n",
    "#  'rank_diff_rank_hist_purchase_amount_per_install_sum', 'rank_elapsed_time',\n",
    "#  'rank_hist_authorized_flag_mean', 'rank_hist_category_1_mean', 'rank_hist_category_2_raw_mean', \n",
    "#  'rank_hist_first_buy', 'rank_hist_installments_mean', 'rank_hist_month_lag_-4_sum',\n",
    "#  'rank_hist_purchase_date_count_max', 'rank_hist_purchase_date_count_mean', 'rank_hist_purchase_date_min',\n",
    "#  'rank_hist_size_per_merchant', 'rank_new_hist_purchase_amount_per_install_sum',\n",
    "#  'rank_new_hist_purchase_date_diff', 'rank_new_hist_purchase_date_min',\n",
    "#  'rank_new_hist_purchase_date_uptonow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "_uuid": "548bf964dd6474f40a3ead0e85e62a49a14d3236"
   },
   "outputs": [],
   "source": [
    "# overall_sel_feats[2] =['hist_month_lag_-1_mean', 'hist_month_lag_0_sum',\n",
    "#  'hist_purchase_amount_per_install_mean', 'hist_purchase_amount_per_install_var',\n",
    "#  'new_hist_month_lag_1_mean', 'new_hist_purchase_amount_per_install_mean',\n",
    "#  'new_hist_purchase_amount_scaled_mean', 'rank_hist_category_2_raw_mean',\n",
    "#  'rank_hist_purchase_amount_per_install_mean', 'rank_hist_purchase_amount_per_install_sum',\n",
    "#  'rank_hist_purchase_date_count_max', 'rank_new_hist_purchase_date_uptonow',\n",
    "#                       'rank_diff_rank_hist_purchase_amount_per_install_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "_uuid": "64b7e724c53a302d6de532e5a19ce9b48e2c0bd0"
   },
   "outputs": [],
   "source": [
    "# overall_sel_feats[3] =['hist_purchase_amount_per_install_mean', 'hist_purchase_amount_per_install_sum',\n",
    "#  'hist_purchase_amount_per_install_var', 'new_hist_purchase_amount_per_install_sum',\n",
    "#  'new_hist_purchase_amount_scaled_mean', 'new_hist_purchase_amount_scaled_sum',\n",
    "#  'rank_elapsed_time', 'rank_hist_category_1_mean', 'rank_hist_first_buy',\n",
    "#  'rank_hist_month_lag_-6_sum', 'rank_hist_month_lag_0_sum',\n",
    "#  'rank_hist_purchase_date_count_max', 'rank_hist_purchase_date_count_mean', 'rank_hist_purchase_date_min',\n",
    "#  'rank_hist_size_per_merchant', 'rank_new_hist_purchase_date_min', 'rank_new_hist_purchase_date_uptonow',\n",
    "#                       'rank_diff_rank_hist_purchase_amount_per_install_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "_uuid": "53922f4827f17956f955bc90c7c8d584a0eaf226"
   },
   "outputs": [],
   "source": [
    "# overall_sel_feats[4] =['hist_month_lag_-1_mean',\n",
    "#  'hist_purchase_amount_per_install_mean', 'hist_purchase_amount_per_install_sum',\n",
    "#  'hist_purchase_amount_per_install_var', 'hist_purchase_amount_scaled_sum',\n",
    "#  'new_hist_category_2_raw_mean', 'new_hist_month_lag_1_mean',\n",
    "#  'new_hist_purchase_amount_scaled_mean', 'new_hist_purchase_amount_scaled_sum',\n",
    "#  'rank_elapsed_time', 'rank_hist_authorized_flag_mean', 'rank_hist_category_1_mean',\n",
    "#  'rank_hist_first_buy', 'rank_hist_installments_mean', 'rank_hist_purchase_date_count_max',\n",
    "#  'rank_hist_purchase_date_count_mean', 'rank_hist_size_per_merchant', 'rank_new_hist_first_buy',\n",
    "#  'rank_new_hist_purchase_date_min', 'rank_new_hist_purchase_date_uptonow',\n",
    "#                       'rank_diff_rank_hist_purchase_amount_per_install_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "_uuid": "4ec994e039e86df7a9dd35fa57c973138e82ed6a"
   },
   "outputs": [],
   "source": [
    "num_round = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "_uuid": "0b52ab2e4b930353709fb7932fa3e89d2229812c"
   },
   "outputs": [],
   "source": [
    "#Run for fold-wise selected feature model training and predictions\n",
    "for i,val in enumerate(overall_sel_feats_cls):\n",
    "    if i==4:\n",
    "        overall_sel_feats_cls[i] = sel_feats + sel_enc_cols + new_feats_cls\n",
    "    else:\n",
    "        overall_sel_feats_cls[i] += sel_feats + sel_enc_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "_uuid": "3d26ea74bf0940b19df9a5a040bd9d576b2b34f7"
   },
   "outputs": [],
   "source": [
    "# features = sel_feats + sel_enc_cols + new_feats_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "_uuid": "e9092013ce11d2c4600a95dad8297bb9b38a2fc5"
   },
   "outputs": [],
   "source": [
    "# print(df_train['hist_card_id_size'].describe())\n",
    "# print(df_train[df_train['hist_card_id_size']<=10].shape)\n",
    "\n",
    "# cardsizes = [20,35,50,75,100,150,10000]\n",
    "# prevval =0\n",
    "# for val in cardsizes:\n",
    "#     mask = (df_train['hist_card_id_size']<=val) & (df_train['hist_card_id_size']>prevval)\n",
    "#     mask_outlier  = df_train['outliers']==1\n",
    "#     print()\n",
    "#     totalsize = df_train[mask].shape[0]\n",
    "#     print('card size:',val)\n",
    "#     print('total:',totalsize)\n",
    "#     outliersize = df_train[mask & mask_outlier].shape[0]\n",
    "#     print('outliers:',outliersize)\n",
    "#     print('non outliers:',df_train[(mask) & (~mask_outlier)].shape[0])\n",
    "#     prevval = val\n",
    "#     outlierratio = outliersize / totalsize\n",
    "#     print('ratio:',outlierratio)\n",
    "# # print(df_train[(df_train['hist_card_id_size']<=20) & (df_train['hist_card_id_size']>10)].shape)\n",
    "# # print(df_train[(df_train['hist_card_id_size']<=50) & (df_train['hist_card_id_size']>20)].shape)\n",
    "# # print(df_train[(df_train['hist_card_id_size']<=100) & (df_train['hist_card_id_size']>50)].shape)\n",
    "# # print(df_train[(df_train['hist_card_id_size']<=500) & (df_train['hist_card_id_size']>100)].shape)\n",
    "# # print(df_train[(df_train['hist_card_id_size']<=1000) & (df_train['hist_card_id_size']>500)].shape)\n",
    "# # print(df_train[(df_train['hist_card_id_size']<=5000) & (df_train['hist_card_id_size']>1000)].shape)\n",
    "# # print(df_train[(df_train['hist_card_id_size']<=10000) & (df_train['hist_card_id_size']>5000)].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "_uuid": "fcb1f9a19d3fbd3d69e609349034981a2189b81f"
   },
   "outputs": [],
   "source": [
    "# # df_train_filtered = df_train.copy()\n",
    "# # df_train_filtered.sort_values('hist_card_id_size',inplace=True)\n",
    "# df_train_filtered = df_train.copy()\n",
    "# df_train_filtered['target'] = target\n",
    "# # df_train_filtered = df_train_filtered[df_train_filtered['hist_card_id_size'] <=10]\n",
    "# mask  = (df_train_filtered['hist_card_id_size'] >10) & (df_train_filtered['hist_card_id_size'] <20)\n",
    "# test_mask  = (df_test['hist_card_id_size'] >10) & (df_test['hist_card_id_size'] <20)\n",
    "# df_train_filtered = df_train_filtered[mask]\n",
    "# target_filtered = df_train_filtered['target']\n",
    "# del df_train_filtered['target']\n",
    "# df_test_filtered = df_test[test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "_uuid": "f98944ff1593fe65a75e2887ccbe2de3d4843460"
   },
   "outputs": [],
   "source": [
    "# target_filtered.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "_uuid": "84f83a291a841663338cf10d452d6a6c56d4e056"
   },
   "outputs": [],
   "source": [
    "# df_train_filtered.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "_uuid": "93ab0167db55672a584e0928065f19f825e782a2"
   },
   "outputs": [],
   "source": [
    "oof_labels_p_prob = np.zeros(len(df_train))\n",
    "oof_rank_p_labels_p_prob = np.zeros(len(df_train))\n",
    "\n",
    "oof_rank = np.zeros(len(df_train))\n",
    "oof_rank_prob = np.zeros(len(df_train))\n",
    "oof_rank_p_labels = np.zeros(len(df_train))    \n",
    "oof_labels = np.zeros(len(df_train))\n",
    "oof_rank_p_prob = np.zeros(len(df_train))\n",
    "oof_rank_p_prob_prop_high = np.zeros(len(df_train))\n",
    "oof_rank_p_prob_prop_low = np.zeros(len(df_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "_uuid": "8084bc55eda09e17010e2e576439e37321c4106f"
   },
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "fold_to_start = None\n",
    "fold_to_stop= None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "_uuid": "243ccb2ab8da85c4e3a368bab8e5c2b8f34b920d"
   },
   "outputs": [],
   "source": [
    "param ={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "_uuid": "f4c204fb96ee8fc6305f776667deda57aa337142"
   },
   "outputs": [],
   "source": [
    "param = {'num_leaves': 31,\n",
    "         'min_data_in_leaf': 30, \n",
    "         'objective':'binary',\n",
    "#          'num_class':2,\n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.01,\n",
    "         \"min_child_samples\": 20,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9 ,\n",
    "         \"bagging_seed\": 11,\n",
    "#          \"metric\": 'binary_logloss',\n",
    "         \"metric\": 'auc',\n",
    "#          \"metric\": 'multi_logloss',\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"verbosity\": -1,\n",
    "         \"nthread\": 4,\n",
    "         'n_estimators' : 10000,\n",
    "         \"random_state\": 4590}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "_uuid": "d47e4db35090fcdbfae27fd2c9ea75cf39f42820"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fold nÂ°0\n",
      "y_train shape: (201917,)\n",
      "val shape: (40384, 249)\n",
      "y_val shape: (40384,)\n",
      "tr shape: (161533, 250)\n",
      "val shape: (40384, 250)\n",
      "yval 1 shape: (442,)\n",
      "yval 0 shape: (39942,)\n",
      "yval 1 ratio: 0.010944928684627575\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\tvalid_0's auc: 0.900823\n",
      "[200]\tvalid_0's auc: 0.905773\n",
      "[300]\tvalid_0's auc: 0.907993\n",
      "[400]\tvalid_0's auc: 0.908254\n",
      "[500]\tvalid_0's auc: 0.908363\n",
      "[600]\tvalid_0's auc: 0.908469\n",
      "[700]\tvalid_0's auc: 0.908479\n",
      "Early stopping, best iteration is:\n",
      "[555]\tvalid_0's auc: 0.908604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:21: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:21: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt_cutoff: 0.04480101898516457\n",
      "f1 score after calib: 0.2655935613682092\n",
      "auc score after calib: 0.9086044674279968\n",
      "log loss score after calib: 0.04984 \n",
      "CUMULATIVE SCORE\n",
      "opt_cutoff: 0.04480101898516457\n",
      "f1 score: 0.2655935613682092\n",
      "conf matrix: [[39522   420]\n",
      " [  311   131]]\n",
      "\n",
      "fold nÂ°1\n",
      "y_train shape: (201917,)\n",
      "val shape: (40384, 249)\n",
      "y_val shape: (40384,)\n",
      "tr shape: (161533, 250)\n",
      "val shape: (40384, 250)\n",
      "yval 1 shape: (442,)\n",
      "yval 0 shape: (39942,)\n",
      "yval 1 ratio: 0.010944928684627575\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\tvalid_0's auc: 0.901992\n",
      "[200]\tvalid_0's auc: 0.906092\n",
      "[300]\tvalid_0's auc: 0.906668\n",
      "[400]\tvalid_0's auc: 0.906661\n",
      "Early stopping, best iteration is:\n",
      "[246]\tvalid_0's auc: 0.907103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:21: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:21: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt_cutoff: 0.043167104040228284\n",
      "f1 score after calib: 0.2606284658040665\n",
      "auc score after calib: 0.9071032522043841\n",
      "log loss score after calib: 0.05004 \n",
      "CUMULATIVE SCORE\n",
      "opt_cutoff: 0.04271501550498901\n",
      "f1 score: 0.26248216833095583\n",
      "conf matrix: [[78941   943]\n",
      " [  609   275]]\n",
      "\n",
      "fold nÂ°2\n",
      "y_train shape: (201917,)\n",
      "val shape: (40383, 249)\n",
      "y_val shape: (40383,)\n",
      "tr shape: (161534, 250)\n",
      "val shape: (40383, 250)\n",
      "yval 1 shape: (441,)\n",
      "yval 0 shape: (39942,)\n",
      "yval 1 ratio: 0.0109204368174727\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\tvalid_0's auc: 0.904034\n",
      "[200]\tvalid_0's auc: 0.907104\n",
      "[300]\tvalid_0's auc: 0.908685\n",
      "[400]\tvalid_0's auc: 0.909272\n",
      "[500]\tvalid_0's auc: 0.909495\n",
      "[600]\tvalid_0's auc: 0.909393\n",
      "[700]\tvalid_0's auc: 0.909503\n",
      "Early stopping, best iteration is:\n",
      "[578]\tvalid_0's auc: 0.909581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:21: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:21: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt_cutoff: 0.04454049142654412\n",
      "f1 score after calib: 0.2474012474012474\n",
      "auc score after calib: 0.9095807969174351\n",
      "log loss score after calib: 0.05078 \n",
      "CUMULATIVE SCORE\n",
      "opt_cutoff: 0.04271501550498901\n",
      "f1 score: 0.257207644962747\n",
      "conf matrix: [[118461   1365]\n",
      " [   929    396]]\n",
      "\n",
      "fold nÂ°3\n",
      "y_train shape: (201917,)\n",
      "val shape: (40383, 249)\n",
      "y_val shape: (40383,)\n",
      "tr shape: (161534, 250)\n",
      "val shape: (40383, 250)\n",
      "yval 1 shape: (441,)\n",
      "yval 0 shape: (39942,)\n",
      "yval 1 ratio: 0.0109204368174727\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\tvalid_0's auc: 0.898824\n",
      "[200]\tvalid_0's auc: 0.901854\n",
      "[300]\tvalid_0's auc: 0.903838\n",
      "[400]\tvalid_0's auc: 0.904503\n",
      "[500]\tvalid_0's auc: 0.904946\n",
      "[600]\tvalid_0's auc: 0.905175\n",
      "[700]\tvalid_0's auc: 0.905405\n",
      "[800]\tvalid_0's auc: 0.905536\n",
      "[900]\tvalid_0's auc: 0.90516\n",
      "Early stopping, best iteration is:\n",
      "[796]\tvalid_0's auc: 0.905598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:21: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:21: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt_cutoff: 0.033756842106711066\n",
      "f1 score after calib: 0.2522686025408349\n",
      "auc score after calib: 0.9055975836164253\n",
      "log loss score after calib: 0.05106 \n",
      "CUMULATIVE SCORE\n",
      "opt_cutoff: 0.04271501550498901\n",
      "f1 score: 0.25227888642522794\n",
      "conf matrix: [[157987   1781]\n",
      " [  1255    511]]\n",
      "\n",
      "fold nÂ°4\n",
      "y_train shape: (201917,)\n",
      "val shape: (40383, 249)\n",
      "y_val shape: (40383,)\n",
      "tr shape: (161534, 250)\n",
      "val shape: (40383, 250)\n",
      "yval 1 shape: (441,)\n",
      "yval 0 shape: (39942,)\n",
      "yval 1 ratio: 0.0109204368174727\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\tvalid_0's auc: 0.877625\n",
      "[200]\tvalid_0's auc: 0.883928\n",
      "[300]\tvalid_0's auc: 0.886886\n",
      "[400]\tvalid_0's auc: 0.887704\n",
      "[500]\tvalid_0's auc: 0.888318\n",
      "[600]\tvalid_0's auc: 0.888252\n",
      "[700]\tvalid_0's auc: 0.888417\n",
      "[800]\tvalid_0's auc: 0.88812\n",
      "[900]\tvalid_0's auc: 0.887992\n",
      "Early stopping, best iteration is:\n",
      "[702]\tvalid_0's auc: 0.88844\n",
      "opt_cutoff: 0.06715065423244596\n",
      "f1 score after calib: 0.22439024390243903\n",
      "auc score after calib: 0.8884397682762454\n",
      "log loss score after calib: 0.05146 \n",
      "CUMULATIVE SCORE\n",
      "opt_cutoff: 0.04430684055553044\n",
      "f1 score: 0.24537409493161705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:21: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf matrix: [[197555   2155]\n",
      " [  1598    609]]\n",
      "valid scores: [0.9086044674279968, 0.9071032522043841, 0.9095807969174351, 0.9055975836164252, 0.8884397682762455]\n",
      "CUMULATIVE SCORE\n",
      "opt_cutoff: 0.04430684055553044\n",
      "f1 score: 0.24537409493161705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:21: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf matrix: [[197555   2155]\n",
      " [  1598    609]]\n",
      "\n",
      "CV score log loss: 0.05064 \n",
      "CV score f1 score: 0.24537409493161705 cutoff:0.04430684055553044\n",
      "CV score AUC score: 0.8954637532078967\n",
      "Rank CV score f1 score: 0.24492017416545722 cutoff:0.9836564891167076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:21: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:21: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank Prob CV score f1 score: 0.24541607898448517 cutoff:0.04396695663364109\n",
      "Rank Plus Prob CV score f1 score: 0.24578866768759572 cutoff:1.0251491616753337\n",
      "Rank Plus Prob Prop High CV score f1 score: 0.24541607898448517 cutoff:1.4319536930308123\n",
      "Rank Plus Prob Prop Low CV score f1 score: 0.24561403508771934 cutoff:0.9954264363315664\n",
      "Labels CV score f1 score: 0.24944500504540867 cutoff:1.0\n",
      "Labels plus rank head: [0.16905629 0.48134116 0.55049029 0.3320209  0.05507132 0.93410757\n",
      " 0.27850828 0.4427358  0.80398177 0.44188292]\n",
      "Labels Plus Rank f1 score: 0.24979822437449556 cutoff:0.9906396255850234\n",
      "Labels Plus Rank AUC score: 0.903892347574123 \n",
      "Normal and Rank Average score f1 score: 0.24578866768759572 cutoff:0.5125745808376668\n",
      "Normal and Labels Average score f1 score: 0.24979822437449556 cutoff:0.03357532711622298\n",
      "Normal and Labels Average score AUC score: 0.8954868700984802\n",
      "Normal and Labels and Rank Average score f1 score: 0.24979822437449556 cutoff:0.5288951399087347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:21: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:21: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal and Labels and Rank Average score AUC score: 0.9038931416571246 \n"
     ]
    }
   ],
   "source": [
    "randomno = 4590\n",
    "fold_importance_df,predictions,oof,train_outlier_preds,dummy_overall_imp_df,dummy_overall_sel_feats\\\n",
    "    = runlgb(randomno,\n",
    "            False,df_train,df_test,param,overall_sel_feats_cls,\n",
    "             target,lgb_fit,lgb_predict,lgb_getbestscore,\n",
    "             score_function=get_logloss_score,regression=False,\n",
    "             fold_feats=True, \n",
    "#                  feval=lgb_sk_f1,\n",
    "             fold_to_start=fold_to_start,\n",
    "             fold_to_stop=fold_to_stop,\n",
    "#              targetenc=True,\n",
    "#              to_enc_cols= toenccols,\n",
    "#              likely_card_ids=[]\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "_uuid": "793d99ff1f475515fe2b30286e14b9d5a30224e0"
   },
   "outputs": [],
   "source": [
    "# oof_rank_p_labels_1 = oof_rank_p_labels.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "_uuid": "73a03b34789abf03cb5b09fc56bd6d69599b3cdb"
   },
   "outputs": [],
   "source": [
    "np.savetxt('oof_cls_fold_feats.csv',oof,delimiter=',')\n",
    "np.savetxt('test_preds_cls_fold_feats.csv',predictions,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "_uuid": "deb67218f01b7b6fff2e841a9b9812516e6c20ba",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hist_category_2_raw_mean', 'hist_category_3_A_mean', 'hist_category_3_A_sum', 'hist_city_id_nunique', 'hist_month_lag_-1_mean', 'hist_month_lag_-2_mean', 'hist_month_lag_-5_mean', 'hist_month_lag_-6_mean', 'hist_month_lag_-7_mean', 'hist_month_lag_-8_mean', 'hist_month_lag_-8_sum', 'hist_month_lag_-9_mean', 'hist_month_lag_-9_sum', 'hist_month_lag_0_mean', 'hist_month_lag_0_sum', 'hist_purchase_amount_per_install_mean', 'hist_purchase_amount_per_install_sum', 'hist_purchase_amount_per_install_var', 'hist_purchase_amount_scaled_sum', 'new_hist_category_2_raw_mean', 'new_hist_category_3_A_sum', 'new_hist_category_3_raw_mean', 'new_hist_month_lag_1_mean', 'new_hist_month_lag_1_sum', 'new_hist_purchase_amount_scaled_sum', 'new_hist_purchase_amount_scaled_var', 'rank_diff_rank_hist_purchase_amount_per_install_sum', 'rank_elapsed_time', 'rank_hist_authorized_flag_sum', 'rank_hist_category_2_raw_mean', 'rank_hist_first_buy', 'rank_hist_installments_mean', 'rank_hist_month_lag_-1_sum', 'rank_hist_month_lag_-4_sum', 'rank_hist_month_lag_-5_sum', 'rank_hist_month_lag_-6_sum', 'rank_hist_purchase_date_count_mean', 'rank_hist_purchase_date_diff', 'rank_hist_size_per_merchant', 'rank_new_hist_purchase_date_min', 'rank_new_hist_purchase_date_uptonow', 'elapsed_time', 'hist_authorized_flag_mean', 'hist_authorized_flag_sum', 'hist_category_1_mean', 'hist_category_1_sum', 'hist_installments_sum', 'hist_month_diff_mean', 'hist_month_lag_mean', 'hist_month_lag_min', 'hist_month_lag_var', 'hist_month_nunique', 'hist_most_recent_sales_range_std', 'hist_purchase_date_diff', 'hist_purchase_date_max', 'hist_purchase_date_min', 'hist_purchase_date_uptonow', 'hist_weekofyear_nunique', 'new_hist_category_1_mean', 'new_hist_category_1_sum', 'new_hist_month_lag_mean', 'new_hist_purchase_date_diff', 'new_hist_purchase_date_max', 'new_hist_purchase_date_uptonow', 'trans_merged_targetenc_merchant_id_mean']\n"
     ]
    }
   ],
   "source": [
    "features = selected_features_cls +  sel_feats + sel_enc_cols\n",
    "# features.remove('rank_diff_rank_hist_purchase_amount_per_install_sum')\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "_uuid": "fa081955fbdd35d386560b993ef534e550d4896b"
   },
   "outputs": [],
   "source": [
    "param = {'colsample_bytree': 0.933637062627785,\n",
    "   'min_child_samples': 465,\n",
    "   'num_leaves': 75,\n",
    "   'reg_alpha': 0.616207848670193,\n",
    "   'reg_lambda': 0.829210506269319,\n",
    "   'subsample': 0.936495757097884,\n",
    "   'subsample_for_bin': 80000,\n",
    "   'learning_rate': 0.01,\n",
    "   'boosting': 'gbdt',\n",
    "   'bagging_seed': 2018,\n",
    "   'bagging_frequency': 2,\n",
    "   'min_data_in_bin': 100,\n",
    "   'n_estimators': 10000,\n",
    "   'objective': 'binary',\n",
    "   'metric': 'auc',\n",
    "   'random_state': 2333,\n",
    "   'max_depth': 7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "_uuid": "1e3eb58b0f37ef2d4ded777fb02a9eff5923a6c0"
   },
   "outputs": [],
   "source": [
    "param['scale_pos_weight'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "_uuid": "68ac7a8e688fe4c0956c4e08ffe031a2cd5decd6"
   },
   "outputs": [],
   "source": [
    "# param = {'colsample_bytree': 0.8897311029338786,\n",
    "#    'min_child_samples': 55,\n",
    "#    'num_leaves': 75,\n",
    "#    'reg_alpha': 0.7190472014738334,\n",
    "#    'reg_lambda': 0.4675245415536591,\n",
    "#    'subsample': 0.9562661113791268,\n",
    "#    'subsample_for_bin': 90000,\n",
    "#    'learning_rate': 0.01,\n",
    "#    'boosting': 'gbdt',\n",
    "#    'bagging_seed': 2018,\n",
    "#    'bagging_frequency': 2,\n",
    "#    'min_data_in_bin': 100,\n",
    "#    'n_estimators': 10000,\n",
    "#    'objective': 'binary',\n",
    "#    'metric': 'auc',\n",
    "#    'random_state': 2333,\n",
    "#    'max_depth': 8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "_uuid": "db77df8b08314535af22451c87e535df20c293c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fold nÂ°0\n",
      "y_train shape: (201917,)\n",
      "val shape: (40384, 249)\n",
      "y_val shape: (40384,)\n",
      "tr shape: (161533, 250)\n",
      "val shape: (40384, 250)\n",
      "yval 1 shape: (442,)\n",
      "yval 0 shape: (39942,)\n",
      "yval 1 ratio: 0.010944928684627575\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\tvalid_0's auc: 0.908486\n",
      "[200]\tvalid_0's auc: 0.909602\n",
      "[300]\tvalid_0's auc: 0.909742\n",
      "[400]\tvalid_0's auc: 0.909947\n",
      "[500]\tvalid_0's auc: 0.909747\n",
      "Early stopping, best iteration is:\n",
      "[376]\tvalid_0's auc: 0.910018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:21: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:21: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt_cutoff: 0.05603475033367883\n",
      "f1 score after calib: 0.26916221033868093\n",
      "auc score after calib: 0.9100184520949042\n",
      "log loss score after calib: 0.04766 \n",
      "CUMULATIVE SCORE\n",
      "opt_cutoff: 0.05603475033367883\n",
      "f1 score: 0.26916221033868093\n",
      "conf matrix: [[39413   529]\n",
      " [  292   150]]\n",
      "\n",
      "fold nÂ°1\n",
      "y_train shape: (201917,)\n",
      "val shape: (40384, 249)\n",
      "y_val shape: (40384,)\n",
      "tr shape: (161533, 250)\n",
      "val shape: (40384, 250)\n",
      "yval 1 shape: (442,)\n",
      "yval 0 shape: (39942,)\n",
      "yval 1 ratio: 0.010944928684627575\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\tvalid_0's auc: 0.904916\n",
      "[200]\tvalid_0's auc: 0.905637\n",
      "[300]\tvalid_0's auc: 0.905398\n",
      "Early stopping, best iteration is:\n",
      "[184]\tvalid_0's auc: 0.905946\n",
      "opt_cutoff: 0.0902151800117532\n",
      "f1 score after calib: 0.2508250825082508\n",
      "auc score after calib: 0.9059459746043528\n",
      "log loss score after calib: 0.04851 \n",
      "CUMULATIVE SCORE\n",
      "opt_cutoff: 0.07574326822562058\n",
      "f1 score: 0.2584269662921348\n",
      "conf matrix: [[79063   821]\n",
      " [  632   252]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:21: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fold nÂ°2\n",
      "y_train shape: (201917,)\n",
      "val shape: (40383, 249)\n",
      "y_val shape: (40383,)\n",
      "tr shape: (161534, 250)\n",
      "val shape: (40383, 250)\n",
      "yval 1 shape: (441,)\n",
      "yval 0 shape: (39942,)\n",
      "yval 1 ratio: 0.0109204368174727\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\tvalid_0's auc: 0.90776\n",
      "[200]\tvalid_0's auc: 0.910849\n",
      "[300]\tvalid_0's auc: 0.911331\n",
      "[400]\tvalid_0's auc: 0.911544\n",
      "[500]\tvalid_0's auc: 0.911962\n",
      "[600]\tvalid_0's auc: 0.911841\n",
      "[700]\tvalid_0's auc: 0.911679\n",
      "Early stopping, best iteration is:\n",
      "[527]\tvalid_0's auc: 0.912045\n",
      "opt_cutoff: 0.08966568710701275\n",
      "f1 score after calib: 0.25\n",
      "auc score after calib: 0.9120445167034149\n",
      "log loss score after calib: 0.04876 \n",
      "CUMULATIVE SCORE\n",
      "opt_cutoff: 0.07841993315700052\n",
      "f1 score: 0.25450762829403606\n",
      "conf matrix: [[118634   1192]\n",
      " [   959    366]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:21: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fold nÂ°3\n",
      "y_train shape: (201917,)\n",
      "val shape: (40383, 249)\n",
      "y_val shape: (40383,)\n",
      "tr shape: (161534, 250)\n",
      "val shape: (40383, 250)\n",
      "yval 1 shape: (441,)\n",
      "yval 0 shape: (39942,)\n",
      "yval 1 ratio: 0.0109204368174727\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\tvalid_0's auc: 0.905227\n",
      "[200]\tvalid_0's auc: 0.905774\n",
      "[300]\tvalid_0's auc: 0.906464\n",
      "[400]\tvalid_0's auc: 0.906373\n",
      "[500]\tvalid_0's auc: 0.906495\n",
      "[600]\tvalid_0's auc: 0.90634\n",
      "[700]\tvalid_0's auc: 0.906015\n",
      "Early stopping, best iteration is:\n",
      "[570]\tvalid_0's auc: 0.906548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:21: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:21: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt_cutoff: 0.04929449156995747\n",
      "f1 score after calib: 0.24137931034482757\n",
      "auc score after calib: 0.9065476005968292\n",
      "log loss score after calib: 0.04963 \n",
      "CUMULATIVE SCORE\n",
      "opt_cutoff: 0.058474909797922976\n",
      "f1 score: 0.24924576467857973\n",
      "conf matrix: [[157762   2006]\n",
      " [  1230    536]]\n",
      "\n",
      "fold nÂ°4\n",
      "y_train shape: (201917,)\n",
      "val shape: (40383, 249)\n",
      "y_val shape: (40383,)\n",
      "tr shape: (161534, 250)\n",
      "val shape: (40383, 250)\n",
      "yval 1 shape: (441,)\n",
      "yval 0 shape: (39942,)\n",
      "yval 1 ratio: 0.0109204368174727\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\tvalid_0's auc: 0.888675\n",
      "[200]\tvalid_0's auc: 0.889734\n",
      "[300]\tvalid_0's auc: 0.890594\n",
      "[400]\tvalid_0's auc: 0.890992\n",
      "[500]\tvalid_0's auc: 0.890959\n",
      "[600]\tvalid_0's auc: 0.890835\n",
      "Early stopping, best iteration is:\n",
      "[435]\tvalid_0's auc: 0.891068\n",
      "opt_cutoff: 0.11699396040682118\n",
      "f1 score after calib: 0.2425\n",
      "auc score after calib: 0.8910733488728725\n",
      "log loss score after calib: 0.04928 \n",
      "CUMULATIVE SCORE\n",
      "opt_cutoff: 0.058429508087668405\n",
      "f1 score: 0.2458742814759874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:21: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf matrix: [[197187   2523]\n",
      " [  1545    662]]\n",
      "valid scores: [0.9100184520949042, 0.9059459746043528, 0.9120445167034149, 0.9065476005968292, 0.8910678987933864]\n",
      "CUMULATIVE SCORE\n",
      "opt_cutoff: 0.058429508087668405\n",
      "f1 score: 0.2458742814759874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:21: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf matrix: [[197187   2523]\n",
      " [  1545    662]]\n",
      "\n",
      "CV score log loss: 0.04877 \n",
      "CV score f1 score: 0.2458742814759874 cutoff:0.058429508087668405\n",
      "CV score AUC score: 0.8980308397788483\n",
      "Rank CV score f1 score: 0.2466555183946488 cutoff:0.9872474247226625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:21: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:21: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank Prob CV score f1 score: 0.2458742814759874 cutoff:0.05750060869455043\n",
      "Rank Plus Prob CV score f1 score: 0.24537379718726868 cutoff:1.0423798578132886\n",
      "Rank Plus Prob Prop High CV score f1 score: 0.24573758339510748 cutoff:1.5683973021083903\n",
      "Rank Plus Prob Prop Low CV score f1 score: 0.24591857821864022 cutoff:1.0055394224446779\n",
      "Labels CV score f1 score: 0.2495860927152318 cutoff:1.0\n",
      "Labels plus rank head: [0.16425228 0.37547978 0.49616185 0.39073372 0.04204635 0.93111133\n",
      " 0.38231434 0.55382215 0.73841125 0.45186212]\n",
      "Labels Plus Rank f1 score: 0.24994827229464098 cutoff:0.9911348834905778\n",
      "Labels Plus Rank AUC score: 0.9051494660461112 \n",
      "Normal and Rank Average score f1 score: 0.24537379718726868 cutoff:0.5211899289066443\n",
      "Normal and Labels Average score f1 score: 0.24994827229464098 cutoff:0.05849698020341059\n",
      "Normal and Labels Average score AUC score: 0.8980560099411932\n",
      "Normal and Labels and Rank Average score f1 score: 0.24994827229464098 cutoff:0.5540644219486994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:21: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:21: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal and Labels and Rank Average score AUC score: 0.905147756498849 \n"
     ]
    }
   ],
   "source": [
    "randomno = 4590\n",
    "fold_importance_df,predictions_2,oof_2,train_outlier_preds,dummy_overall_imp_df,dummy_overall_sel_feats\\\n",
    "    = runlgb(randomno,\n",
    "            False,df_train,df_test,param,features,\n",
    "             target,lgb_fit,lgb_predict,lgb_getbestscore,\n",
    "             score_function=get_logloss_score,regression=False,\n",
    "#              fold_feats=True, \n",
    "#                  feval=lgb_sk_f1,\n",
    "             fold_to_start=fold_to_start,\n",
    "             fold_to_stop=fold_to_stop,\n",
    "#              targetenc=True,\n",
    "#              to_enc_cols= toenccols,\n",
    "#              likely_card_ids=[]\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "_uuid": "fb436b1943fc3a2ae0c5779945b751079e68c04d"
   },
   "outputs": [],
   "source": [
    "# oof_rank_p_labels_2 = oof_rank_p_labels.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "_uuid": "0de17d8d1e7257993d0a81f0564bccc6ebf0c5a7"
   },
   "outputs": [],
   "source": [
    "# #rank + labels returned high cum score\n",
    "\n",
    "# # oof_ens = (oof_rank_p_labels + oof_labels_p_prob ) / 2\n",
    "# oof_ens = oof_rank_p_labels \n",
    "# opt_cutoff, f1score = get_opt_cutoff_prec(target, oof_ens )\n",
    "# print(\"Mean  oof score f1 score: {0} cutoff:{1}\".format(f1score,opt_cutoff))\n",
    "# print(\"Mean  oof AUC score: {0} \".format(roc_auc_score(target, oof_ens )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "_uuid": "200ce221e8871a6c1277669de92d3d074130ba17"
   },
   "outputs": [],
   "source": [
    "# oof = oof_rank_p_labels.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "_uuid": "7ae857661241ff6e1bc389de8323060b58f073aa"
   },
   "outputs": [],
   "source": [
    "# oof_sc_1 = oof_rank_p_labels.copy()\n",
    "# oof_raw_sc_1 = oof_2.copy()\n",
    "# oof_rank_sc_1 = oof_rank.copy()\n",
    "# oof_label_sc_1 = oof_label.copy()\n",
    "# # oof_sc_3, oof_sc_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "_uuid": "2b38014ec63d6e5e0fef8f835126e786938e60d0"
   },
   "outputs": [],
   "source": [
    "np.savetxt('oof_cls_overall_sel.csv',oof_2,delimiter=',')\n",
    "np.savetxt('test_preds_cls_overall_sel.csv',predictions_2,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "_uuid": "dc24496b27f622690952229433fee76aa32fb589"
   },
   "outputs": [],
   "source": [
    "# # oof_ens = (0.333*oof_sc_5 + 0.333*oof_sc_3 + 0.333*oof_sc_4 + ) \n",
    "# oof_ens = oof_sc_1 + oof_sc_2\n",
    "# # oof_ens = (oof_sc_5 + oof_sc_3 + oof_sc_4 + oof_sc_10 ) / 4 \n",
    "# # predictions_ens =(predictions + predictions_2) / 2\n",
    "# opt_cutoff, f1score = get_opt_cutoff_prec(target, oof_ens )\n",
    "# print(\"Mean  oof score f1 score: {0} cutoff:{1}\".format(f1score,opt_cutoff))\n",
    "# print(\"Mean  oof AUC score: {0} \".format(roc_auc_score(target, oof_ens )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "_uuid": "5851c1f3ba98ad407cb6779891d57ab8c56f40b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean  oof score f1 score: 0.25049780963759455 cutoff:0.05477038535050867\n",
      "Mean  oof AUC score: 0.8982393455558135 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:21: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# oof_ens = (oof + oof_2) / 2\n",
    "w=[0.6,0.4]\n",
    "oof_ens = (w[0]*oof + w[1]*oof_2) \n",
    "\n",
    "# predictions_ens =(predictions + predictions_2) / 2\n",
    "predictions_ens =(w[0]*predictions + w[1]*predictions_2) \n",
    "opt_cutoff, f1score = get_opt_cutoff_prec(target, oof_ens )\n",
    "print(\"Mean  oof score f1 score: {0} cutoff:{1}\".format(f1score,opt_cutoff))\n",
    "print(\"Mean  oof AUC score: {0} \".format(roc_auc_score(target, oof_ens )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "_uuid": "0cf4cc0e27684686a3502225e33360c0ee6b39c9"
   },
   "outputs": [],
   "source": [
    "oof = oof_ens\n",
    "predictions = predictions_ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "_uuid": "56bdd8128f65efc54d5a00a6e86fd848fa13089b"
   },
   "outputs": [],
   "source": [
    "np.savetxt('oof_cls_ens.csv',oof,delimiter=',')\n",
    "np.savetxt('test_preds_ens.csv',predictions,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "_uuid": "9d7f1310869095c9775ae6e5a1176cc795a46b22"
   },
   "outputs": [],
   "source": [
    "# n_ensembles = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "_uuid": "3d26ea74bf0940b19df9a5a040bd9d576b2b34f7"
   },
   "outputs": [],
   "source": [
    "# predictions_list=[]\n",
    "# oof_list =[]\n",
    "# for i in range(0,n_ensembles):\n",
    "# #     randomno = 1234\n",
    "#     randomno = 888 + i\n",
    "#     param['random_state'] = randomno\n",
    "#     fold_importance_df,predictions,oof,oof_labels,oof_rank,\\\n",
    "#     train_outlier_preds,dummy_overall_imp_df,dummy_overall_sel_feats\\\n",
    "#         = runlgb(4590,\n",
    "#                 False,df_train,df_test,param,features,\n",
    "#                  target,\n",
    "#                  score_function=get_logloss_score,regression=False,\n",
    "#     #              fold_feats=True, \n",
    "#                  feval=lgb_sk_f1,\n",
    "#                  fold_to_start=fold_to_start,\n",
    "#                  fold_to_stop=fold_to_stop,\n",
    "#     #              targetenc=True,\n",
    "#     #              to_enc_cols= toenccols,\n",
    "# #                  likely_card_ids=[]\n",
    "#                 )\n",
    "#     oof_list +=[oof]\n",
    "#     predictions_list +=[predictions]\n",
    "#     np.savetxt('test_'+str(randomno)+'.csv',predictions,delimiter=',')\n",
    "#     np.savetxt('oof_'+str(randomno)+'.csv',oof,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "_uuid": "910af43c1dd2abc9a7a71ff971ac723aa4b55a63"
   },
   "outputs": [],
   "source": [
    "# for i,oof_cur in enumerate(oof_list):\n",
    "#     opt_cutoff, f1score = get_opt_cutoff_prec(target, oof_cur)\n",
    "#     print()\n",
    "#     print('Ensemble:',i)\n",
    "#     opt_cutoff, f1score = get_opt_cutoff_prec(target, oof_cur)\n",
    "#     print(\"Current Ensemble oof score f1 score: {0} cutoff:{1}\".format(f1score,opt_cutoff))\n",
    "#     print(\"Current Ensemble oof AUC score: {0} \".format(roc_auc_score(target, oof_cur)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "_uuid": "5161b700a916927db5656ae78cd143a6ed3e9d55"
   },
   "outputs": [],
   "source": [
    "# predictions = np.mean(np.array(predictions_list),axis=0)\n",
    "# oof = np.mean(np.array(oof_list),axis=0)\n",
    "\n",
    "# opt_cutoff, f1score = get_opt_cutoff_prec(target, oof)\n",
    "# print(\"Ensemble oof score f1 score: {0} cutoff:{1}\".format(f1score,opt_cutoff))\n",
    "# print(\"Ensemble oof AUC score: {0} \".format(roc_auc_score(target, oof)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "_uuid": "c548be449836ef0c9d21f5d95b6644d5cc89cae7"
   },
   "outputs": [],
   "source": [
    "# fold_importance_df['importance_ratio'] = 100 * fold_importance_df['importance'] / fold_importance_df['importance'].sum()\n",
    "# fold_importance_df['gain_ratio'] = 100 * fold_importance_df['gain'] / fold_importance_df['gain'].sum()\n",
    "# fold_importance_df=fold_importance_df.sort_values(by=\"importance\", ascending=False)\n",
    "# fold_importance_df.to_csv('feature_importance.csv')\n",
    "# fold_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "_uuid": "8cae9b0f1e3cd4f1572efd0d9533104a3a90e430"
   },
   "outputs": [],
   "source": [
    "# fold_importance_df=fold_importance_df.sort_values(by=\"gain\", ascending=False)\n",
    "# fold_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "_uuid": "5c0661c8572239c272ac98b23408081a19f61b17"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.listdir( '../input/elo-output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "_uuid": "f644cf8303208db77dad35ed8528870921481684"
   },
   "outputs": [],
   "source": [
    "# Path = '../input/elo-output/'\n",
    "# ooflist=[]\n",
    "# testlist=[]\n",
    "# # endpoints =[2,5,7]\n",
    "# endpoints =[552,1104,1656,2206]\n",
    "\n",
    "# oof = np.full(df_train.shape[0],0,dtype='float')\n",
    "# predictions = np.full(df_test.shape[0],0,dtype='float')\n",
    "\n",
    "# for val in endpoints:\n",
    "#     print('cur end point:',val)\n",
    "# #     cur_oof=np.loadtxt(Path + 'oof_'+ str(val) +'.csv',delimiter=',')\n",
    "#     cur_oof=np.loadtxt(Path + 'oof_hs_'+ str(val) +'.csv',delimiter=',')\n",
    "#     print(cur_oof[cur_oof!=0].shape)\n",
    "# #     ooflist += [cur_oof]\n",
    "#     oof += cur_oof\n",
    "# #     cur_test_preds=np.loadtxt(Path + 'test_preds_'+ str(val) +'.csv',delimiter=',')\n",
    "#     cur_test_preds=np.loadtxt(Path + 'test_preds_hs_'+ str(val) +'.csv',delimiter=',')\n",
    "# #     testlist += [cur_test_preds]\n",
    "#     print(cur_test_preds[cur_test_preds!=0].shape)\n",
    "#     predictions +=cur_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "_uuid": "e5c29d3294549dbc430985d778ee7103f970bdb6"
   },
   "outputs": [],
   "source": [
    "# print(oof[oof>1].shape)\n",
    "# print(predictions[predictions>1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "_uuid": "b641b28f1c08abb57af2f5f8fe3ba0d426047e7c"
   },
   "outputs": [],
   "source": [
    "# print(oof.mean())\n",
    "# print(predictions.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "_uuid": "c39eb4bf7a81727755f1beb51544bd60120fab44"
   },
   "outputs": [],
   "source": [
    "# print(target[target==0].shape[0] / (target[target==0].shape[0] + target[target==1].shape[0]))\n",
    "# target[target==1].shape[0] / (target[target==0].shape[0] + target[target==1].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "_uuid": "4d51b98a9798d5d16183139d6b8648b5bdbe0b46"
   },
   "outputs": [],
   "source": [
    "# print(f1_score(oof,target))\n",
    "# print('conf matrix:',confusion_matrix(target,oof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "_uuid": "10689a8f6866ffbc22e24f91ac34bc8bbc4d783f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt_cutoff: 0.05477038535050867\n",
      "f1 score: 0.25049780963759455\n",
      "conf matrix: [[197524   2186]\n",
      " [  1579    628]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:21: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.05477038535050867, 0.25049780963759455, array([0, 0, 0, ..., 0, 0, 0]))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computef1scoreandconfmatrix(target,oof)\n",
    "\n",
    "# permute sel feats\n",
    "# opt_cutoff: 0.11941022019862527\n",
    "# f1 score: 0.24567541302235177\n",
    "# conf matrix: [[197404   2306]\n",
    "#  [  1576    631]]\n",
    "# (0.11941022019862527, 0.24567541302235177, array([0, 0, 0, ..., 0, 0, 0]))\n",
    "\n",
    "\n",
    "# with new feats sum and mean, cat raw\n",
    "# cur opt_cutoff: 0.11131955523682135\n",
    "# cur f1 score: 0.23903508771929824\n",
    "# cur conf matrix: [[197099   2611]\n",
    "#  [  1554    653]]\n",
    "\n",
    "# cur opt_cutoff: 0.09891675917584493\n",
    "# cur f1 score: 0.2400388726919339\n",
    "# cur conf matrix: [[196484   3226]\n",
    "#  [  1467    740]]\n",
    "\n",
    "# with new feats combined rank_diff, rank indl cols, category_3_A, month_lag dummies sum\n",
    "# opt_cutoff: 0.11965149342035582\n",
    "# f1 score: 0.24696197569580555\n",
    "# conf matrix: [[197445   2265]\n",
    "#  [  1578    629]]\n",
    "\n",
    "# with new feats rank_diff, rank indl cols, category_3_A, month_lag dummies sum\n",
    "# opt_cutoff: 0.11363498063696807\n",
    "# f1 score: 0.24554158062699455\n",
    "# conf matrix: [[197244   2466]\n",
    "#  [  1554    653]]\n",
    "\n",
    "# with new feats rank_diff, category_3_A, month_lag dummies sum\n",
    "# valid scores: [0.043260558903229475, 0.044006556599281085, 0.04365489765477539, 0.04395118265001193, 0.04531144654168368]\n",
    "# CV score: 0.04404 \n",
    "# opt_cutoff: 0.10800507734665102\n",
    "# f1 score: 0.2432909604519774\n",
    "# conf matrix: [[196942   2768]\n",
    "#  [  1519    688]]\n",
    "\n",
    "# with new feats category_3_A, month_lag dummies sum\n",
    "# opt_cutoff: 0.10907925221938841\n",
    "# f1 score: 0.2447227191413238\n",
    "# conf matrix: [[197011   2699]\n",
    "#  [  1524    683]]\n",
    "\n",
    "# with new feats category_3_A, month_lag dummies mean\n",
    "# opt_cutoff: 0.12271163161550341\n",
    "# f1 score: 0.24064386317907444\n",
    "# conf matrix: [[197545   2165]\n",
    "#  [  1610    597]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "_uuid": "516b9fb9954251c0cef847d98fc09486f5fd9e61"
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import f1_score\n",
    "# f1_score(target,pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "_uuid": "701007c960f359cd084e088fd703bf9571a64d3d"
   },
   "outputs": [],
   "source": [
    "# cutoff=0.0175\n",
    "# conf matrix: [[175815  23895]\n",
    "#  [   553   1654]]\n",
    "\n",
    "# cutoff=0.0162\n",
    "# conf matrix: [[174197  25513]\n",
    "#  [   528   1679]]\n",
    "\n",
    "# cutoff=0.0437\n",
    "# conf matrix: [[189851   9859]\n",
    "#  [   972   1235]]\n",
    "\n",
    "# cutoff=0.0003\n",
    "# conf matrix: [[ 26966 172744]\n",
    "#  [     0   2207]]\n",
    "\n",
    "# cutoff=0.001\n",
    "# conf matrix: [[ 72168 127542]\n",
    "#  [    20   2187]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "_uuid": "2edab8b8bd95a3ac0cadd0d999e5c48fc226156c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf matrix: [[191376   8334]\n",
      " [  1073   1134]]\n"
     ]
    }
   ],
   "source": [
    "pred_labels = convert_probtolabels(oof,cutoff=0.0165) #0.0462\n",
    "print('conf matrix:',confusion_matrix(target,pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "_uuid": "78eed64301f931e6970f3fd0e62dc5b113e9b06d"
   },
   "outputs": [],
   "source": [
    "# factored_cutoff = 0.0062\n",
    "# pred_labels = convert_probtolabels(oof,cutoff=factored_cutoff) #0.01637\n",
    "# print('conf matrix:',confusion_matrix(target,pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "_uuid": "6fdcc2ed254871000db9e122d197aadf874cc71e"
   },
   "outputs": [],
   "source": [
    "# # In case missing some predictable outlier, we choose top 25000 with highest outliers likelyhood.\n",
    "# df_outlier_prob_oof[df_outlier_prob_oof['target']>factored_cutoff].shape\n",
    "# outlier_id_oof = pd.DataFrame(df_outlier_prob_oof[df_outlier_prob_oof['target']>=factored_cutoff]['card_id'])\n",
    "# print(outlier_id_oof.shape)\n",
    "\n",
    "# model_full_oof = pd.concat([model_without_outliers_oof,train_outlier_preds_df])\n",
    "# print(model_full_oof.shape)\n",
    "\n",
    "# model_full_oof.loc[model_full_oof[\"card_id\"].isin(outlier_id_oof[\"card_id\"].values), \"target\"]\\\n",
    "#     = best_oof[best_oof[\"card_id\"].isin(outlier_id_oof[\"card_id\"].values)][\"target\"]\n",
    "\n",
    "# # mask = model_full_oof['card_id'].isin(df_train['card_id'])\n",
    "# model_full_oof.sort_index(inplace=True)\n",
    "# print(\"CV score: {:<8.5f}\".format(mean_squared_error(model_full_oof['target'], target_reg)**0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "_uuid": "264d1d76da682d6ad1751bfc09058e2d6481044b"
   },
   "outputs": [],
   "source": [
    "# cols = (fold_importance_df[[\"feature\", \"importance\"]]\n",
    "#         .groupby(\"feature\")\n",
    "#         .mean()\n",
    "#         .sort_values(by=\"importance\", ascending=False)[:1000].index)\n",
    "\n",
    "# best_features = fold_importance_df.loc[fold_importance_df.feature.isin(cols)]\n",
    "# best_features=best_features.sort_values(by=\"importance\", ascending=False)\n",
    "# best_features.to_csv('best_features.csv')\n",
    "# print(best_features['importance'].sum())\n",
    "# best_features['importance_ratio'] = best_features['importance'] / best_features['importance'].sum()\n",
    "# print(best_features[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "_uuid": "41f9c7d0a52f8d5669704619fdf76ad1451afaa2",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>card_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C_ID_0ab67a22ab</td>\n",
       "      <td>0.018787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C_ID_130fd0cbdd</td>\n",
       "      <td>0.006526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C_ID_b709037bc5</td>\n",
       "      <td>0.007560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C_ID_d27d835a9f</td>\n",
       "      <td>0.006387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C_ID_2b5e3df5c2</td>\n",
       "      <td>0.006575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           card_id    target\n",
       "0  C_ID_0ab67a22ab  0.018787\n",
       "1  C_ID_130fd0cbdd  0.006526\n",
       "2  C_ID_b709037bc5  0.007560\n",
       "3  C_ID_d27d835a9f  0.006387\n",
       "4  C_ID_2b5e3df5c2  0.006575"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 'target' is the probability of whether an observation is an outlier\n",
    "df_outlier_prob = pd.DataFrame({\"card_id\":df_test[\"card_id\"].values})\n",
    "df_outlier_prob[\"target\"] = predictions\n",
    "df_outlier_prob.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "_uuid": "a7105beb18f1b9127c536e81d9c4fe4f39a9ccb7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>card_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C_ID_92a2005557</td>\n",
       "      <td>0.006702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C_ID_3d0044924f</td>\n",
       "      <td>0.006826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C_ID_d639edf6cd</td>\n",
       "      <td>0.006384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C_ID_186d6a6901</td>\n",
       "      <td>0.006778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C_ID_cdbd2c0db2</td>\n",
       "      <td>0.006110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           card_id    target\n",
       "0  C_ID_92a2005557  0.006702\n",
       "1  C_ID_3d0044924f  0.006826\n",
       "2  C_ID_d639edf6cd  0.006384\n",
       "3  C_ID_186d6a6901  0.006778\n",
       "4  C_ID_cdbd2c0db2  0.006110"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### OOF 'target' is the probability of whether an observation is an outlier\n",
    "df_outlier_prob_oof = pd.DataFrame({\"card_id\":df_train[\"card_id\"].values})\n",
    "df_outlier_prob_oof[\"target\"] = oof\n",
    "df_outlier_prob_oof.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a6cdbee1c92967a2ecb3924ab58af8d86675df31"
   },
   "source": [
    "# Part 3 Combining Submission:\n",
    "So far so good !\n",
    "We now have three dataset:\n",
    "\n",
    "1. Best Submission\n",
    "2. Prediction Using Model Without Outliers\n",
    "3. Probability of Outliers In Test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "_uuid": "d56e431808a9a70a102491ac7d77e4404e312a4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1310.4038"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if the test set has the same ratio of outliers as training set, \n",
    "# then the numbuer of outliers in test is about: (1.06% outliers in training set)\n",
    "123623*0.0106"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "_uuid": "326282923b81b45d81e092a5cc0ef9b3787435c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt_cutoff: 0.05477038535050867\n",
      "f1 score: 0.25049780963759455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:21: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n"
     ]
    }
   ],
   "source": [
    "opt_cutoff, f1_score = get_opt_cutoff_prec(target,df_outlier_prob_oof['target'])\n",
    "print('opt_cutoff:',opt_cutoff)\n",
    "print('f1 score:',f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "_uuid": "c892bacbe9372046ec8213decc4ecc35f1449b40"
   },
   "outputs": [],
   "source": [
    "# factored_cutoff = opt_cutoff/8\n",
    "# factored_cutoff = 0.01 # 0.000301 #0.001\n",
    "# factored_cutoff = 0.0071"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "_uuid": "f98bf40eacd1a99cc45b483959e9d2248f03c951"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "factored cutoff: 0.00871862230985493\n",
      "(26584,)\n",
      "526\n"
     ]
    }
   ],
   "source": [
    "# factor = 8\n",
    "factorrange= np.linspace(1,20, num=2000, endpoint=False)\n",
    "for factor in factorrange:\n",
    "    factored_cutoff = opt_cutoff/factor\n",
    "    predicted_card_ids = df_outlier_prob_oof[df_outlier_prob_oof['target']>=factored_cutoff]['card_id'].values\n",
    "    outlier_card_ids = df_train[~mask_without_outlier]['card_id'].values\n",
    "    leftout_outliers = len(set(outlier_card_ids).difference(set(predicted_card_ids)))\n",
    "    if leftout_outliers <=528:\n",
    "        break\n",
    "        \n",
    "print('factored cutoff:',factored_cutoff)\n",
    "print(df_outlier_prob_oof[df_outlier_prob_oof['target']>=factored_cutoff]['card_id'].shape)\n",
    "print(leftout_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "_uuid": "7da409551c7cabfa3836e1eb043101d8adba6850"
   },
   "outputs": [],
   "source": [
    "# if leftout_outliers >400:\n",
    "#     factorrange= np.linspace(20,30, num=1000, endpoint=False)\n",
    "#     for factor in factorrange:\n",
    "#         factored_cutoff = opt_cutoff/factor\n",
    "#         predicted_card_ids = df_outlier_prob_oof[df_outlier_prob_oof['target']>=factored_cutoff]['card_id'].values\n",
    "#         outlier_card_ids = df_train[~mask_without_outlier]['card_id'].values\n",
    "#         leftout_outliers = len(set(outlier_card_ids).difference(set(predicted_card_ids)))\n",
    "#         if leftout_outliers <=528:\n",
    "#             break\n",
    "\n",
    "#     print('factored cutoff:',factored_cutoff)\n",
    "#     print(df_outlier_prob_oof[df_outlier_prob_oof['target']>=factored_cutoff]['card_id'].shape)\n",
    "#     print(leftout_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "37446d93bf7fcd79aba1e358595bd2a3a6e28938"
   },
   "source": [
    "OOF CV Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "_uuid": "c2939dd076cbe482faf657145abea696fe8ed53e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26584, 1)\n"
     ]
    }
   ],
   "source": [
    "# In case missing some predictable outlier, we choose top 25000 with highest outliers likelyhood.\n",
    "# df_outlier_prob_oof[df_outlier_prob_oof['target']>factored_cutoff].shape\n",
    "outlier_id_oof = pd.DataFrame(df_outlier_prob_oof[df_outlier_prob_oof['target']>=factored_cutoff]['card_id'])\n",
    "\n",
    "# df_outlier_prob_oof.sort_values('target',ascending=False,inplace=True)\n",
    "# outlier_id_oof=pd.DataFrame(df_outlier_prob_oof['card_id'][0:25000])\n",
    "print(outlier_id_oof.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a1f4fc73bb34ee4531477244838a4be00664a74b"
   },
   "source": [
    "Run Regression of Best Submission with full train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "_uuid": "e5f02e796fa0ece454b97f244f24f87173a500dd"
   },
   "outputs": [],
   "source": [
    "# param = {'objective':'regression',\n",
    "#          'num_leaves': 31,\n",
    "#          'min_data_in_leaf': 25,\n",
    "#          'max_depth': 7,\n",
    "#          'learning_rate': 0.01,\n",
    "#          'lambda_l1':0.13,\n",
    "#          \"boosting\": \"gbdt\",\n",
    "#          \"feature_fraction\":0.85,\n",
    "#          'bagging_freq':8,\n",
    "#          \"bagging_fraction\": 0.9 ,\n",
    "#          \"metric\": 'rmse',\n",
    "#          \"verbosity\": -1,\n",
    "#          \"random_state\": 2333,\n",
    "#          'n_estimators': 10000,\n",
    "#          'n_jobs' :-1\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "_uuid": "8f0baeb9a492df4d527a0bf76a440a756e8af744"
   },
   "outputs": [],
   "source": [
    "param = {'colsample_bytree': 0.7370323842138731,\n",
    "   'min_child_samples': 30,\n",
    "   'num_leaves': 73,\n",
    "   'reg_alpha': 0.3016168759062065,\n",
    "   'reg_lambda': 0.339362990388993,\n",
    "   'subsample': 0.9072420448274023,\n",
    "   'subsample_for_bin': 80000,\n",
    "   'learning_rate': 0.01,\n",
    "   'boosting': 'gbdt',\n",
    "   'bagging_seed': 2018,\n",
    "   'min_data_in_bin': 100,\n",
    "   'bagging_freq': 2,\n",
    "   'n_estimators': 10000,\n",
    "   'objective': 'regression',\n",
    "   'metric': 'rmse',\n",
    "   'random_state': 2333,\n",
    "   'max_depth': 7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "_uuid": "8da5eb7f46c879cbe06f9178796fa6b361611c03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fold nÂ°0\n",
      "y_train shape: (201917,)\n",
      "val shape: (40384, 249)\n",
      "y_val shape: (40384,)\n",
      "tr shape: (161533, 250)\n",
      "val shape: (40384, 250)\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\tvalid_0's rmse: 3.70533\n",
      "[200]\tvalid_0's rmse: 3.66476\n",
      "[300]\tvalid_0's rmse: 3.65079\n",
      "[400]\tvalid_0's rmse: 3.64598\n",
      "[500]\tvalid_0's rmse: 3.64366\n",
      "[600]\tvalid_0's rmse: 3.64189\n",
      "[700]\tvalid_0's rmse: 3.64134\n",
      "[800]\tvalid_0's rmse: 3.64062\n",
      "[900]\tvalid_0's rmse: 3.64045\n",
      "[1000]\tvalid_0's rmse: 3.64083\n",
      "Early stopping, best iteration is:\n",
      "[840]\tvalid_0's rmse: 3.64033\n",
      "\n",
      "fold nÂ°1\n",
      "y_train shape: (201917,)\n",
      "val shape: (40384, 249)\n",
      "y_val shape: (40384,)\n",
      "tr shape: (161533, 250)\n",
      "val shape: (40384, 250)\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\tvalid_0's rmse: 3.69924\n",
      "[200]\tvalid_0's rmse: 3.66356\n",
      "[300]\tvalid_0's rmse: 3.65342\n",
      "[400]\tvalid_0's rmse: 3.64902\n",
      "[500]\tvalid_0's rmse: 3.64704\n",
      "[600]\tvalid_0's rmse: 3.64626\n",
      "[700]\tvalid_0's rmse: 3.64536\n",
      "[800]\tvalid_0's rmse: 3.64473\n",
      "[900]\tvalid_0's rmse: 3.6442\n",
      "[1000]\tvalid_0's rmse: 3.6446\n",
      "[1100]\tvalid_0's rmse: 3.6445\n",
      "Early stopping, best iteration is:\n",
      "[922]\tvalid_0's rmse: 3.64414\n",
      "\n",
      "fold nÂ°2\n",
      "y_train shape: (201917,)\n",
      "val shape: (40383, 249)\n",
      "y_val shape: (40383,)\n",
      "tr shape: (161534, 250)\n",
      "val shape: (40383, 250)\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\tvalid_0's rmse: 3.70679\n",
      "[200]\tvalid_0's rmse: 3.6716\n",
      "[300]\tvalid_0's rmse: 3.65757\n",
      "[400]\tvalid_0's rmse: 3.65211\n",
      "[500]\tvalid_0's rmse: 3.64933\n",
      "[600]\tvalid_0's rmse: 3.64764\n",
      "[700]\tvalid_0's rmse: 3.64714\n",
      "[800]\tvalid_0's rmse: 3.64554\n",
      "[900]\tvalid_0's rmse: 3.64505\n",
      "[1000]\tvalid_0's rmse: 3.64502\n",
      "[1100]\tvalid_0's rmse: 3.6453\n",
      "Early stopping, best iteration is:\n",
      "[941]\tvalid_0's rmse: 3.64479\n",
      "\n",
      "fold nÂ°3\n",
      "y_train shape: (201917,)\n",
      "val shape: (40383, 249)\n",
      "y_val shape: (40383,)\n",
      "tr shape: (161534, 250)\n",
      "val shape: (40383, 250)\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\tvalid_0's rmse: 3.70263\n",
      "[200]\tvalid_0's rmse: 3.67095\n",
      "[300]\tvalid_0's rmse: 3.65894\n",
      "[400]\tvalid_0's rmse: 3.65331\n",
      "[500]\tvalid_0's rmse: 3.6511\n",
      "[600]\tvalid_0's rmse: 3.64994\n",
      "[700]\tvalid_0's rmse: 3.6491\n",
      "[800]\tvalid_0's rmse: 3.64836\n",
      "[900]\tvalid_0's rmse: 3.6485\n",
      "[1000]\tvalid_0's rmse: 3.64825\n",
      "[1100]\tvalid_0's rmse: 3.64826\n",
      "[1200]\tvalid_0's rmse: 3.64837\n",
      "Early stopping, best iteration is:\n",
      "[1078]\tvalid_0's rmse: 3.64792\n",
      "\n",
      "fold nÂ°4\n",
      "y_train shape: (201917,)\n",
      "val shape: (40383, 249)\n",
      "y_val shape: (40383,)\n",
      "tr shape: (161534, 250)\n",
      "val shape: (40383, 250)\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\tvalid_0's rmse: 3.69094\n",
      "[200]\tvalid_0's rmse: 3.66189\n",
      "[300]\tvalid_0's rmse: 3.65267\n",
      "[400]\tvalid_0's rmse: 3.64948\n",
      "[500]\tvalid_0's rmse: 3.64709\n",
      "[600]\tvalid_0's rmse: 3.64706\n",
      "[700]\tvalid_0's rmse: 3.64763\n",
      "Early stopping, best iteration is:\n",
      "[563]\tvalid_0's rmse: 3.64679\n",
      "valid scores: [3.6403296972717567, 3.6441391464427686, 3.6447940945136743, 3.6479184695522218, 3.6467885797987196]\n",
      "CV score: 3.64479 \n",
      "CPU times: user 20min 39s, sys: 13.7 s, total: 20min 53s\n",
      "Wall time: 5min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fold_importance_df,best_predictions,best_oof_preds,train_outlier_preds,dummy_overall_imp_df,dummy_overall_sel_feats\\\n",
    "    = runlgb(4590,False,df_train,df_test,param,overall_sel_feats,target_reg,\n",
    "             lgb_fit, lgb_predict,lgb_getbestscore,\n",
    "            fold_feats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "_uuid": "757dc1a42a60c7058f4b0843f70c2a90086c193c"
   },
   "outputs": [],
   "source": [
    "np.savetxt('oof_fulltrain_reg.csv',best_oof_preds,delimiter=',')\n",
    "np.savetxt('predictions_fulltrain_reg.csv',best_predictions,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "_uuid": "d65beab83d20bf2b8733c0d51247a62b3c7fc314"
   },
   "outputs": [],
   "source": [
    "best_oof_preds_prev = np.loadtxt('../input/elo-output/LGB_targetenc_card_smoothing.npy')\n",
    "best_submission_prev = pd.read_csv('../input/elo-output/submit_targetenc_nullreplacecategory.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "_uuid": "bc372bc6b69e89492765889bdbde589f30781534"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ens mean score: 3.642772218408375\n"
     ]
    }
   ],
   "source": [
    "best_oof_ens = (best_oof_preds_prev + best_oof_preds) / 2\n",
    "best_predictions_ens =(best_submission_prev['target'].values + best_predictions) / 2\n",
    "score = mean_squared_error(target_reg,best_oof_ens)**0.5\n",
    "print('ens mean score:',score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "_uuid": "9a8651968318d1a902d750f7b98f21cc5ea941fb"
   },
   "outputs": [],
   "source": [
    "np.savetxt('oof_fulltrain_reg_ens.csv',best_oof_ens,delimiter=',')\n",
    "np.savetxt('predictions_fulltrain_reg_ens.csv',best_predictions_ens,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "_uuid": "7683486c27e7ffbd6d656681e4fe5953372efbfc"
   },
   "outputs": [],
   "source": [
    "best_oof = pd.DataFrame()\n",
    "best_oof['card_id'] = df_train['card_id']\n",
    "best_oof['target'] = best_oof_ens\n",
    "\n",
    "best_submission = pd.DataFrame()\n",
    "best_submission['card_id'] = df_test['card_id']\n",
    "best_submission['target'] = best_predictions_ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "_uuid": "f6a29364f7fa3c1364341350f770dc8e1764c602"
   },
   "outputs": [],
   "source": [
    "# best_oof_preds = np.loadtxt('../input/elo-output/LGB_targetenc_card_smoothing.npy')\n",
    "# best_oof = pd.DataFrame()\n",
    "# best_oof['card_id'] = df_train['card_id']\n",
    "# best_oof['target'] = best_oof_preds\n",
    "# print(best_oof.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "_uuid": "8de5cbb6b4292318196b3c3e185d13c3a3e75fe7"
   },
   "outputs": [],
   "source": [
    "# model_full_oof = pd.concat([model_without_outliers_oof,train_outlier_preds_df])\n",
    "# print(model_full_oof.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "_uuid": "be2ae510eb826709562e631b8d5942cd79ebcdd6"
   },
   "outputs": [],
   "source": [
    "model_full_oof.loc[model_full_oof[\"card_id\"].isin(outlier_id_oof[\"card_id\"].values), \"target\"]\\\n",
    "    = best_oof[best_oof[\"card_id\"].isin(outlier_id_oof[\"card_id\"].values)][\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "_kg_hide-output": false,
    "_uuid": "e4ad34ae748d7b4bebf082a0961653e72b0d1ad6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   -0.202817\n",
       "1    0.273385\n",
       "2    0.571852\n",
       "3    0.250635\n",
       "4   -0.076238\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_full_oof['target'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "_uuid": "039b09da943e3ca1e2228899ef8b98978c308e42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 3.64096 \n"
     ]
    }
   ],
   "source": [
    "# mask = model_full_oof['card_id'].isin(df_train['card_id'])\n",
    "model_full_oof.sort_index(inplace=True)\n",
    "print(\"CV score: {:<8.5f}\".format(mean_squared_error(model_full_oof['target'], target_reg)**0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "_uuid": "67bbacebffac9b6399e2aa663801554900eb7418"
   },
   "outputs": [],
   "source": [
    "model_full_oof.to_csv('Meta_Ensembled_Tuned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "_uuid": "18cb01a9193c9e2fc6136d225e538537694a6fba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 3.64479 \n"
     ]
    }
   ],
   "source": [
    "print(\"CV score: {:<8.5f}\".format(mean_squared_error(best_oof_preds, target_reg)**0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5a703fc577c6ef68573ba939502d26752aa1d49c"
   },
   "source": [
    "Test new submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "_uuid": "40e683026c0e476bae7d15a759b97fa027af977d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16600, 1)\n"
     ]
    }
   ],
   "source": [
    "# In case missing some predictable outlier, we choose top 25000 with highest outliers likelyhood.\n",
    "# df_outlier_prob[df_outlier_prob['target']>factored_cutoff].shape\n",
    "# df_outlier_prob.sort_values('target',ascending=False,inplace=True)\n",
    "# outlier_id=pd.DataFrame(df_outlier_prob['card_id'][0:25000])\n",
    "outlier_id = pd.DataFrame(df_outlier_prob[df_outlier_prob['target']>=factored_cutoff]['card_id'])\n",
    "print(outlier_id.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "_uuid": "e4c9575fad64e1cf9bd38e48fcf5e988b850189f"
   },
   "outputs": [],
   "source": [
    "# best_submission = pd.read_csv('../input/elo-output/submit_targetenc_nullreplacecategory.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "_uuid": "3a34e578727d85a6a368c97c4f23c3e3e11b5175"
   },
   "outputs": [],
   "source": [
    "model_without_outliers.loc[model_without_outliers[\"card_id\"].isin(outlier_id[\"card_id\"].values), \"target\"]\\\n",
    "    = best_submission[best_submission[\"card_id\"].isin(outlier_id[\"card_id\"].values)][\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "_uuid": "21e2c892dc716481293cb0b34f6b67be3a912aa9"
   },
   "outputs": [],
   "source": [
    "model_without_outliers.to_csv(\"submission_ensembled_Tuned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "_uuid": "243f1506ce88adfb4824399b26cab106bc6b890e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "_uuid": "4937674c43fe9818745a98cacae2122db275241b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
