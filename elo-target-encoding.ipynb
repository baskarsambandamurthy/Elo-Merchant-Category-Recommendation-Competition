{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold,StratifiedKFold\nimport warnings\nimport time\nimport sys\nimport datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nfrom sklearn.cluster import MiniBatchKMeans, KMeans\n\nfrom numpy import nansum\nfrom numpy import nanmean\n\nfrom scipy import stats\nimport os\n\npd.options.display.max_rows = 999\npd.options.display.max_columns  = 999","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a059a8dcf9d93a650f1ccaa8e2bfa3e087219f3"},"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n## 1. Loading the data\n\nFirst, we load the `new_merchant_transactions.csv` and `historical_transactions.csv`. In practice, these two files contain the same variables and the difference between the two tables only concern the position with respect to a reference date.  Also, booleans features are made numeric:"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"7a7877dff5c337c09ca111cdcbf527362c9217c7"},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82c25c7cd0d075195fb7bd63211c66f0dac9304b"},"cell_type":"code","source":"def aggregate_transactions(history,agg_func):\n    \n#     if 'purchase_date' in history.columns:\n#         history.loc[:, 'purchase_date'] = pd.DatetimeIndex(history['purchase_date']).\\\n#                                           astype(np.int64) * 1e-9\n\n    agg_history = history.groupby(['card_id']).agg(agg_func)\n#     print('groupby complete')\n    agg_history.columns = ['_'.join(col).strip() for col in agg_history.columns.values]\n    agg_history.reset_index(inplace=True)\n#     print('reset index complete')\n    \n#     df = (history.groupby('card_id')\n#           .size()\n#           .reset_index(name='transactions_count'))\n    \n#     agg_history = pd.merge(df, agg_history, on='card_id', how='left')\n    \n    return agg_history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97d0aa931c92b6ce46abb8cfb68bc83d3c7dd96e"},"cell_type":"code","source":"from tqdm import tqdm\nimport gc\n\ndef getenccolname(colname,cols_agg):\n    if 'var' in cols_agg:\n        colname =\"targetvarenc_\"+colname\n    elif 'std' in cols_agg:\n        colname =\"targetstdenc_\"+colname \n    elif 'sum' in cols_agg:   \n        colname =\"targetsumenc_\"+colname\n    elif 'min' in cols_agg:   \n        colname =\"targetminenc_\"+colname\n    elif 'max' in cols_agg:   \n        colname =\"targetmaxenc_\"+colname\n    elif 'median' in cols_agg:   \n        colname =\"targetmedianenc_\"+colname\n    elif 'count' in cols_agg:   \n        colname =\"targetcountenc_\"+colname\n    elif 'iqmean' in cols_agg:   \n        colname =\"targetiqmeanenc_\"+colname\n    else:\n        colname =\"targetenc_\"+colname\n        \n    return colname\n\ndef _zscore(x):\n    if len(x) > 3:\n        v = x.values\n        m = (v.sum() - v) / (v.size - 1)\n        print('v shape',v.shape)\n        print('m shape',m.shape)\n        print('v values',m.shape)\n        print(v[0:5])\n        print()\n        print()\n        print()\n        print('m values',m.shape)\n        print(m[0:5])\n        vm = v - m[:, None]\n        np.fill_diagonal(vm, 0)\n        s = ((vm ** 2).sum(1) / (v.size - 2)) ** .5\n        return (v - m) / s\n    else:\n        return np.zeros_like(x)\n\ndef iqrdata(df):\n    return df[(df[targetcolname] >= df[targetcolname].quantile(0.25)) & (df[targetcolname] <= df[targetcolname].quantile(0.75)) ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c96b623f6d4b50dadff39b354890ffff7e6f00a"},"cell_type":"code","source":"# # filename ='temp_concat.csv'\n# # os.remove(filename)\n# # os.listdir('.')\n\n# tempdf = pd.DataFrame()\n# tempdf['test'] = pd.Series([-3.5,12,4.2,18,np.nan,25,40,np.nan,1,5,32,15,12,45,98,152,3,6,51,12,8,92,77,102,31,42,np.nan,52,65,35,7,95.4,132])\n\n# tempdf = tempdf[(tempdf['test'] >=10) | (tempdf['test'].isnull())]\n# tempdf['indcol'] = tempdf.index\n# print(tempdf)\n# print(tempdf.shape)\n# tempdf1 = tempdf[0:6]\n# # print(tempdf1)\n# # tempdf1['indcol'] = tempdf1.index\n# tempdf1.to_csv('xyz.csv')\n# tempdf2 = tempdf[6:10]\n# # tempdf2['indcol'] = tempdf2.index\n# tempdf2.to_csv('xyz.csv',mode='a')\n# tempdf3 = tempdf[10:]\n# # tempdf3['indcol'] = tempdf3.index\n# tempdf3.to_csv('xyz.csv',mode='a')\n\n\n# temp = pd.read_csv('xyz.csv',index_col =0,squeeze=True)\n# print()\n# print( temp[temp.isnull()].shape)\n# # temp = pd.read_csv('xyz.csv',squeeze=True)\n# print('temp')\n# print(temp)\n# print()\n\n# temp_byhand= concat_byhand([tempdf1[['test','indcol']],tempdf2[['test','indcol']],tempdf3[['test','indcol']]])\n# print('temp_byhand')\n# print(temp_byhand)\n\n# # temp_new = pd.DataFrame()\n# temp_byhand.rename(columns={0: 'enccol', 1: 'indcol'}, inplace=True)\n# temp_byhand['indcol'] = temp_byhand['indcol'].astype('int64')\n# temp_byhand.set_index('indcol',inplace=True)\n# print(temp_byhand)\n\n# # train[enccol].to_frame().to_csv(filename)\n# # val[enccol].to_frame().to_csv(filename, mode='a',header=False)\n# # test[enccol].to_frame().to_csv(filename, mode='a',header=False)\n\n# # filename ='temp_concat.csv'\n# # enccol = 'targetenc_merchant_id'\n# # tempdf = pd.DataFrame()\n# # tempdf[enccol] = pd.read_csv(filename,index_col =0,squeeze=True,dtype={enccol: np.float64},nrows=15203900)\n# # print(tempdf[enccol].head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1410ae91d9952d8c963c96810c9aa92866da0d5"},"cell_type":"code","source":"def concat_byhand(dfs):\n    mtot=0\n    with open('df_all.bin','wb') as f:\n        for df in dfs:\n            m,n =df.shape\n            mtot += m\n            f.write(df.values.tobytes())\n            typ=df.values.dtype                \n    #del dfs\n    with open('df_all.bin','rb') as f:\n        buffer=f.read()\n        data=np.frombuffer(buffer,dtype=typ).reshape(mtot,n)\n        df_all=pd.DataFrame(data=data,columns=list(range(n))) \n    os.remove('df_all.bin')\n    return df_all","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5bc3ee23c9c2e6a7df508ea015c3702e7c562e69"},"cell_type":"code","source":"def targetencode_train_main(train,val,test,catcolnames,targetcolname,\n                             smoothing, min_samples_leaf,noise_level,cutoff,cols_agg=['mean']):\n\n    for i,curcol in enumerate(catcolnames):\n        print()\n        print('curcol: ',curcol)\n        print()\n        \n        enccol = getenccolname(curcol,'mean')\n        #remove encoding column from test if exist\n        if enccol in test:\n            del test[enccol]\n        #Smoothing the target encoding values\n        averages = train[[curcol,targetcolname]].groupby(curcol)[targetcolname].agg([\"mean\", \"count\"])\n#         print('averages before smoothing:',averages.head(15))\n        \n        smoothing_v = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf[i]) / smoothing[i]))\n        averages[enccol] = train[targetcolname].mean() * (1 - smoothing_v) + averages[\"mean\"] * smoothing_v\n\n        averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n\n        np.random.seed(42)\n        noise = np.random.randn(len(averages[enccol])) * noise_level[i]\n        averages[enccol] = averages[enccol] + noise\n        \n#         print('averages after smoothing:',averages.head(15))\n        del smoothing_v,noise;gc.collect()\n\n        start = time.time()\n        train[enccol] = train[curcol].map(averages[enccol])\n        val[enccol] = val[curcol].map(averages[enccol])\n        test[enccol] = test[curcol].map(averages[enccol])\n        end = time.time()\n        print('update exec time:',end- start)\n        \n        print(train[enccol].shape)\n        print(val[enccol].shape)\n        print(test[enccol].shape)\n        \n        del averages;gc.collect()\n\n    return train,val,test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"57fbdbe0f4dd8e2e2dfc7280e06c8e36159b47a7"},"cell_type":"code","source":"# curcol ='merchant_id'\n# targetcolname= 'target'\n# averages = merged_trans[[curcol,targetcolname]].groupby(curcol)[targetcolname].agg(\"count\")\n# averages = averages.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67fe56afa48f18be8803752ea3ffed650118f19a"},"cell_type":"code","source":"# print(averages.loc[averages['target'] ==1,curcol].nunique())\n# print(averages.loc[averages['target'] <2,curcol].nunique())\n# print(averages.loc[averages['target'] <=5,curcol].nunique())\n# print(averages.loc[averages['target'] <=15,curcol].nunique())\n# print(averages.loc[averages['target'] <=25,curcol].nunique())\n# print(averages.loc[averages['target'] <=50,curcol].nunique())\n# print(averages.loc[averages['target'] <=100,curcol].nunique())\n# print(averages.loc[averages['target'] >100,curcol].nunique())\n# print(averages.loc[averages['target'] >1000,curcol].nunique())\n# print(averages.loc[averages['target'] >2000,curcol].nunique())\n# print(averages.loc[averages['target'] >10000,curcol].nunique())\n# print(averages.loc[averages['target'] >50000,curcol].nunique())\n# print(averages.loc[averages['target'] >100000,curcol].nunique())\n# # print(averages.loc[averages['target'] >500000,curcol].nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"888f29099a4f7eb35320676b8f13d708d52cae31"},"cell_type":"code","source":"from functools import partial\n\ndef performsmoothing(averages,targetcolname,train,agg,countSeries,smoothing,min_samples_leaf,noise_level,global_agg_val=None):\n    \n#         smoothing_v = 1 / (1 + np.exp(-((averages[\"count\"] ) - min_samples_leaf) / smoothing))\n        smoothing_v = 1 / (1 + np.exp(-((countSeries - min_samples_leaf) / smoothing)) )\n        \n#         print('averages[count] describe:',averages[\"count\"].describe())\n        print('smoothing_v describe:',smoothing_v.describe())\n        if agg=='mean':\n#             global_agg_val = train[targetcolname].mean()\n#             print('train[targetcolname] describe:',train[targetcolname].describe())\n            if global_agg_val is None:\n                global_agg_val = np.nanmean(train[targetcolname].values)\n        elif agg=='std':\n            global_agg_val= train[targetcolname].std()\n         \n        newcol ='newcol'\n        if agg=='std':\n            print('std before smoothing:',averages[agg].head(25))\n        \n        print('averages[agg].shape:',averages[agg].shape)\n        print('smoothing_v.shape:',smoothing_v.shape)\n#         print('global_agg_val:',global_agg_val)\n        \n        averages[newcol] = global_agg_val * (1 - smoothing_v) + averages[agg] * smoothing_v\n\n        np.random.seed(42)\n        noise = np.random.randn(len(averages[newcol])) * noise_level\n        averages[newcol] = averages[newcol] + noise\n        \n        if agg=='std':\n            print('std after smoothing:',averages[newcol].head(25))\n        \n        del smoothing_v,noise;gc.collect()\n        \n        return averages[newcol]\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"813ddb1d1df941da17cd96d517749100ca93f818"},"cell_type":"code","source":"def leaveoneoutmean(data,enccol_sum,enccol_count,targetcol):\n    return (data[enccol_sum] - data[targetcol]) / (data[enccol_count] - 1)\ndef targetencode_merchant(card_ids_tr,card_ids_val,trans,catcolnames,targetcolname,\n                         smoothing,min_samples_leaf,noise_level):\n    #Target Encoding\n    \n#     trans['indcol'] = trans.index\n   \n    tr_mask =( trans['card_id'].isin(card_ids_tr) ) # | (trans['card_id'].isin(card_ids_val))\n    val_mask = trans['card_id'].isin(card_ids_val)\n    test_mask = (~tr_mask) & (~val_mask)\n#     test_mask = (~tr_mask) \n    \n    for curcol in catcolnames:\n        print()\n        print('curcol: ',curcol)\n        print()\n        \n        train = trans.loc[tr_mask,[curcol,targetcolname,'card_id','merchant_category_id_mean']]\n        print('merchant nunique before:',train['merchant_id'].nunique())\n        train['group_count'] = train.groupby([curcol])['card_id'].transform('count')\n        mask = (train['group_count']>=3) \n        print('merchant nunique of less than cutoff:',train.loc[~mask,'merchant_id'].nunique())\n        train= train[mask]\n        print('merchant nunique after cutoff:',train['merchant_id'].nunique())\n        print('valid merchant nunique :',trans.loc[val_mask,'merchant_id'].nunique())\n        print('test merchant nunique :',trans.loc[test_mask,'merchant_id'].nunique())    \n        \n        enccol_sum = getenccolname(curcol,'sum')\n        enccol_count = getenccolname(curcol,'count')\n        enccol_mean = getenccolname(curcol,'mean')\n        \n        averages = train.groupby(curcol).agg({targetcolname: \"mean\",\n                                           'group_count': \"first\",\n                                           'merchant_category_id_mean': \"mean\"})\n        averages.columns =['mean',enccol_count,'merchant_category_id_mean']\n#         averages = train[[curcol,targetcolname,'group_count']].groupby(curcol).agg({targetcolname:\"sum\", \n#                                                                      'group_count': \"first\"})\n#         averages.columns =[enccol_sum,enccol_count]\n        start = time.time()\n    \n#         trans.loc[(tr_mask),enccol_sum] = trans.loc[(tr_mask),curcol].map(averages[enccol_sum])\n#         trans.loc[(tr_mask),enccol_count] = trans.loc[(tr_mask),curcol].map(averages[enccol_count])\n        #LOO - Leave one out - exclude the current card id's target and then compute mean for merchant id\n#         trans.loc[(tr_mask),enccol_mean]  = trans[tr_mask][enccol_sum]  / trans[tr_mask][enccol_count]\n        averages[enccol_mean] = performsmoothing(averages,targetcolname,trans[tr_mask],'mean',averages[enccol_count],\n                                          smoothing,min_samples_leaf,noise_level,global_agg_val=averages['merchant_category_id_mean'])\n        averages.drop(['mean'],axis=1,inplace=True)\n        trans.loc[(tr_mask),enccol_mean] = trans.loc[(tr_mask),curcol].map(averages[enccol_mean])\n        trans.loc[(val_mask),enccol_mean] = trans.loc[(val_mask),curcol].map(averages[enccol_mean])\n        #Fill NA in validation\n        trans.loc[(val_mask) & (trans[enccol_mean].isnull()),enccol_mean]=trans.loc[(val_mask) & (trans[enccol_mean].isnull()),'merchant_category_id_mean']\n        trans.loc[(test_mask),enccol_mean] = trans.loc[(test_mask),curcol].map(averages[enccol_mean])\n        #Fill NA in test\n        trans.loc[(test_mask) & (trans[enccol_mean].isnull()),enccol_mean]=trans.loc[(test_mask) & (trans[enccol_mean].isnull()),'merchant_category_id_mean']\n#         trans.loc[(tr_mask),enccol_mean]  = leaveoneoutmean(trans[tr_mask],enccol_sum,enccol_count,targetcolname)\n        del train;gc.collect()\n#         trans.loc[(val_mask),enccol_sum] = trans.loc[(val_mask),curcol].map(averages[enccol_sum])\n#         trans.loc[(val_mask),enccol_count] = trans.loc[(val_mask),curcol].map(averages[enccol_count])\n        # For validation, the target of the validation cards have not been used in sum, so compute the mean directly \n#         trans.loc[(val_mask),enccol_mean]  = trans[val_mask][enccol_sum]  / trans[val_mask][enccol_count]\n        \n#         trans.loc[(test_mask),enccol_sum]  = trans.loc[(test_mask),curcol].map(averages[enccol_sum])\n#         trans.loc[(test_mask),enccol_count] = trans.loc[(test_mask),curcol].map(averages[enccol_count])\n# #         For test, target is NA always, so compute the mean directly \n#         trans.loc[(test_mask),enccol_mean]  = trans[test_mask][enccol_sum]  / trans[test_mask][enccol_count]\n        \n        end = time.time()\n        print('trans update exec time:',end- start)\n        \n        del averages;gc.collect()\n\n#         print('concat end')\n#             print('Fill NA for test...')\n#             global_mean= train[enccol].mean()\n#             test[enccol].fillna(global_mean,inplace=True)\n            \n#     print('Target Encoding per Trans completed')\n    return trans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d51809ae4b82c4dcfe723f54ce741951d897fcb"},"cell_type":"code","source":"def targetencode_card(trans,targetcolnames,\n                        smoothing, min_samples_leaf,noise_level):\n    #Target Encoding\n    \n    trans_cutoff = trans[trans['targetcountenc_merchant_id'] >=2]\n    \n    print('Target enc card:')\n    #curcol belongs to target col name and not catcolnames\n    for curcol in targetcolnames:\n        print()\n        print('curcol: ',curcol)\n        print()\n        \n        trans_cutoff['card_merch_count'] = trans_cutoff[['card_id','merchant_id']].groupby(['card_id'])['merchant_id'].transform('count')\n        trans_cutoff['card_merch_card_count_max'] = trans_cutoff[['card_id','targetcountenc_merchant_id']].groupby(['card_id'])['targetcountenc_merchant_id'].transform('max')\n        trans_cutoff['card_net_count'] = trans_cutoff['card_merch_card_count_max'] + trans_cutoff['card_merch_count']\n\n#         enccol = getenccolname(curcol,'mean')\n\n        averages = trans_cutoff[['card_id',curcol,'card_net_count']].groupby('card_id').agg({curcol: nanmean,\n                                                                                     'card_net_count': 'first'})\n        averages.columns =['mean','count']\n        print('averages columns:',averages.columns)\n        print('averages head:',averages.head())\n        \n#         print(trans_cutoff[(trans_cutoff['card_id'].isin(['C_ID_0001238066'])) & (~trans_cutoff[curcol].isnull())].head(10))\n#         print(trans_cutoff[(trans_cutoff['card_id'].isin(['C_ID_0001793786'])) & (~trans_cutoff[curcol].isnull())].head(10))\n\n        agg_history = pd.DataFrame()\n        agg_history[curcol+'_mean']= performsmoothing(averages,curcol,trans_cutoff,'mean',averages['count'],\n                                          smoothing,min_samples_leaf,noise_level)\n\n        averages.drop([\"mean\"], axis=1, inplace=True)\n#         agg_history.columns = ['_'.join(col).strip() for col in agg_history.columns.values]\n        print(agg_history.columns)\n        agg_history.reset_index(inplace=True)\n        \n        del averages;gc.collect()\n\n    #         print('concat end')\n    #             print('Fill NA for test...')\n    #             global_mean= train[enccol].mean()\n    #             test[enccol].fillna(global_mean,inplace=True)\n\n    #     print('Target Encoding per Trans completed')\n    return agg_history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76a28e96cdea7b34e7d7b2353d83a9231f692eb8"},"cell_type":"code","source":"def targetenc_merge(train,val,test,trans_agg,dispname):\n    train = pd.merge(train, trans_agg, on='card_id', how='left')\n    val = pd.merge(val, trans_agg, on='card_id', how='left')\n    test = pd.merge(test, trans_agg, on='card_id', how='left')\n    del trans_agg;gc.collect()\n    print('{0} encoding merge complete'.format(dispname))\n    return train,val,test\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffeb2b22568473e8d743b98c769f05708825a50b"},"cell_type":"code","source":"def gensumtargetenc(train,val,test):\n    targetenccols = [col for col in train.columns if ('targetenc_' in col) & ('_mean' in col)]\n    for df in [train,val,test]:\n        df['sum_targetenc'] = df[targetenccols].sum(axis=1)\n        df['mean_targetenc'] = df[targetenccols].mean(axis=1)\n        df['std_targetenc'] = df[targetenccols].std(axis=1)\n#         print('sum target enc describe:',df['sum_targetenc'].describe())\n#         print('mean target enc describe:',df['mean_targetenc'].describe())\n    return train,val,test\n\ndef droptargetenccols(train, val,test):\n     #remove target encoding fields if present\n    targetenccols = [col for col in train.columns if ('targetenc' in col) or ('targetstdenc' in col)]\n    train.drop(targetenccols,axis=1,inplace=True)\n    val.drop(targetenccols,axis=1,inplace=True)\n    targetenccols_test = [col for col in test.columns if ('targetenc' in col) or ('targetstdenc' in col)]\n    test.drop(targetenccols_test,axis=1,inplace=True)\n    \n    return train, val,test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"23dd7921c4e6158c1641dc4ae25406de2cf22bf7"},"cell_type":"code","source":"def targetencode_modelfold_mergedtrans(train, val,test,df_trans_data,df_trans_col_names,catcolnames,targetcolname,\n                           smoothing, min_samples_leaf,noise_level, cutoff):\n    #Retrieve current train card ids and filter out hist and new only for these card ids\n    card_ids_tr= list(train['card_id'].unique())\n    card_ids_val= list(val['card_id'].unique())\n\n    #Perform target encoding for each transaction data\n    for i,df in enumerate(df_trans_data):\n        print()\n        print('************** TRANS {0} **********************'.format(df_trans_col_names[i]))\n        \n        df= targetencode_merchant(card_ids_tr,card_ids_val,df,catcolnames,targetcolname,\n                                 smoothing, min_samples_leaf,noise_level)\n        \n#         targetcolnames_card =[]\n#         for col in catcolnames:\n#             targetcolnames_card +=['targetenc_'+col]\n#         df_agg= targetencode_card(df,targetcolnames_card,\n#                         smoothing[i], min_samples_leaf[i],noise_level[i])\n        \n#         df_agg.columns = [df_trans_col_names[i] + '_' + c if c != 'card_id' else c for c in df_agg.columns]\n    \n        agg_func={}\n        for curcol in catcolnames:\n            agg_func['targetenc_'+curcol] =['mean']\n\n        df_agg = aggregate_transactions(df,agg_func)\n        df_agg.columns = [df_trans_col_names[i] + '_' + c if c != 'card_id' else c for c in df_agg.columns]\n    \n    \n    \n        print('aggregate_transactions complete')\n\n        train_index = train.index\n        val_index = val.index\n        test_index = test.index\n\n        train,val,test = targetenc_merge(train,val,test,df_agg,df_trans_col_names[i])    \n        \n        #restore indices\n        train.index = train_index\n        val.index = val_index\n        test.index = test_index\n    \n        #drop target enc in trans to clear memory\n        targetenccols = [col for col in df.columns if ('targetenc' in col) or ('targetstdenc' in col)]\n        df.drop(targetenccols,axis=1,inplace=True)\n\n    print('Target Encoding completed')\n    #generate sum target encoding columns\n#     if len(df_trans_data) >1:\n#     print('computing sum target encoding...')\n#     train,val,test=gensumtargetenc(train,val,test)\n\n    return train,val,test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffeb2b22568473e8d743b98c769f05708825a50b"},"cell_type":"code","source":"# def targetencode_modelfold_mergedtrans(train, val,test,df_trans_data,df_trans_col_names,catcolnames,targetcolname,\n#                            smoothing, min_samples_leaf,noise_level, cutoff):\n#     #Retrieve current train card ids and filter out hist and new only for these card ids\n#     card_ids_tr= list(train['card_id'].unique())\n#     card_ids_val= list(val['card_id'].unique())\n\n#     #Perform target encoding for each transaction data\n#     for i,df in enumerate(df_trans_data):\n#         print()\n#         print('************** TRANS {0} **********************'.format(df_trans_col_names[i]))\n        \n#         df= targetencode_withoutfold(card_ids_tr,card_ids_val,df,catcolnames,targetcolname,\n#                                      smoothing[i], 2000,noise_level[i], cutoff[i],50)\n#         print('cutoff 100 complete')\n#         df= targetencode_withoutfold(card_ids_tr,card_ids_val,df,catcolnames,targetcolname,\n#                                      smoothing[i], 1000,noise_level[i], 100,1000)\n# #         print('cutoff 1000 complete')\n# #         df= targetencode_withoutfold(card_ids_tr,card_ids_val,df,catcolnames,targetcolname,\n# #                                      smoothing[i], 1000,noise_level[i],1000,5000)        \n# #         print('cutoff 5000 complete')\n# #         df= targetencode_withoutfold(card_ids_tr,card_ids_val,df,catcolnames,targetcolname,\n# #                                      smoothing[i], 5000,noise_level[i],5000,10000) \n# #         print('cutoff 10000 complete')\n# #         df= targetencode_withoutfold(card_ids_tr,card_ids_val,df,catcolnames,targetcolname,\n# #                                      smoothing[i], 10000,noise_level[i],10000,100000)         \n# #         print('cutoff 100000 complete')\n#         #Aggregate encoding data\n# #         agg_func = {\n# #         'targetenc_merchant_id' : ['mean'],\n# #         }\n#         agg_func={}\n#         for curcol in catcolnames:\n#             agg_func['targetenc_'+curcol] =['mean']\n#             agg_func['targetstdenc_'+curcol] =['min','max']\n# #             agg_func['targetenc_'+curcol] =['mean','std','min','max','count']\n\n#         df_agg = aggregate_transactions(df,agg_func)\n#         df_agg.columns = [df_trans_col_names[i] + '_' + c if c != 'card_id' else c for c in df_agg.columns]\n    \n#         print('aggregate_transactions complete')\n\n#         train_index = train.index\n#         val_index = val.index\n#         test_index = test.index\n\n#         train,val,test = targetenc_merge(train,val,test,df_agg,df_trans_col_names[i])    \n        \n#         #restore indices\n#         train.index = train_index\n#         val.index = val_index\n#         test.index = test_index\n    \n#         #drop target enc in trans to clear memory\n#         targetenccols = [col for col in df.columns if ('targetenc' in col) or ('targetstdenc' in col)]\n#         df.drop(targetenccols,axis=1,inplace=True)\n\n#     print('Target Encoding completed')\n#     #generate sum target encoding columns\n# #     if len(df_trans_data) >1:\n# #     print('computing sum target encoding...')\n# #     train,val,test=gensumtargetenc(train,val,test)\n\n#     return train,val,test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"888f29099a4f7eb35320676b8f13d708d52cae31"},"cell_type":"code","source":"# def targetencode_withoutfold(card_ids_tr,card_ids_val,trans,catcolnames,targetcolname,\n#                              smoothing, min_samples_leaf,noise_level,cutoff_low,cutoff_high,cols_agg=['mean']):\n#     #Target Encoding\n    \n# #     trans['indcol'] = trans.index\n   \n#     tr_mask = trans['card_id'].isin(card_ids_tr)\n#     val_mask = trans['card_id'].isin(card_ids_val)\n#     test_mask = (~tr_mask) & (~val_mask)\n\n#     for curcol in catcolnames:\n#         print()\n#         print('curcol: ',curcol)\n#         print()\n# #         train = trans[tr_mask]\n# #         val = trans[val_mask]\n# #         test = trans[test_mask]\n#         train = trans.loc[tr_mask,[curcol,targetcolname,'card_id']]\n#         val = trans.loc[val_mask,[curcol,targetcolname,'card_id']]\n#         test = trans.loc[test_mask,[curcol,targetcolname,'card_id']]\n\n#         train['group_count'] = train.groupby([curcol])['card_id'].transform('count')\n#         mask = (train['group_count']>=cutoff_low) &  (train['group_count']<cutoff_high)\n# #         mask = (train['group_count']>=cutoff)\n#         train= train[mask]\n        \n#         enccol = getenccolname(curcol,'mean')\n#         enccol_std = getenccolname(curcol,'std')\n        \n#         averages = train[[curcol,targetcolname]].groupby(curcol)[targetcolname].agg([\"std\",\"mean\", \"count\"])\n# #         print('enccol_std name;',enccol_std)\n# #         averages[enccol_std]= performsmoothing(averages,targetcolname,train,'std',enccol_std,\n# #                                               smoothing,2000,noise_level)\n        \n#         averages[enccol]= performsmoothing(averages,targetcolname,train,'mean',enccol,\n#                                           smoothing,min_samples_leaf,noise_level)\n\n        \n       \n#         averages.drop([\"std\",\"mean\", \"count\"], axis=1, inplace=True)\n\n# #         #Smoothing the target encoding values\n# #         #Add standard deviation also through smoothing\n# #         averages = train[[curcol,targetcolname]].groupby(curcol)[targetcolname].agg([\"std\",\"mean\", \"count\"])\n# # #         print('averages before smoothing:',averages.head(15))\n        \n# #         smoothing_v = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n# #         averages[enccol] = train[targetcolname].mean() * (1 - smoothing_v) + averages[\"mean\"] * smoothing_v\n\n# #         averages.drop([\"std\",\"mean\", \"count\"], axis=1, inplace=True)\n\n# #         np.random.seed(42)\n# #         noise = np.random.randn(len(averages[enccol])) * noise_level\n# #         averages[enccol] = averages[enccol] + noise\n        \n# #         print('averages after smoothing:',averages.head(15))\n# #         del smoothing_v,noise;gc.collect()\n\n#         start = time.time()\n    \n#         trans.loc[(tr_mask),enccol] = trans.loc[(tr_mask),curcol].map(averages[enccol])\n#         trans.loc[(tr_mask),enccol_std] = trans.loc[(tr_mask),curcol].map(averages[enccol_std])\n#         del train;gc.collect()\n#         trans.loc[(val_mask),enccol] = trans.loc[(val_mask),curcol].map(averages[enccol])\n#         trans.loc[(val_mask),enccol_std] = trans.loc[(val_mask),curcol].map(averages[enccol_std])\n#         del val;gc.collect()\n#         trans.loc[(test_mask),enccol]  = trans.loc[(test_mask),curcol].map(averages[enccol])\n#         trans.loc[(test_mask),enccol_std] = trans.loc[(test_mask),curcol].map(averages[enccol_std])\n#         del test;gc.collect()\n        \n#         end = time.time()\n#         print('trans update exec time:',end- start)\n#         print('train cur merchant null but enccol nonnull  =', trans.loc[(tr_mask) & (trans[curcol].isnull()) & (~trans[enccol].isnull())].shape )\n        \n# #         print('train total  =',trans.loc[(tr_mask),curcol].nunique() )\n# #         print('train na  =', trans.loc[(tr_mask) & (trans[enccol].isnull()),curcol].nunique() )\n# #         print('test total  =', trans.loc[(val_mask),curcol].nunique())\n# #         print('test na  =', trans.loc[(val_mask) & (trans[enccol].isnull()),curcol].nunique() )\n# #         print('valid total  =', trans.loc[(test_mask),curcol].nunique() )\n# #         print('valid na  =', trans.loc[(test_mask) & (trans[enccol].isnull()),curcol].nunique() )\n\n\n#         print(trans[enccol].shape)\n# #         print(trans.loc[trans[curcol].isin(list(averages.index[0:15])),[curcol,enccol]].head(30))\n#         del averages;gc.collect()\n\n# #         print('concat end')\n# #             print('Fill NA for test...')\n# #             global_mean= train[enccol].mean()\n# #             test[enccol].fillna(global_mean,inplace=True)\n            \n# #     print('Target Encoding per Trans completed')\n#     return trans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76a28e96cdea7b34e7d7b2353d83a9231f692eb8"},"cell_type":"code","source":"#Target Encoding on categorical features\n\n# def targetencode_modelfold(train, val,test,hist,new,catcolnames,targetcolname,\n#                            smoothing, min_samples_leaf,noise_level):\n#     #Retrieve current train card ids and filter out hist and new only for these card ids\n#     card_ids_tr= list(train['card_id'].unique())\n#     card_ids_val= list(val['card_id'].unique())\n    \n#     #Perform target encoding for each transaction data\n#     print()\n#     print('************** HIST TRANS **********************')\n#     hist= targetencode_withoutfold(card_ids_tr,card_ids_val,hist,catcolnames,targetcolname,smoothing[0], min_samples_leaf[0],noise_level[0])\n#     print()\n#     print('************** NEW TRANS **********************')\n#     new = targetencode_withoutfold(card_ids_tr,card_ids_val,new,catcolnames,targetcolname,smoothing[1], min_samples_leaf[1],noise_level[1])\n    \n#     #Aggregate encoding data\n#     agg_func = {\n# #     'targetenc_merchant_id' : ['mean'],\n#     'targetenc_merchant_id' : ['mean','std',np.ptp],\n#     }\n\n#     hist_agg = aggregate_transactions(hist,agg_func)\n#     hist_agg.columns = ['hist_' + c if c != 'card_id' else c for c in hist_agg.columns]\n# #     print('hist aggregate_transactions complete')\n\n#     new_agg = aggregate_transactions(new,agg_func)\n#     new_agg.columns = ['new_' + c if c != 'card_id' else c for c in hist_agg.columns]\n    \n#     print('aggregate_transactions complete')\n\n#     #remove target encoding fields if present\n#     targetenccols = [col for col in train.columns if 'targetenc_' in col]\n#     train.drop(targetenccols,axis=1,inplace=True)\n#     val.drop(targetenccols,axis=1,inplace=True)\n#     targetenccols_test = [col for col in test.columns if 'targetenc_' in col]\n#     test.drop(targetenccols_test,axis=1,inplace=True)\n\n        \n#     train_index = train.index\n#     val_index = val.index\n#     test_index = test.index\n    \n#     train,val,test = targetenc_merge(train,val,test,hist_agg,'history')    \n#     train,val,test = targetenc_merge(train,val,test,new_agg,'new')    \n\n#     #restore indices\n#     train.index = train_index\n#     val.index = val_index\n#     test.index = test_index\n\n#     print('Target Encoding completed')\n    \n#     #drop target enc in trans to clear memory\n#     hist.drop(targetenccols,axis=1,inplace=True)\n#     new.drop(targetenccols,axis=1,inplace=True)\n    \n#     return train,val,test","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b787e3bd4b773012c2ec6a9f47e2b71bc115dc55"},"cell_type":"markdown","source":"Read Data"},{"metadata":{"trusted":true,"_uuid":"40011d29095248a754857beee88bcf1640197e29"},"cell_type":"code","source":"# Path = '../input/elo-ref-2-data-conversion/'\n# historical_transactions = pd.read_hdf(Path+'historical_transactions.hdf')\n# new_transactions = pd.read_hdf(Path+'new_transactions.hdf')\n# print('transactions read complete')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0aa430843c0c64afba714a3ff9898ed9e170f7a9"},"cell_type":"code","source":"Path = '../input/elo-ref-2-data-conversion/'\nhistorical_transactions = pd.read_csv(Path+'historical_transactions.csv',index_col=0)\nnew_transactions = pd.read_csv(Path+'new_transactions.csv',index_col=0)\nprint('transactions read complete')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff76fba44b374f9467d64b8e37089bea4c6450ad"},"cell_type":"code","source":"historical_transactions = reduce_mem_usage(historical_transactions)\nnew_transactions = reduce_mem_usage(new_transactions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eff39192d57204e5da6aa795afa5852904d77565"},"cell_type":"code","source":"# train_raw = pd.read_csv('../input/elo-merchant-category-recommendation/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45883088a7b65b17c846b6b82d0190c33ce77492"},"cell_type":"code","source":"# Path = '../input/elo-preproc-3/'\n# train = pd.read_hdf(Path+'train_preproc.hdf')\n# print('train read complete')\n# test = pd.read_hdf(Path+'test_preproc.hdf')\n# print('test read complete')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32820a6b4f989df01a032ceb321e8688498deb26"},"cell_type":"code","source":"Path = '../input/elo-preproc-3/'\ntrain = pd.read_csv(Path+'train_preproc.csv')\nprint('train read complete')\ntest = pd.read_csv(Path+'test_preproc.csv')\nprint('test read complete')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf1f9cb5a149eedc775758123bc061eeb7dbc09a"},"cell_type":"code","source":"# target = train_raw['target']\n# train['target'] = target","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"93c54a31cd55d6b9c270d924c3802098d1220e29"},"cell_type":"markdown","source":"**Test with Label**"},{"metadata":{"trusted":true,"_uuid":"e7b6a540c85f8e081b932c683eaf39b406c7a68e"},"cell_type":"code","source":"# #train and test sample\n# train_copy = train.copy()\n# test_copy = test.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"872d529663e56cb5af29555ea35d08c4ca401aec"},"cell_type":"code","source":"# train = train_copy.sample(frac=0.01)\n# print(train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8890486a963b8f8195e7a8b7707ef16d76a15d8"},"cell_type":"code","source":"# #split train to train and test for target enc test check\n# n_splits=5\n# folds_test = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=4590)\n# for fold_, (trn_idx, val_idx) in enumerate(folds_test.split(train,train['outliers'].values)):\n#     print(\"fold {}\".format(fold_))\n#     test = train.iloc[val_idx]\n#     test_target=train['target'].iloc[val_idx]\n#     train = train.iloc[trn_idx]\n#     target=train['target']\n    \n#     break\n\n# print(train.shape)\n# print(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94e0aba2fa523c67ccc5830da76ac736c3a6559c"},"cell_type":"code","source":"# # Filter history and new trans to contain only the new train and test ids\n# filter_cardids = list(set(train['card_id'].unique()).union(test['card_id'].unique()))\n# historical_transactions = historical_transactions[historical_transactions['card_id'].isin(filter_cardids)]\n# new_transactions = new_transactions[new_transactions['card_id'].isin(filter_cardids)]\n# print(historical_transactions.shape)\n# print(new_transactions.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce2680c9bd3295d34adf7da6a8c7b80231997abc"},"cell_type":"markdown","source":"![](http://)K FOLD Setting"},{"metadata":{"trusted":true,"_uuid":"fcdde8f1522fbda1daca4e9a6216394772931c09"},"cell_type":"code","source":"hastestlabels = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"603bc543ccf9911a2eca3f0e4bf31bd9b703c07e"},"cell_type":"code","source":"n_splits=5\nfolds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=4590)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3fca2fa7f7ef7d9c5cdefe4f52f16b9e8dafc5a9"},"cell_type":"code","source":"target = train['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"205e1ac5276b4d5b03d855c9670bdf47bc26a0f0"},"cell_type":"code","source":"# historical_group =historical_transactions[trans_cols].groupby(['merchant_id','card_id']).agg('first').reset_index()\n# print(historical_group.head())\n# historical_transactions['merch_card_count'] =historical_group.groupby(['merchant_id'])['card_id'].transform('count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7beb6d6d85c5b4812e909ce016e2ff5a63cb4162"},"cell_type":"code","source":"#merge target to history data \n# also add merch_card_purchase_count\n# historical_transactions =historical_transactions.groupby(['merchant_id','card_id']).size().reset_index(name='merch_card_purchase_count')\n# print('history grouping complete')\n\n# new_transactions =new_transactions.groupby(['merchant_id','card_id']).size().reset_index(name='merch_card_purchase_count')\n# print('new grouping complete')\n\n# trans_cols =['merchant_id','card_id','merchant_category_id','subsector_id']\n\n# historical_transactions =historical_transactions[trans_cols].groupby(['merchant_id','card_id']).agg('first').reset_index()\n# # historical_transactions['merch_card_count'] =historical_transactions.groupby(['merchant_id'])['card_id'].transform('count')\n# print('history grouping complete')\n\n# new_transactions =new_transactions[trans_cols].groupby(['merchant_id','card_id']).agg('first').reset_index()\n# # new_transactions['merch_card_count'] =new_transactions.groupby(['merchant_id'])['card_id'].transform('count')\n# print('new grouping complete')\n\n\nhistorical_transactions = pd.merge(historical_transactions,train[['target','card_id']],on='card_id',how='left')\nprint('history merge complete')\n\nnew_transactions = pd.merge(new_transactions,train[['target','card_id']],on='card_id',how='left')\nprint('new merge complete')\n\nfor i,df in enumerate([historical_transactions,new_transactions]):\n    trans_cols =['target','merchant_category_id']\n    df['merchant_category_id_mean'] =df[trans_cols].groupby(['merchant_category_id']).transform('mean')\n    trans_cols =['target','subsector_id']\n    df['subsector_id_mean'] =df[trans_cols].groupby(['subsector_id']).transform('mean')\n    print('grouping target mean completed:',i)\n\n\nhistorical_transactions['outliers'] = 0\nhistorical_transactions.loc[historical_transactions['target'] < -30, 'outliers'] = 1\nhistorical_transactions['outliers'].value_counts()\n\nnew_transactions['outliers'] = 0\nnew_transactions.loc[new_transactions['target'] < -30, 'outliers'] = 1\nnew_transactions['outliers'].value_counts()\n\nprint('target process complete')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9304bdfe30b6115844182677a782721903471468"},"cell_type":"code","source":"\n# tr_mask = ~historical_transactions['target'].isnull()\n# train_temp = historical_transactions.loc[tr_mask,['merchant_id','target','card_id','merch_card_count']]\n# mask = (train_temp['merch_card_count']>=2) \n# train_temp= train_temp[mask]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"23e84830cbb1baebafd4e10652da9e4f6cb191cc"},"cell_type":"code","source":"# averages = train_temp[['merchant_id','card_id']].groupby('merchant_id')['card_id'].agg([\"count\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ba66445e03d8e813027cdcf3201864dbe647bbb"},"cell_type":"code","source":"# print(averages.head())\n# print(averages[averages['count']==1].head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"648162560fc24bb006ad983cd8318d3062d858dd"},"cell_type":"code","source":"targetcolname ='target'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1027583dc30255517df8f040497dda163557927"},"cell_type":"code","source":"# #Computer merchant level outliers\n# import scipy.stats as sss\n\n# for i,df in enumerate([historical_transactions,new_transactions]):\n#     df['merch_zscore']=df.groupby(['merchant_id'])[targetcolname].transform(sss.zscore)\n#     df.loc[df['merch_zscore'].abs()>3,'target'] = np.nan\n#     print('transaction complete:i',i)\n    \n# print('merchant outlier complete')\n# #only 12 outliers detected and need to explore","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d4ca2b0f9f6fb508840ca14306ac2f1773b2837"},"cell_type":"markdown","source":"**Old Target Encoding**"},{"metadata":{"trusted":true,"_uuid":"a4b2c5f02c8e80842ad0decdb2819d236512ce1f"},"cell_type":"code","source":"# print(historical_transactions[historical_transactions['merch_card_count']>=10].shape)\n# new_transactions[new_transactions['merch_card_count']>=10].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"51baf482a1ca76195f240b6fa669420815e2accd"},"cell_type":"code","source":"# #No of test records whose merchant id in train is having only one card\n\n# mask = historical_transactions['target'].isnull()\n\n# hist_merch_ids = historical_transactions.loc[(~mask) & (historical_transactions['merch_card_count']<10),'merchant_id'].unique()\n# train_w_test_single =  historical_transactions.loc[(mask) & (historical_transactions['merchant_id'].isin(hist_merch_ids)),'merchant_id'].unique()\n# print(len(train_w_test_single))\n\n# mask = new_transactions['target'].isnull()\n# new_merch_ids = new_transactions.loc[(~mask) & (new_transactions['merch_card_count']<10),'merchant_id'].unique()\n# train_w_test_single =  new_transactions.loc[(mask) & (new_transactions['merchant_id'].isin(new_merch_ids)),'merchant_id'].unique()\n# print(len(train_w_test_single))\n\n# # 51% of hist test merchants and 65% of new test merchants will be NA for merchant card count < 10 in train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94a35906f3501480313e58c64af3b30eade7603f"},"cell_type":"code","source":"# mask = historical_transactions['target'].isnull()\n# print(historical_transactions.loc[(mask),'merchant_id'].nunique())\n# mask = new_transactions['target'].isnull()\n# print(new_transactions.loc[(mask),'merchant_id'].nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"edaafff77bf9628a9f55440a3cbdf736f27fb15b"},"cell_type":"code","source":"# #for train trans- compute target encoding average for each merchant record excluding the current card target value\n# #for test trans- compute target encoding average including all merchant records\n# for df in [historical_transactions,new_transactions]:\n# # for df in [historical_transactions]:\n#     mask = df['target'].isnull()\n#     # for train \n#     df.loc[~mask,'targetenc_mean_merchant_id']= df.loc[~mask,'targetsumenc_merchant_id'] - df.loc[~mask,'target'] / (df.loc[~mask,'targetcountenc_merchant_id']- 1)\n#     # for test \n#     df.loc[mask,'targetenc_mean_merchant_id']= df.loc[mask,'targetsumenc_merchant_id'] / df.loc[mask,'targetcountenc_merchant_id']\n# #         df['targetenc_mean']= df['targetenc_sum'] - df['target'] * df['merch_card_purchase_count'] / (df['targetenc_count']- df['merch_card_purchase_count'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee45e6e8a951618fcab61f3b1c45a206628a0bfe"},"cell_type":"code","source":"# # no  of null merchants\n# for i,df in enumerate([historical_transactions,new_transactions]):\n#     mask = df['target'].isnull()\n#     print('i:',i)\n#     print('train total merchant count:',df.loc[(~mask),'merchant_id'].nunique())\n#     print('train null merchant count:',df.loc[(~mask) & (df['targetsumenc_merchant_id'].isnull()),'merchant_id'].nunique())\n#     print('test total merchant count:',df.loc[(mask),'merchant_id'].nunique())\n#     print('test null merchant count:',df.loc[(mask) & (df['targetsumenc_merchant_id'].isnull()),'merchant_id'].nunique())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18d2307599b17eeb7020d5da6148719107349218"},"cell_type":"markdown","source":"**Save Data**"},{"metadata":{"trusted":true,"_uuid":"5fb2d8ffbb3f1e0c586082dd2c3bad569c5ef922"},"cell_type":"code","source":"# train.to_hdf('train_preproc.hdf',key='data')\n# test.to_hdf('test_preproc.hdf',key='data')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00b9c26724353ddb92dfb846e1660ed5cc2e1229"},"cell_type":"code","source":"train['target'] = target\ntrain['outliers'] = 0\ntrain.loc[train['target'] < -30, 'outliers'] = 1\ntrain['outliers'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d288d4109791e57fe6878cf15b8ddbc9e12510b"},"cell_type":"code","source":"sel_feats = ['feature_1',\n 'feature_2',\n 'feature_3',\n 'hist_authorized_flag_mean',\n 'hist_authorized_flag_sum',\n 'hist_card_id_size',\n 'hist_category_1_mean',\n 'hist_category_1_sum',\n 'hist_category_3_mean_mean',\n 'hist_first_buy',\n 'hist_installments_min',\n 'hist_installments_sum',\n 'hist_merchant_group_id_nunique',\n 'hist_merchant_id_nunique',\n 'hist_month_diff_mean',\n 'hist_month_lag_max',\n 'hist_month_lag_mean',\n 'hist_month_nunique',\n 'hist_most_recent_sales_range_std',\n 'hist_purchase_amount_max',\n 'hist_purchase_amount_mean',\n 'hist_purchase_amount_min',\n 'hist_purchase_amount_sum',\n 'hist_purchase_amount_var',\n 'hist_purchase_date_average',\n 'hist_purchase_date_count_mean',\n 'hist_purchase_date_diff',\n 'hist_purchase_date_max',\n 'hist_purchase_date_min',\n 'hist_purchase_date_uptonow',\n 'hist_purchase_duration_max_max',\n 'hist_purchase_duration_max_mean',\n 'hist_repeat_purchase_amount_sum_max',\n 'hist_repeat_purchase_amount_sum_mean',\n 'hist_repeat_purchase_amount_sum_min',\n 'hist_subsector_id_nunique',\n 'hist_sum_purchases_lag_max',\n 'hist_sum_purchases_lag_min',\n 'hist_sum_purchases_lag_std',\n 'hist_sum_sales_lag_max',\n 'hist_sum_sales_lag_mean',\n 'hist_sum_sales_lag_min',\n 'hist_sum_sales_lag_sum',\n 'hist_sum_sales_p_purchases_lag_std',\n 'hist_sum_sales_p_purchases_lag_sum',\n 'hist_weekofyear_nunique',\n 'hist_year_nunique',\n 'new_hist_card_id_size',\n 'new_hist_category_1_mean',\n 'new_hist_category_1_sum',\n 'new_hist_category_2_mean_mean',\n 'new_hist_category_3_mean_mean',\n 'new_hist_hour_nunique',\n 'new_hist_installments_max',\n 'new_hist_installments_mean',\n 'new_hist_installments_min',\n 'new_hist_installments_var',\n 'new_hist_merchant_category_id_nunique',\n 'new_hist_merchant_id_nunique',\n 'new_hist_month_diff_mean',\n 'new_hist_month_lag_max',\n 'new_hist_month_lag_mean',\n 'new_hist_month_lag_var',\n 'new_hist_purchase_amount_max',\n 'new_hist_purchase_amount_mean',\n 'new_hist_purchase_amount_min',\n 'new_hist_purchase_amount_var',\n 'new_hist_purchase_date_average',\n 'new_hist_purchase_date_diff',\n 'new_hist_purchase_date_max',\n 'new_hist_purchase_date_min',\n 'new_hist_purchase_date_uptonow',\n 'new_hist_subsector_id_nunique',\n 'new_most_recent_purchases_range_std',\n#  'new_sum_purchases_lag_std',\n#  'new_sum_purchases_lag_sum',\n#  'new_sum_sales_lag_sum',\n#  'new_sum_sales_p_purchases_lag_sum',\n 'weekofyear']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b69394fe32227e10f57aa9579362577aeececa10"},"cell_type":"markdown","source":"KMeans on all features before target encoding(i.e except target encoding feats)"},{"metadata":{"trusted":true,"_uuid":"1e798ceb48f7ac96588b05414b770bcc6624b2be"},"cell_type":"code","source":"# #KMeans na replaced col creation\n# for df in [train,test]:\n#     print('*****************')\n#     for col in sel_feats:\n#         if df.isnull().any()[col]:\n#             print(col)\n#             newcol =  col+'_na_replaced'\n#             for df_l2 in [train,test]:\n#                 if newcol not in df_l2.columns:\n#                     df_l2[newcol] = df_l2[col]\n#             df[newcol].fillna(df[col].mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1887764f228ae207571321689d3360344612d1e"},"cell_type":"code","source":"# # Kmeans feats : add null filled columns  and remove original of these columns\n# orig_cols  = [col.replace('_na_replaced','') for col in list(train.columns)  if '_na_replaced'  in col]\n# na_replaced_cols  = [col for col in list(train.columns) if '_na_replaced'  in col]\n# # print(orig_cols)\n# sel_feats_kmeans = sel_feats.copy() \n# for val in orig_cols:\n#     sel_feats_kmeans.remove(val)\n\n# sel_feats_kmeans += na_replaced_cols\n# print(len(sel_feats_kmeans))\n# print(sel_feats_kmeans)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4afd3adf06e3365ac3ec831becaf9ed60384c299"},"cell_type":"code","source":"# kmeans = KMeans(n_clusters=15,random_state=2018)\n# print('kmeans fit start..')\n# # kmeans = kmeans.fit(pd.concat([train[sel_feats_kmeans],test[sel_feats_kmeans]]))\n# kmeans = kmeans.fit(train[sel_feats_kmeans])\n# print('kmeans fit end')\n# # \n# train['kmeans_cluster']=kmeans.predict(train[sel_feats_kmeans])\n# test['kmeans_cluster']=kmeans.predict(test[sel_feats_kmeans])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2fa941aed83c56cff9343ec64dba8e15fe3b5cf"},"cell_type":"code","source":"# #save kmeans cluster data\n# train[['card_id','kmeans_cluster']].to_hdf('train_kmeanscluster.hdf',key='data')\n# test[['card_id','kmeans_cluster']].to_hdf('test_kmeanscluster.hdf',key='data')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b0eba3e733ecdbab96d631eb46d42453d82aa20"},"cell_type":"code","source":"excluded_cols = ['card_id', 'first_active_month','target','outliers']\n\n# features = [c for c in train.columns if c not in excluded_cols]\n# features = sel_feats + ['kmeans_cluster']\nfeatures = sel_feats \n\nprint(excluded_cols)\n\n# categorical_feats = [c for c in features if 'feature_' in c ]\n\nprint(features)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e67399b25b3886e306b274069ab240af7d060397"},"cell_type":"markdown","source":"We then set the hyperparameters of the LGBM model:"},{"metadata":{"_uuid":"b7a7377cd7401f2cbd13ea707fbc2a2bebe229a6"},"cell_type":"markdown","source":"1. 1. 1. 1. 1. We now train the model. Here, we use a standard KFold split of the dataset in order to validate the results and to stop the training. Interstingly, during the writing of this kernel, the model was enriched adding new features, which improved the CV score. The variations observed on the CV were found to be quite similar to the variations on the LB: it seems that the current competition won't give us headaches to define the correct validation scheme:"},{"metadata":{"trusted":true,"_uuid":"2070b1bbc2c3f733edff08b2d4c8e7c66a360e55"},"cell_type":"code","source":"# print(train['hist_targetenc_merchant_id_mean'].isna().sum())\n# print(test['hist_targetenc_merchant_id_mean'].isna().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4e8c693a2e023554c0f3dcf395de5c7ae2dcbb3"},"cell_type":"code","source":"param = {'num_leaves': 31,\n         'min_data_in_leaf': 30, \n         'objective':'regression',\n         'max_depth': -1,\n         'learning_rate': 0.01,\n         \"min_child_samples\": 20,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9 ,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.1,\n         \"verbosity\": -1,\n         \"nthread\": 4,\n         \"random_state\": 4590}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"53f6a5ee61188d661ff4dd5908da1a1b21e6a890"},"cell_type":"code","source":"# catcolnames =['merchant_id','state_id','city_id','merchant_category_id','subsector_id','category_1','category_2','category_3']\ncatcolnames =['merchant_id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c68afb58ddfc356063171ebde768845c41a308a3"},"cell_type":"code","source":"min_samples_leaf=2000\nsmoothing=10\nnoise_level=0.1\ncutoff =2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8def3c24540dea59ee51b63a9a6f97abf42c7205"},"cell_type":"code","source":"# min_samples_leaf=[2000,2000,2000,2000,2000]\n# smoothing=[10]\n# noise_level=[0.1]\n# cutoff =[2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e12e13573b872841be06fe2f01fe5423f93e24e"},"cell_type":"code","source":"catcolnames_main= ['dayofweek', 'weekofyear', 'month', 'elapsed_time','feature_1','feature_2','feature_3']\nmin_samples_leaf_main=[400,1000,650,4000,250,150,100]\n# dayofweek - 7, weekofyear - 21, month -12,  elapsed_time - 75, feature_1 - 5, feature_2 - 3, feature_3 - 2\nsmoothing_main=[10,10,10,10,10,10,10]\nnoise_level_main=[0.1,0.1,0.1,0.1,0.1,0.1,0.1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aea17134f3c590a38f5d828c858fe1810e4a195d"},"cell_type":"code","source":"# hist_auth = historical_transactions[historical_transactions['authorized_flag']==1]\n# hist_unauth = historical_transactions[historical_transactions['authorized_flag']==0]\n# new_auth = new_transactions[new_transactions['authorized_flag']==1]\n# new_unauth = new_transactions[new_transactions['authorized_flag']==0]\n\n# merged_dfs=[hist_auth,hist_unauth,new_auth]\n# merged_colnames =['hist_auth','hist_unauth','new']\n\n# del historical_transactions,new_transactions;gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4f526f6a266fd2590f265f9d7fe4b70dd423ca6"},"cell_type":"code","source":"# #Sample temp code\n# temp_df=[]\n# for i,df in enumerate(merged_dfs):\n#     cur_df  = df.sample(frac=0.01)\n#     temp_df += [cur_df]\n#     print(cur_df.shape)\n\n# merged_dfs= temp_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"923d9808b41665721be3dcce645be03ab18d1f6e"},"cell_type":"code","source":"# historical_transactions = pd.concat([hist_auth,hist_unauth])\n# new_transactions = pd.concat([new_auth,new_unauth])\n\n# del hist_auth,hist_unauth,new_auth,new_unauth;gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"203b9d9f5e0338a16b72ae8183cfcd7da994a433"},"cell_type":"code","source":"historical_transactions['ishist']=1\nnew_transactions['ishist']=0\n\nmerged_trans = pd.concat([historical_transactions,new_transactions])\n\ndel historical_transactions,new_transactions;gc.collect()\n\nmerged_dfs = [merged_trans]\nmerged_colnames =['trans_merged']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"91fddc42b6bf4fa8785b196ae6b0cc39b54769bb"},"cell_type":"code","source":"# merged_trans_copy = merged_trans.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d9979679d1bf49236798d27ea1f271576b653b6"},"cell_type":"code","source":"# merged_trans = merged_trans_copy\n# # merged_trans = merged_trans_copy.sample(frac=0.01)\n# merged_dfs = [merged_trans]\n# merged_trans.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae1f4cbde9207e4d617f48f281b8a3d33e6d7ecf"},"cell_type":"code","source":"# historical_transactions = merged_trans[merged_trans['ishist']==1]\n# new_transactions = merged_trans[merged_trans['ishist']==0]\n# del merged_trans;gc.collect()\n# merged_dfs=[historical_transactions,new_transactions]\n# merged_colnames =['history','new']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"02ad84000699c9d529f5a0dd4a1cb9fcb21b8094"},"cell_type":"code","source":"num_round = 10000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f6c1856617eb92f21f9ca23b0c4a7fad02f090e8"},"cell_type":"code","source":"n_splits = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6eb3ca646141dc67d766fe018aef516dbbec6deb"},"cell_type":"code","source":"folds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=4590)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8ae87815190a7caffcbd4de34e79f10d10012bb"},"cell_type":"code","source":"start = time.time()\n# n_splits=5\n# folds = KFold(n_splits=n_splits, shuffle=True, random_state=4590)\noof = np.zeros(len(train))\npredictions = np.zeros(len(test))\nfeature_importance_df = pd.DataFrame()\nvalid_scores =[]\ntest_scores =[]\nnum_iterations =[]\nfold_importance_df = pd.DataFrame()\n# fold_importance_df[\"feature\"] = features\n# fold_importance_df[\"importance\"] = 0\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train,train['outliers'].values)):\n    print('******************************************************')\n    print(\"FOLD  ---  {}\".format(fold_))\n    print('******************************************************')\n   \n    tr = train.iloc[trn_idx]\n    val = train.iloc[val_idx]\n    \n    #drop any existing target enc cols\n    tr,val,test = droptargetenccols(tr,val,test)\n   \n    #target encoding on transaction merchant id\n    tr,val,test = targetencode_modelfold_mergedtrans(tr, val,test,merged_dfs,merged_colnames,catcolnames,targetcolname,\n                                         smoothing,min_samples_leaf,noise_level,cutoff)\n\n    enc_cols = [col for col in tr.columns if 'targetenc' in col]\n    print('enc cols:',enc_cols)\n    print('save encoding feats...')\n    #save target encoding features in separate file\n    tr[['card_id']+enc_cols].to_hdf('train_targetenc_feats'+str(fold_)+'.hdf',key='data')\n    val[['card_id']+enc_cols].to_hdf('val_targetenc_feats'+str(fold_)+'.hdf',key='data')\n    test[['card_id']+enc_cols].to_hdf('test_targetenc_feats'+str(fold_)+'.hdf',key='data')\n    \n    print('save encoding feats in csv...')\n    tr[['card_id']+enc_cols].to_csv('train_targetenc_feats'+str(fold_)+'.csv')\n    val[['card_id']+enc_cols].to_csv('val_targetenc_feats'+str(fold_)+'.csv')\n    test[['card_id']+enc_cols].to_csv('test_targetenc_feats'+str(fold_)+'.csv')\n\n\nend = time.time()\nprint('Target Enc Execution Time:',end-start)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad27e5b63d765bbc2f72777fe50070c46eb21656"},"cell_type":"code","source":"# test_mask = merged_dfs[0]['target'].isnull()\n# tr_merch_ids = merged_dfs[0].loc[~test_mask,'merchant_id'].unique()\n# test_merch_ids = merged_dfs[0].loc[test_mask,'merchant_id'].unique()\n# test_m_tr = set(test_merch_ids).difference(set(tr_merch_ids))\n# print(len(test_m_tr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"120479c4615580d50369e719aed09f92ff856a1a"},"cell_type":"code","source":"# n_splits=2206\n# fold_to_start = 52\n# fold_to_stop = 54","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d820ba89b7c9c03d80c6abbf6f426ae8a6871486"},"cell_type":"code","source":"# folds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=4590)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da3b828532f6d959de1cba9073301691a56dedf8","scrolled":false},"cell_type":"code","source":"# start = time.time()\n# # folds = KFold(n_splits=n_splits, shuffle=True, random_state=4590)\n# oof = np.zeros(len(train))\n# predictions = np.zeros(len(test))\n# feature_importance_df = pd.DataFrame()\n# valid_scores =[]\n# test_scores =[]\n# num_iterations =[]\n# fold_importance_df = pd.DataFrame()\n# # fold_importance_df[\"feature\"] = features\n# # fold_importance_df[\"importance\"] = 0\n\n# for fold_, (trn_idx, val_idx) in enumerate(folds.split(train,train['outliers'].values)):\n#     if (fold_to_stop is not None):\n#         if (fold_ >=fold_to_stop):\n#             break\n\n#     if (fold_to_start is not None):\n#         if (fold_ < fold_to_start):\n#             continue\n#     print('******************************************************')\n#     print(\"FOLD  ---  {}\".format(fold_))\n#     print('******************************************************')\n   \n#     tr = train.iloc[trn_idx]\n#     val = train.iloc[val_idx]\n    \n#     print('tr shape:',tr.shape)\n#     print('val shape:',val.shape)\n    \n#     #drop any existing target enc cols\n#     tr,val,test = droptargetenccols(tr,val,test)\n    \n# #     #target encoding on train main columns\n# #     tr,val,test =  targetencode_train_main(tr,val,test,catcolnames_main,targetcolname,\n# #                                          smoothing_main,min_samples_leaf_main,noise_level_main,cutoff)\n# #     tr,val,test = gensumtargetenc(tr,val,test)\n   \n#     #target encoding on transaction merchant id\n#     tr,val,test = targetencode_modelfold_mergedtrans(tr, val,test,merged_dfs,merged_colnames,catcolnames,targetcolname,\n#                                          smoothing,min_samples_leaf,noise_level,cutoff)\n\n#     enc_cols = [col for col in tr.columns if 'targetenc' in col]\n# #     enc_cols = [col for col in tr.columns if ('targetenc' in col) or ('targetstdenc' in col)]\n    \n# #     enc_cols=[]\n#     print('enc cols:',enc_cols)\n#     print('save encoding feats...')\n#     #save target encoding features in separate file\n#     tr[['card_id']+enc_cols].to_hdf('train_targetenc_feats'+str(fold_)+'.hdf',key='data')\n#     val[['card_id']+enc_cols].to_hdf('val_targetenc_feats'+str(fold_)+'.hdf',key='data')\n#     test[['card_id']+enc_cols].to_hdf('test_targetenc_feats'+str(fold_)+'.hdf',key='data')\n    \n# #     print('enc cols for lgb train:',enc_cols)\n\n# #     print('main train hist na merchants:',tr[tr['hist_targetenc_merchant_id_mean'].isnull()].shape)\n# #     print('main train new na merchants:',tr[tr['new_hist_targetenc_merchant_id_mean'].isnull()].shape)\n# #     print('main val hist na merchants:',val[val['hist_targetenc_merchant_id_mean'].isnull()].shape)\n# #     print('main val new na merchants:',val[val['new_hist_targetenc_merchant_id_mean'].isnull()].shape)\n# #     print('main test hist na merchants:',test[test['hist_targetenc_merchant_id_mean'].isnull()].shape)\n# #     print('main test new na merchants:',test[test['new_hist_targetenc_merchant_id_mean'].isnull()].shape)    \n\n#     cur_features = features + enc_cols\n    \n#     trn_data = lgb.Dataset(tr[cur_features], label=target.iloc[trn_idx])#, categorical_feature=categorical_feats)\n#     val_data = lgb.Dataset(val[cur_features], label=target.iloc[val_idx])#, categorical_feature=categorical_feats)\n\n#     clf = lgb.train(param, trn_data, num_round, valid_sets = [val_data], verbose_eval=100, early_stopping_rounds = 100)\n#     oof[val_idx] = clf.predict(val[cur_features], num_iteration=clf.best_iteration)\n    \n#     fold_importance_df[\"feature\"] = cur_features\n#     if fold_==0:\n#         fold_importance_df[\"importance\"] =0\n#     fold_importance_df[\"importance\"] += clf.feature_importance() / n_splits\n#     valid_scores+=[clf.best_score['valid_0']['rmse']]\n#     num_iterations+=[clf.best_iteration]\n#     cur_preds= clf.predict(test[cur_features], num_iteration=clf.best_iteration) \n#     predictions += cur_preds / folds.n_splits\n    \n#     if hastestlabels:\n#         test_score=mean_squared_error(cur_preds, test_target)**0.5\n#         print(\"test cur score: {:<8.5f}\".format(test_score))        \n#         test_scores += [test_score]\n# print('num_iterations:',num_iterations)\n# print('valid scores:',valid_scores)\n# print(\"CV score: {:<8.5f}\".format(mean_squared_error(oof, target)**0.5))\n# if hastestlabels:\n#     print(\"Test CV score: {:<8.5f}\".format(mean_squared_error(predictions, test_target)**0.5))        \n#     print('Test scores:',test_scores)\n# end = time.time()\n# print('Model Execution Time:',end-start)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f409e57cb81877a7072f62a93fa4f2192a87157a"},"cell_type":"code","source":"# corr = train[['hist_targetenc_mean_merchant_id_mean','new_targetenc_mean_merchant_id_mean','target']].corr()\n# corr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b23550968ef3fb49ae0fcc5533551d702297c990","scrolled":false},"cell_type":"code","source":"\n# n_splits=5\n# folds = KFold(n_splits=n_splits, shuffle=True, random_state=15)\n# oof = np.zeros(len(train))\n# predictions = np.zeros(len(test))\n# start = time.time()\n# feature_importance_df = pd.DataFrame()\n# valid_scores =[]\n# fold_importance_df = pd.DataFrame()\n# fold_importance_df[\"feature\"] = features\n# fold_importance_df[\"importance\"] = 0\n# for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n#     print(\"fold n{}\".format(fold_))\n#     trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=target.iloc[trn_idx], categorical_feature=categorical_feats)\n#     val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx], categorical_feature=categorical_feats)\n\n#     num_round = 10000\n#     clf = lgb.train(param, trn_data, num_round, valid_sets = [val_data], verbose_eval=100, early_stopping_rounds = 200)\n#     oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n#     fold_importance_df[\"importance\"] += clf.feature_importance() / n_splits\n# #     fold_importance_df[\"fold\"] = fold_ + 1\n#     feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n#     print(clf.best_score)\n#     valid_scores+=[clf.best_score['valid_0']['rmse']]\n#     predictions += clf.predict(test[features], num_iteration=clf.best_iteration) / folds.n_splits\n\n# print('valid scores:',valid_scores)\n# print(\"CV score: {:<8.5f}\".format(mean_squared_error(oof, target)**0.5))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a1f0a866e05f8a450960e2d787a641fc35991a1"},"cell_type":"markdown","source":"<a id=\"4\"></a> <br>\n## 4. Feature importance\nFinally, we can have a look at the features that were used by the model:"},{"metadata":{"trusted":true,"_uuid":"740bbd4abfc41ed73360e22ec42abaf1d4c6bffc"},"cell_type":"code","source":"# print(len(np.unique(oof)))\n# print(train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a871fbcecc5a66e2580868aeeff8d4448c3ba1b"},"cell_type":"code","source":"# np.savetxt('LGB_targetenc_card_smoothing.npy',oof)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d479e83032448481b40c216264a039cacdb2f9a1","scrolled":false},"cell_type":"code","source":"# cols = (fold_importance_df[[\"feature\", \"importance\"]]\n#         .groupby(\"feature\")\n#         .mean()\n#         .sort_values(by=\"importance\", ascending=False)[:1000].index)\n\n# best_features = fold_importance_df.loc[fold_importance_df.feature.isin(cols)]\n\n# plt.figure(figsize=(14,25))\n# sns.barplot(x=\"importance\",\n#             y=\"feature\",\n#             data=best_features.sort_values(by=\"importance\",\n#                                            ascending=False))\n# plt.title('LightGBM Features (avg over folds)')\n# plt.tight_layout()\n# plt.savefig('lgbm_importances.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e90f4d4f572b7a8311431066354d267e9072e98f"},"cell_type":"code","source":"# best_features=best_features.sort_values(by=\"importance\", ascending=False)\n# best_features.to_csv('best_features.csv')\n# print(best_features[:100])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"003ae1b1bd522b1b0d992ff220ed98d2a6d7477a"},"cell_type":"markdown","source":"<a id=\"5\"></a> <br>\n## 5. Submission\nNow, we just need to prepare the submission file:"},{"metadata":{"trusted":true,"_uuid":"82d5ac08a13603b2a66c59d98584c4b709daee2d"},"cell_type":"code","source":"# sub_df = pd.DataFrame({\"card_id\":test[\"card_id\"].values})\n# sub_df[\"target\"] = predictions\n# sub_df.to_csv(\"submit_targetenc_card_smoothing.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa5a22ef0ae0506dae21739f0c12fd97da8284e0"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}